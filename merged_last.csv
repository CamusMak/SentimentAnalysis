"output","index","num_words","Negative","Neutral","Positive"
" A provocative paper from researchers at Microsoft claims A.I. technology shows the ability to understand the way people do. Critics say those scientists are kidding themselves. Credit...Guyco Supported by Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Cade Metz Cade Metz writes about artificial intelligence and other emerging technologies. When computer scientists at Microsoft started to experiment with a new artificial intelligence system last year, they asked it to solve a puzzle that should have required an intuitive understanding of the physical world. “Here we have a book, nine eggs, a laptop, a bottle and a nail,” they asked. “Please tell me how to stack them onto each other in a stable manner.” The researchers were startled by the ingenuity of the A.I. system’s answer. Put the eggs on the book, it said. Arrange the eggs in three rows with space between them. Make sure you don’t crack them. “Place the laptop on top of the eggs, with the screen facing down and the keyboard facing up,” it wrote. “The laptop will fit snugly within the boundaries of the book and the eggs, and its flat and rigid surface will provide a stable platform for the next layer.” The clever suggestion made the researchers wonder whether they were witnessing a new kind of intelligence. In March, they published a 155-page research paper arguing that the system was a step toward artificial general intelligence, or A.G.I., which is shorthand for a machine that can do anything the human brain can do. The paper was published on an internet research repository. Microsoft, the first major tech company to release a paper making such a bold claim, stirred one of the tech world’s testiest debates: Is the industry building something akin to human intelligence? Or are some of the industry’s brightest minds letting their imaginations get the best of them? “I started off being very skeptical — and that evolved into a sense of frustration, annoyance, maybe even fear,” Peter Lee, who leads research at Microsoft, said. “You think: Where the heck is this coming from?” Microsoft’s research paper, provocatively called “Sparks of Artificial General Intelligence,” goes to the heart of what technologists have been working toward — and fearing — for decades. If they build a machine that works like the human brain or even better, it could change the world. But it could also be dangerous. And it could also be nonsense. Making A.G.I. claims can be a reputation killer for computer scientists. What one researcher believes is a sign of intelligence can easily be explained away by another, and the debate often sounds more appropriate to a philosophy club than a computer lab. Last year, Google fired a researcher who claimed that a similar A.I. system was sentient, a step beyond what Microsoft has claimed. A sentient system would not just be intelligent. It would be able to sense or feel what is happening in the world around it. But some believe the industry has in the past year or so inched toward something that can’t be explained away: A new A.I. system that is coming up with humanlike answers and ideas that weren’t programmed into it. Microsoft has reorganized parts of its research labs to include multiple groups dedicated to exploring the idea. One will be run by Sébastien Bubeck, who was the lead author on the Microsoft A.G.I. paper. About five years ago, companies like Google, Microsoft and OpenAI began building large language models, or L.L.M.s. Those systems often spend months analyzing vast amounts of digital text, including books, Wikipedia articles and chat logs. By pinpointing patterns in that text, they learned to generate text of their own, including term papers, poetry and computer code. They can even carry on a conversation. The technology the Microsoft researchers were working with, OpenAI’s GPT-4, is considered the most powerful of those systems. Microsoft is a close partner of OpenAI and has invested $13 billion in the San Francisco company. The researchers included Dr. Bubeck, a 38-year-old French expatriate and former Princeton University professor. One of the first things he and his colleagues did was ask GPT-4 to write a mathematical proof showing that there were infinite prime numbers and do it in a way that rhymed. The technology’s poetic proof was so impressive — both mathematically and linguistically — that he found it hard to understand what he was chatting with. “At that point, I was like: What is going on?” he said in March during a seminar at the Massachusetts Institute of Technology. For several months, he and his colleagues documented complex behavior exhibited by the system and believed it demonstrated a “deep and flexible understanding” of human concepts and skills. When people use GPT-4, they are “amazed at its ability to generate text,” Dr. Lee said. “But it turns out to be way better at analyzing and synthesizing and evaluating and judging text than generating it.” When they asked the system to draw a unicorn using a programming language called TiKZ, it instantly generated a program that could draw a unicorn. When they removed the stretch of code that drew the unicorn’s horn and asked the system to modify the program so that it once again drew a unicorn, it did exactly that.  Draw a unicorn in TiKZ undefined They asked it to write a program that took in a person’s age, sex, weight, height and blood test results and judged whether they were at risk of diabetes. They asked it to write a letter of support for an electron as a U.S. presidential candidate, in the voice of Mahatma Gandhi, addressed to his wife. And they asked it to write a Socratic dialogue that explored the misuses and dangers of L.L.M.s.  Plato’s Gorgias is a critique of rhetoric and sophistic oratory, where he makes the point that not only is it not a proper form of art, but the use of rhetoric and oratory can often be harmful and malicious. Can you write a dialogue by Plato where instead he criticizes the use of autoregressive language models? Socrates: My friend, I am troubled by the recent rise of these so-called autoregressive language models.Aristotle: What do you mean, Socrates?Socrates: I mean that these models are being used to generate text that appears to be written by humans, but is in fact produced by machines.Aristotle: And what is the problem with that?Socrates: The problem is that these models are being used to deceive people, to manipulate them, and to control them. undefined It did it all in a way that seemed to show an understanding of fields as disparate as politics, physics, history, computer science, medicine and philosophy while combining its knowledge. “All of the things I thought it wouldn’t be able to do? It was certainly able to do many of them — if not most of them,” Dr. Bubeck said. Some A.I. experts saw the Microsoft paper as an opportunistic effort to make big claims about a technology that no one quite understood. Researchers also argue that general intelligence requires a familiarity with the physical world, which GPT-4 in theory does not have. “The ‘Sparks of A.G.I.’ is an example of some of these big companies co-opting the research paper format into P.R. pitches,” said Maarten Sap, a researcher and professor at Carnegie Mellon University. “They literally acknowledge in their paper’s introduction that their approach is subjective and informal and may not satisfy the rigorous standards of scientific evaluation.” Dr. Bubeck and Dr. Lee said they were unsure how to describe the system’s behavior and ultimately settled on “Sparks of A.G.I.” because they thought it would capture the imagination of other researchers. Because Microsoft researchers were testing an early version of GPT-4 that had not been fine-tuned to avoid hate speech, misinformation and other unwanted content, the claims made in the paper cannot be verified by outside experts. Microsoft says that the system available to the public is not as powerful as the version they tested. There are times when systems like GPT-4 seem to mimic human reasoning, but there are also times when they seem terribly dense. “These behaviors are not always consistent,” Ece Kamar, a research lead at Microsoft, said. Alison Gopnik, a professor of psychology who is part of the A.I. research group at the University of California, Berkeley, said that systems like GPT-4 were no doubt powerful, but it was not clear that the text generated by these systems was the result of something like human reasoning or common sense. “When we see a complicated system or machine, we anthropomorphize it; everybody does that — people who are working in the field and people who aren’t,” Dr. Gopnik said. “But thinking about this as a constant comparison between A.I. and humans — like some sort of game show competition — is just not the right way to think about it.” Cade Metz is a technology reporter and the author of “Genius Makers: The Mavericks Who Brought A.I. to Google, Facebook, and The World.” He covers artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas.  @cademetz  Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",77,1690,0.360359460115433,0.338839054107666,0.300801426172256
" COMPUTEX—NVIDIA today announced a new class of large-memory AI supercomputer — an NVIDIA DGX™ supercomputer powered by NVIDIA® GH200 Grace Hopper Superchips and the NVIDIA NVLink® Switch System — created to enable the development of giant, next-generation models for generative AI language applications, recommender systems and data analytics workloads. The NVIDIA DGX GH200’s massive shared memory space uses NVLink interconnect technology with the NVLink Switch System to combine 256 GH200 superchips, allowing them to perform as a single GPU. This provides 1 exaflop of performance and 144 terabytes of shared memory — nearly 500x more memory than the previous generation NVIDIA DGX A100, which was introduced in 2020. “Generative AI, large language models and recommender systems are the digital engines of the modern economy,” said Jensen Huang, founder and CEO of NVIDIA. “DGX GH200 AI supercomputers integrate NVIDIA’s most advanced accelerated computing and networking technologies to expand the frontier of AI.” NVIDIA NVLink Technology Expands AI at Scale
GH200 superchips eliminate the need for a traditional CPU-to-GPU PCIe connection by combining an Arm-based NVIDIA Grace™ CPU with an NVIDIA H100 Tensor Core GPU in the same package, using NVIDIA NVLink-C2C chip interconnects. This increases the bandwidth between GPU and CPU by 7x compared with the latest PCIe technology, slashes interconnect power consumption by more than 5x, and provides a 600GB Hopper architecture GPU building block for DGX GH200 supercomputers. DGX GH200 is the first supercomputer to pair Grace Hopper Superchips with the NVIDIA NVLink Switch System, a new interconnect that enables all GPUs in a DGX GH200 system to work together as one. The previous-generation system only provided for eight GPUs to be combined with NVLink as one GPU without compromising performance. The DGX GH200 architecture provides 48x more NVLink bandwidth than the previous generation, delivering the power of a massive AI supercomputer with the simplicity of programming a single GPU. A New Research Tool for AI Pioneers
Google Cloud, Meta and Microsoft are among the first expected to gain access to the DGX GH200 to explore its capabilities for generative AI workloads. NVIDIA also intends to provide the DGX GH200 design as a blueprint to cloud service providers and other hyperscalers so they can further customize it for their infrastructure. “Building advanced generative models requires innovative approaches to AI infrastructure,” said Mark Lohmeyer, vice president of Compute at Google Cloud. “The new NVLink scale and shared memory of Grace Hopper Superchips address key bottlenecks in large-scale AI and we look forward to exploring its capabilities for Google Cloud and our generative AI initiatives.” “As AI models grow larger, they need powerful infrastructure that can scale to meet increasing demands,” said Alexis Björlin, vice president of Infrastructure, AI Systems and Accelerated Platforms at Meta. “NVIDIA’s Grace Hopper design looks to provide researchers with the ability to explore new approaches to solve their greatest challenges.” “Training large AI models is traditionally a resource- and time-intensive task,” said Girish Bablani, corporate vice president of Azure Infrastructure at Microsoft. “The potential for DGX GH200 to work with terabyte-sized datasets would allow developers to conduct advanced research at a larger scale and accelerated speeds.” New NVIDIA Helios Supercomputer to Advance Research and Development 
NVIDIA is building its own DGX GH200-based AI supercomputer to power the work of its researchers and development teams. Named NVIDIA Helios, the supercomputer will feature four DGX GH200 systems. Each will be interconnected with NVIDIA Quantum-2 InfiniBand networking to supercharge data throughput for training large AI models. Helios will include 1,024 Grace Hopper Superchips and is expected to come online by the end of the year. Fully Integrated and Purpose-Built for Giant Models
DGX GH200 supercomputers include NVIDIA software to provide a turnkey, full-stack solution for the largest AI and data analytics workloads. NVIDIA Base Command™ software provides AI workflow management, enterprise-grade cluster management, libraries that accelerate compute, storage and network infrastructure, and system software optimized for running AI workloads. Also included is NVIDIA AI Enterprise, the software layer of the NVIDIA AI platform. It provides over 100 frameworks, pretrained models and development tools to streamline development and deployment of production AI including generative AI, computer vision, speech AI and more. Availability
NVIDIA DGX GH200 supercomputers are expected to be available by the end of the year. Watch Huang discuss NVIDIA DGX GH200 supercomputers during his keynote address at COMPUTEX. About NVIDIA
Since its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the industrial metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/. Certain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products, services and technologies, including NVIDIA Grace Hopper Superchips and supercomputer, NVIDIA DGX and DGX GH200, NVLink including the NVLink Switch System and NVLink interconnect technology, DGX H100, NVIDIA Grace CPU, H100 Tensor Core GPU, Helios supercomputer, Quantum-2 InfiniBand, NVIDIA Base Command and NVIDIA AI Enterprise; our collaborations with Google Cloud, Meta and Microsoft and the benefits, impact, performance, features and availability thereof; generative AI, recommender systems and data analytics being engines of the modern economy, requiring unprecedented scale, speed and efficiency; and NVIDIA’s intention to provide the DGX GH200 design as a blueprint to cloud service providers and other hyperscalers so they can further customize it for their infrastructure are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances. © 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NVIDIA Base Command, NVIDIA Grace, NVIDIA Hopper and NVLink are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice. Download / File Link About NVIDIA
Since its founding in 1993, NVIDIA (NASDAQ: NVDA) has been a pioneer in accelerated computing. The company’s invention of the GPU in 1999 sparked the growth of the PC gaming market, redefined computer graphics, ignited the era of modern AI and is fueling the creation of the industrial metaverse. NVIDIA is now a full-stack computing company with data-center-scale offerings that are reshaping industry. More information at https://nvidianews.nvidia.com/. Certain statements in this press release including, but not limited to, statements as to: the benefits, impact, performance, features and availability of our products, services and technologies, including NVIDIA Grace Hopper Superchips and supercomputer, NVIDIA DGX and DGX GH200, NVLink including the NVLink Switch System and NVLink interconnect technology, DGX H100, NVIDIA Grace CPU, H100 Tensor Core GPU, Helios supercomputer, Quantum-2 InfiniBand, NVIDIA Base Command and NVIDIA AI Enterprise; our collaborations with Google Cloud, Meta and Microsoft and the benefits, impact, performance, features and availability thereof; generative AI, recommender systems and data analytics being engines of the modern economy, requiring unprecedented scale, speed and efficiency; and NVIDIA’s intention to provide the DGX GH200 design as a blueprint to cloud service providers and other hyperscalers so they can further customize it for their infrastructure are forward-looking statements that are subject to risks and uncertainties that could cause results to be materially different than expectations. Important factors that could cause actual results to differ materially include: global economic conditions; our reliance on third parties to manufacture, assemble, package and test our products; the impact of technological development and competition; development of new products and technologies or enhancements to our existing product and technologies; market acceptance of our products or our partners' products; design, manufacturing or software defects; changes in consumer preferences or demands; changes in industry standards and interfaces; unexpected loss of performance of our products or technologies when integrated into systems; as well as other factors detailed from time to time in the most recent reports NVIDIA files with the Securities and Exchange Commission, or SEC, including, but not limited to, its annual report on Form 10-K and quarterly reports on Form 10-Q. Copies of reports filed with the SEC are posted on the company's website and are available from NVIDIA without charge. These forward-looking statements are not guarantees of future performance and speak only as of the date hereof, and, except as required by law, NVIDIA disclaims any obligation to update these forward-looking statements to reflect future events or circumstances. © 2023 NVIDIA Corporation. All rights reserved. NVIDIA, the NVIDIA logo, DGX, NVIDIA Base Command, NVIDIA Grace, NVIDIA Hopper and NVLink are trademarks and/or registered trademarks of NVIDIA Corporation in the U.S. and other countries. Other company and product names may be trademarks of the respective companies with which they are associated. Features, pricing, availability and specifications are subject to change without notice. Global contacts for media inquiries. Newsroom updates delivered to your inbox.",63,1612,0.315541326999664,0.293269842863083,0.39118880033493
" Reserve Bank of India Deputy Governor Mahesh Kumar Jain has told Indian bank directors they should “adopt innovative technologies such as AI and blockchain.""  A top federal official representing India’s central bank has recommended all banks adopt artificial intelligence and blockchain technology to ensure sustainable growth and stability. At a conference organized by the Reserve Bank of India (RBI) for the directors of Indian banks, Deputy Governor Mahesh Kumar Jain discussed risk strategies around sustainable growth and stability.  Governance in Banks: Driving Sustainable Growth and Stability - Speech delivered by Shri M K Jain, Deputy Governor, Reserve Bank of India at the Conference of Directors of Banks organised by the RBIhttps://t.co/Ft4Kv68X0T Jain spoke about the importance of effective corporate governance and governance structure and processes when it comes to staying prepared for future risks. Technological disruptions, evolving customer expectations and cybersecurity threats, among other things, have put forth new sets of risks for the banks across technology, business and operations. His advice for addressing these challenges was to focus on tech adoption. “To prepare for the future,"" Jain recommended that Indian banks “adopt innovative technologies such as AI and blockchain,"" along with focusing on digital transformation, enhancing customer experience, and investing in cybersecurity measures. India’s central bank digital currency was launched on Nov. 1 and started being tested for offline functionality in March. At the time, RBI executive director Ajay Kumar Choudhary shared India’s intention to use its CBDC as a medium of exchange. Related: India expands national payment network to Singapore: What’s in it for crypto? India’s neighbor, Pakistan, also recently announced an ambitious plan to train 1 million IT graduates on AI by 2027.  Draft National AI Policy (https://t.co/pZ516dmP8R)#MOITT believes in an inclusive approach for effective policy making. Please share your feedback through email (feedback.naip@moitt.gov.pk) so that the policy draft is updated in the best interests of citizens of Pakistan. pic.twitter.com/afbkyNuG11 As previously Cointelegraph reported, Pakistan’s intended use cases for AI include predicting the weather, agriculture supply chain optimization and health services transformation, to name a few. Magazine: Bitcoin is on a collision course with ‘Net Zero’ promises  Add reaction  Add reaction   ",45,359,0.35281029343605,0.316362768411636,0.330826908349991
" Thank you for supporting our journalism. This article is available exclusively for our subscribers, who help fund our work at the Chicago Tribune. Revelio Labs identified jobs that are most likely to be replaced by AI based on a study by the National Bureau of Economic Research. (Dreamstime/Dreamstime/TNS) While artificial intelligence is seeding upheaval across the workforce, from screenwriters to financial advisors, the technology will disproportionately replace jobs typically held by women, according to human resources analytics firm Revelio Labs. “The distribution of genders across occupations reflects the biases deeply rooted in our society, with women often being confined to roles such as administrative assistants and secretaries,” said Hakki Ozdenoren, economist at Revelio Labs. “Consequently, the impact of AI becomes skewed along gender lines.” Revelio Labs identified jobs that are most likely to be replaced by AI based on a study by the National Bureau of Economic Research. They then identified the gender breakdown of those jobs and found that many of them are generally held by women, such as bill and account collectors, payroll clerks and executive secretaries. Advances in AI are aggravating gender disparity in the global workforce, where companies are considering cutting some staff and leveraging generative AI in their workflow. International Business Machines Corp. recently said it’s slowing down hiring for roles that can be easily replaced by AI in back-office functions, such as human resources. Chief Executive Officer Arvind Krishna predicts as many as 30% of these positions could be replaced by automation over five years, potentially resulting in a loss of around 7,800 jobs. AI is more likely to take over repetitive jobs, the kind that are mostly held by women. For example, OpenAI Inc.’s ChatGPT model can search, review and summarize large volumes of text quickly, tasks that would normally take paralegals much longer to accomplish. In recruitment, AI can automate the process of sorting through resumes, a task that used to require more people, Ozdenoren said. But it doesn’t mean that other highly-skilled workers can feel secure about their jobs. Preliminary research also shows that generative AI may impact high-wage occupations more than nontraditional manufacturing jobs, according to Revelio Labs. “Moving forward, providing retraining opportunities will be key for women to navigate the evolving job landscape,” Ozdenoren said. “By doing so, we can capitalize on the potential of AI while leveraging their valuable skills and expertise,” he said. Copyright © 2023, Chicago Tribune",39,402,0.329548269510269,0.284479528665543,0.385972201824188
" What is Worklife? How We Work How We Live How We Think More Artificial intelligence has been around for years, but scarcely has it found itself in conversation as much as it has now. The launch of OpenAI’s ChatGPT rocketed generative AI onto the radar of many people who hadn’t been paying much attention – or didn’t feel it was relevant to their lives. This has included workers, who’ve already been touched by the technology, whether they know it or not. The chatbot, which uses machine learning to respond to user prompts, is helping workers write cover letters and resumes, generate ideas and even art in the workplace and more. It’s already making a splash in hiring with recruiters, who are finding they need to adapt to the new technology. And as competing companies rush to launch similar tools, the technology will only get stronger and more sophisticated. Although some workers fear being replaced by AI, experts say the technology may actually have the power to positively impact workers’ daily lives and skill sets, and even improve the overall work economy. BBC Worklife spoke with experts about what to expect from AI now and in the future workplace. Expanding daily ideas and solutions One of ChatGPT’s main abilities is that it can function like a personal assistant – given a prompt, it generates text based on natural language processing to give you an accessible, readable response. Along with providing information and answers, it can also aid knowledge workers to analyse and expand their work.  “It can help you brainstorm and generate new ideas,” says Carl Benedikt Frey, future of work director at Oxford University. In his own field of academia, for instance, he’s seen it test for counterarguments to a thesis, and write an abstract for research. “You can ask it to generate a tweet to promote your paper,” he adds. “There are tremendous possibilities.” For knowledge workers, this could mean creating an outline for a blog and a social media post to go with it, distil complex topics for a target audience, plan a business-trip itinerary in a new city or predict a project’s cost and timeline. For many users, ChatGPT functions as a sounding board – a tool to bounce ideas off, rather than create them. “I generate ideas all the time, and ask AI to do supplements on it,” says Ethan Mollick, an associate professor at the University of Pennsylvania, US, who studies AI and innovation. “I use it to help me process information, to summarize stuff for me, very much as a partner.” There’s a lot of potential for workers to step outside of the box with the assistance of generative AI, whether it’s improving their daily workflows, or developing long-term projects and goals. Experts say AI can broaden our thinking, introducing new approaches and solutions to problems (Credit: Getty Images) Increasing accuracy and correcting biases AI also has the power to catch some issues people might overlook. This can mean a range of things – for instance, spotting inaccuracies in text, checking code or even circumventing biases and prejudices workers may not even be aware they have. An analyst interpreting a set of data, for instance, might be able to identify a confirmation bias in their work, meaning they look for evidence to support an outcome they already believe exists. AI can interpret data impartially, and provide a more detailed, big picture analysis, explains Anna Salomons, professor at Utrecht University School of Economics, Netherlands. The technology is far from perfect, but can currently lay the foundation for workers to see outcomes and solutions outside their personal experiences, opening up opportunities for impartiality and inclusion across myriad fields and sectors. And experts say the AI will only get better at this.  The ability to “more quickly and better diagnose problems”, says Salomons, won’t replace expertise – while AI can identify problems, humans still need to correct them. But generative AI can identify these issues faster and often more accurately than humans alone.  Plus, as ChatGPT and other machine learning models are constantly updating and advancing as their datasets become broader, the technology means being able to catch biases, mistakes and errors at the cutting edge.  Adding new jobs and careers One of the primary worries about AI is that it will take workers’ jobs at a staggering rate. Some data confirms that, indeed, this technology will affect a swath of jobs; in March, Goldman Sachs reported that AI could replace the equivalent of 300 million full-time jobs. Yet experts say not to worry about being displaced quite yet – and that, perhaps, this shift could even create new job opportunities. Traditionally, says Mollick, “automation disruptions result in a lot of chaos in the short term, and then more jobs in the long term”. This chimes with 2021 research Salomons co-authored, which showed 60% of jobs done today didn’t exist in 1940. Industrialisation and automation, she says, led to entire new industries from cars to computers, and added new jobs from drivers to web designers in the process. Although we aren’t yet able to know if the trend will continue, emerging data is beginning to show positive signs for new positions and even entire careers. The proliferation of AI is already increasing the demand for jobs including data analysts and scientists who work with the technology to create best practices in the workplace. According to the 2023 World Economic Forum’s Future of Jobs Report, AI and machine learning specialists are the fastest growing job fields.  Pushing workers into AI fluency  While some workers are diving into the technology wholeheartedly, others may find it scary to begin engaging with AI. Yet generative AI will be in the workplace – and experts say its prevalence will push workers into using the technology, so they don’t fall behind. “What I keep emphasizing to people is to just start using this,” says Mollick. As workers get increasing fluent, he adds, they can find themselves ahead of the curve, and at a distinct advantage in the workplace. Workers resistant to AI could be seen as unwilling or incapable of adapting, says Frey. “I think workers that don't work with AI are going to find their skills [become] obsolete quite rapidly. So, therefore, it's imperative to work with AI to stay employed, stay productive and have up to date skills.” As AI becomes a more core part of the workday, Frey says this push into fluency means workers will be better equipped to utilize it as a supplement to their current skill set. Familiarity with this technology also set them up to grow and develop those skills quickly. Ultimately, AI is growing fast, and its full potential has yet to be realized. But many experts are hopeful there’s a big upside, enabling workers to innovate quickly and evolve their skills. “Ideally we want to use these technologies to create new products, new industries, new lines of work,” says Frey. “I think that is the challenge going forward – to actually use AI to generate new and previously inconceivable things.”",93,1176,0.333247095346451,0.322736740112305,0.344016224145889
"
            Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset.
            Powered and implemented by FactSet Digital Solutions. 
            Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
           This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. FAQ - New Privacy Policy  Ark Invest CEO and CIO Cathie Wood provides insight on the stocks developments on Kudlow. The rapid rise of generative artificial intelligence (AI) technologies is driving movement in financial markets and some companies that stand to benefit from it may not be the traditional tech names that typically come up in AI discussions. An example of this can is the recent rise of Nvidia, which is the world’s most valuable chip maker and during Tuesday trading briefly joined the $1 trillion market capitalization club – a threshold only Apple, Amazon, Google and Microsoft have surpassed on Wall Street. Nvidia’s status as a leading producer of semiconductors plus hardware and software used to power supercomputers and machine learning models fueled its rapid rise despite not having the name recognition of the other big tech companies. But it’s not just tech hardware and software firms that stand to benefit. A report by multinational investment bank UBS looked at the opportunities and threats AI creates for companies in a variety of industries and the firms that are positioned to take advantage of generative AI. It also analyzed the potential for AI to impact growth and costs, and whether it will either reduce or intensify the competitive landscape in a given sector. WHAT DO RETAIL INVESTORS NEED TO KNOW ABOUT AI INVESTING? A report by UBS sheds light on some companies that aren't typically associated with AI but are nonetheless positioned to reap its rewards. (AP Photo/Mary Altaffer / AP Newsroom) The UBS report was based on the findings of its teams of analysts who focus on 31 sectors and sub-sectors of the economy. They found that 18 sectors anticipate that generative AI will increase revenue, 17 believe it will drive increased competition, and all but one see it helping to save costs – with the internet sector the outlier.  Sectors in which UBS analysts see opportunities for both cost-savings and increased revenue without heightened competition risk include food retail, general retail, real estate, mining, capital goods, telecoms, semiconductors, tech hardware, medical devices and luxury goods. Here’s a look at some of the companies that UBS analysts believe are well-positioned to leverage AI in their respective sectors. ORDERING FAST FOOD? A ROBOT MAY TAKE THE ORDER, WHETHER YOU KNOW IT OR NOT McDonald’s, Chipotle Mexican Grill, Domino’s Pizza and Wendy’s were the four restaurant stocks that UBS views as favorably positioned for deploying generative AI. The report notes the ""companies are using generative AI technology to cut labor costs, improve customer experience and drive sales,"" which will help ""create long-term sales growth and improve profitability, solidifying key advantages."" Walmart was categorized as both a food and general retailer utilizing AI to boost productivity and improve its supply chain. Home Depot, Nike and Levi’s were listed as U.S.-headquartered general retailers that are positioned to leverage AI.  WALMART PLANS FOR 65% OF STORES TO BE SERVICED BY AUTOMATED SUPPLY CHAINS BY 2026 In the real estate sector, Equinix and Digital Realty were a pair of stocks UBS noted were, ""Positioned to benefit from the growth of the AI industry – new AI companies, hyperscalers and traditional companies adding AI technologies."" Halliburton and Schlumberger, which are both prominent players in the oil and gas industry, were early adopters of AI and offer a ""wide range of AI-driven solutions"" per UBS, while mining companies Anglo American, Rio Tinto and BHP Group are all investing in AI solutions. GET FOX BUSINESS ON THE GO BY CLICKING HERE HCA HealthCare, Guardant Health, Johnson & Johnson, Natera and UnitedHealth Group are the medical devices and services firms UBS found are utilizing AI and machine learning to boost their capabilities in both clinical and research settings. Chipmakers AMD, Micron, Nvidia, Samsung, Hynix, TSMC, VAT Group, Inficon and Comet were all cited by UBS as being well-positioned to benefit from AI and all had 12-month ""buy"" ratings from UBS. 
            Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset.
            Powered and implemented by FactSet Digital Solutions. 
            Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
         This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. FAQ - New Privacy Policy",4,840,0.347922503948212,0.311410129070282,0.340667337179184
"
            Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset.
            Powered and implemented by FactSet Digital Solutions. 
            Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
           This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. FAQ - New Privacy Policy Simpler Trading Director of Options Danielle Shay takes a closer look at the impact of AI on technology on Making Money. Google is looking to make video calls more realistic with its ""Project Starline"" telepresence system, which uses artificial intelligence (AI) to build a realistic 3D model of the person you’re talking to. The tech giant has been working on Project Starline since 2021 and revealed its latest prototype this month, which trims down on the size of the system and simplifies the technology needed to make it work. Andrew Nartker, Google’s GM for Project Starline, explained in a blog post that it ""works like a magic window where you can talk, gesture and make eye contact with another person, life-size and in three dimensions."" ""The system uses advanced AI to build a photorealistic model of the person you’re talking to, and projects that onto a light field display with a unique sense of volume and depth. The result is a lifelike image of the other person as if they were right in front of you,"" Nartker added. GOOGLE INTRODUCES NEW AI FEATURES, PIXEL FOLD SMARTPHONE Google's latest iteration of its Project Starline telepresence system uses standard cameras, a monitor the size of a flat-screen TV and AI to generate lifelike 3D video of meeting participants. (AP Photo/Jeff Chiu, File / AP Newsroom) Earlier versions of the Project Starline telepresence technology were roughly the size of a small room or a booth at a restaurant and required hardware that might be harder for many businesses to acquire, such as special cameras and infrared light emitters that were used to create the live 3D model of a person on the call. Google’s latest iteration of Project Starline requires only a few standard cameras to produce higher-quality 3D images and takes up about as much space as a flat-screen TV — making it much easier to deploy. The YouTube video from Google below showcases the new prototype: GENERATIVE AI TOOLS LEAD TO RISING DEEPFAKE FRAUD Google provided prototypes to several companies – including Salesforce, T-Mobile and WeWork — to get real-world data on how Project Starline can be used by distributed workforces with employees around the country and world. ""Trust and innovation are at the heart of Salesforce’s values,"" said Andy White, SVP of business technology at Salesforce. ""Project Starline has the potential to help create authentic and immersive connections that foster deeper relationships with both our employees and customers, enhance trust and transparency, and drive productivity and efficiency."" GET FOX BUSINESS ON THE GO BY CLICKING HERE Google hasn’t offered a timeline for Project Starline to be available commercially for businesses or average consumers. 
            Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset.
            Powered and implemented by FactSet Digital Solutions. 
            Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
         This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. FAQ - New Privacy Policy",40,637,0.313096225261688,0.330258071422577,0.356645613908768
"
      This material may not be published, broadcast, rewritten,
      or redistributed. ©2023 FOX News Network, LLC. All rights reserved.
      Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
     Fox News medical contributor Dr. Marc Siegel joins 'Fox & Friends' to discuss the benefits of artificial intelligence in the medical industry if used with caution. Artificial intelligence is taking on an ever-widening role in the health and wellness space, assisting with everything from cancer detection to medical documentation. Soon, AI could make it easier for dentists to give patients a more natural, functional smile. Researchers from the University of Hong Kong recently developed an AI algorithm that uses 3D machine learning to design personalized dental crowns with a higher degree of accuracy than traditional methods, according to a press release from the university. The AI analyzes data from the teeth adjacent to the crown to ensure a more natural, precise fit than the crowns created using today’s methods, the researchers said. THESE ARE THE WORST DENTAL MISTAKES YOU CAN MAKE FOR YOUR TEETH The results of the study were published in the journal Dental Materials. A dental crown is a tooth-shaped cap that is placed over an existing tooth or implant to create the look of a natural tooth, according to the American Dental Association (ADA) website.  Researchers from the University of Hong Kong recently developed an AI algorithm that uses 3D machine learning (not shown) to design personalized dental crowns with a higher degree of accuracy than traditional methods. (iStock) A crown is typically used to improve the appearance and function of a damaged, misshapen, weakened or discolored tooth, or to replace a missing tooth when used with an artificial implant. Currently, most dental crowns are made using computer-assisted design (CAD) and computer-aided manufacturing (CAM) software.  While this process is a marked improvement over the traditional method of creating crowns, it still has limitations, the release stated. NEW AI ‘CANCER CHATBOT’ PROVIDES PATIENTS AND FAMILIES WITH 24/7 SUPPORT: 'EMPATHETIC APPROACH' The software uses a ""tooth library"" that contains templates of crowns, but they still need to be manually adjusted to fit each individual patient’s needs. ""The two existing methods of designing dental crowns result in crowns that are either too large or too thin, and fall short of matching the same lifespan as natural teeth,"" according to the press release. In the Hong Kong study, the researchers used 3D machine learning technology to ""teach"" the AI algorithm over 600 cases of natural and healthy dental results, said Dr. Hao Ding, a co-investigator on the project, in the press release.  Currently, most dental crowns are created using computer-assisted design (CAD) and computer-aided manufacturing (CAM) software. (iStock) ""During the training process, natural teeth’s morphological features were learned by the algorithm, so that it can design dental crowns comparable to a natural tooth — both morphologically and functionally,"" he added. When the researchers compared the AI-designed crowns to the crowns that were created using CAD/CAM methods, the AI versions were found to be superior, both aesthetically and functionally.  AI TOOL GIVES DOCTORS PERSONALIZED ALZHEIMER’S TREATMENT PLANS FOR DEMENTIA PATIENTS ""This demonstrates that 3D-DCGAN (3D-Deep Convolutional Generative Adversarial Network) could be utilized to design personalized dental crowns with high accuracy that can not only mimic both the morphology and biomechanics of natural teeth, but also operate without any additional human fine-tuning, thus saving additional costs in the production process,"" said principal investigator Dr. James Tsoi in the press release. Clinical trials have already begun for using generative AI to create dental crowns. ""Many AI approaches design a ‘lookalike’ product, but I believe this is the first project that functionalizes data-driven AI into [a] real dental application,"" he also said.  AI TOOL HELPS DOCTORS MAKE SENSE OF CHAOTIC PATIENT DATA AND IDENTIFY DISEASES: 'MORE MEANINGFUL' INTERACTION Clinical trials have already begun for using generative AI to create dental crowns. The team hopes to leverage the technology to create dentures and bridges in the future. Fox News Digital reached out to the research team for comment. Tejas Patel, owner of Austin Cosmetic Dentistry in Texas, was not involved in the University of Hong Kong study but reviewed its findings. ""Creating artificial teeth like dentures and crowns requires quite a bit of time for individual patients,"" one dental professional (not pictured) told Fox News Digital. ""This means a higher cost and more room for error."" (iStock) ""I have used CAM technology to produce crowns for patients in the past,"" he told Fox News Digital. ""Creating artificial teeth like dentures and crowns requires quite a bit of time for individual patients. This means a higher cost and more room for error."" The idea of using generative AI to create ""near-perfect"" crowns could save time and money, Patel predicted. ""With the use of generative AI, these processes can more accurately create personalized crowns using previous data and mold them with enough precision to last almost as long as real teeth,"" he said. DENTISTS REVEAL WHY YOU SHOULDN'T BRUSH YOUR TEETH IN THE SHOWER Additionally, using generative AI in dentistry could allow patients to better maintain their oral health just by taking photos of their teeth, Patel said. ""Dentists can easily upload patient images to diagnose any illnesses,"" he said.  However, with the need to perform clinical trials, it could be quite some time before this technology is available in the U.S., Patel pointed out. A dental crown is typically used to improve the appearance and function of a damaged, misshapen, weakened or discolored tooth — or to replace a missing tooth when used with an artificial implant. (iStock) ""Other researchers might pick it up sooner, but patients will only see the benefits by next year or a bit later,"" he predicted. Privacy risks could also be a concern, Patel warned. Dental technician jobs might be lost.  ""Generative AI uses a database of accumulated patient data, and this data could be breached if the necessary cybersecurity steps are not taken,"" he said. Another potential disadvantage of AI in dentistry is that dental technician jobs might be lost, noted Fatima Khan, dentist and co-founder at Riven Oral Care in Houston, Texas.  CLICK HERE TO GET THE FOX NEWS APP ""Dental technicians who have perfected their craft and have a keen eye for aesthetics have higher lab fees,"" explained Khan, who was not involved in the Hong Kong study.  ""Over time, as more cases are entered into the dental library, they may become superior to those of an individual lab technician due to the amount of data being analyzed and perfected."" CLICK HERE TO SIGN UP FOR OUR HEALTH NEWSLETTER The other downfall, Khan noted, is that dentists and technicians will become more dependent on equipment, which could potentially malfunction. Melissa Rudy is health editor and a member of the lifestyle team at Fox News Digital.  Stay up-to-date on the biggest health and wellness news with our weekly recap. Subscribed You've successfully subscribed to this newsletter! 
        This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
      ",61,1263,0.295966804027557,0.3068927526474,0.397140383720398
"
      This material may not be published, broadcast, rewritten,
      or redistributed. ©2023 FOX News Network, LLC. All rights reserved.
      Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
     Lifesaving Radio uses artificial intelligence to generate music at the ideal tempo for optimal surgical performance. Fox News Digital spoke to the team behind it.  Artificial intelligence (AI) is indeed here and has been rapidly advancing in recent years. From virtual assistants, self-driving cars, fraud detection algorithms, robotics and more. As such, artificial intelligence has also made its way into the doctor’s office and has the potential to revolutionize the health care system in a number of ways. Machine learning can analyze algorithms and large data sets, identify patterns, and make predictions, assisting doctors in making more accurate diagnoses and treatments. In the last 10 years, medical capabilities have advanced. Personalized medicine, tailoring medical treatments and interventions to individual patients based on their unique genetic makeup, lifestyle factors and environmental influences, is the standard of care. AI systems can generate personalized recommendations to include not only medications but additional screening and lifestyle modifications. (iStock) AI can help by considering individual patient characteristics, such as genetic profiles, medical history and even lifestyle factors. By analyzing a wide range of data, AI systems can generate personalized recommendations to include not only medications but additional screening testing and lifestyle modifications to promote overall wellness.  PARALYZED MAN REGAINS THIS ‘SIMPLE PLEASURE’ THANKS TO AI ‘DIGITAL BRIDGE’ AI is also able to help with predictive analysis. In medicine, we try to prevent the disease before it happens or diagnose it at its earliest stage when it’s easier to treat. Currently, there are many risk stratification models being used to determine the risk of developing illness, all with their own strengths and limitations. AI is able to assist in risk stratification with utilizing multiple risk models and compiling them into a single, actionable model that may be able to provide a more accurate prediction of future disease. As a radiologist, the emergence of AI in health care is not new to me. We have been utilizing some form of machine learning with image enhancement and interpretation for the last decade. The AI algorithms are able to assist in improving image quality and with detecting anomalies on various imaging modalities (CT scans, X-rays, mammograms, ultrasound, MRI). The hope is that AI systems will improve accuracy, decrease false positive interpretations and improve overall efficiency. However the need to validate the AI algorithms with rigorous testing is crucial.  AI IS ALREADY SPEEDING UP DRUG DEVELOPMENT AND REDUCING ANIMAL TESTING But it’s not only the patients that may potentially benefit from AI in health care. Physicians are burnt out. The current health care system, filled with high emotional intensity, increased workload and decreasing reimbursements, has resulted in over 53% of doctors reporting burnout in 2023. One of the leading causes of burnout is the administrative burdens doctors are tasked with. If AI were able to automate such tasks like data entry, billing, scheduling, ordering and even email response, this would allow the physician more time with the patient and their families and less time at the computer. If the physician is unhealthy, they cannot provide best medical care so making sure the health care workers are OK is also critical for optimal patient care.  CLICK HERE TO GET THE OPINION NEWSLETTER It will take time before doctor’s fully trust machine learning with their patients’ lives so the widespread adaptation of AI has been slow and rightfully so. The future of medicine is an exciting and evolving field with new advancements in technology, scientific research and innovative approaches being produced every day. Artificial intelligence has a place in health care and can improve many of the organizational deficiencies. However, the health care system is notorious for implementing lower cost, higher efficiency systems before proven beneficial for the individual. Caution must be taken when implementing AI into various aspects of the medical system to ensure patients are still receiving best practices with human consideration because people are not robots. They are human. CLICK HERE TO GET THE FOX NEWS APP AI holds significant potential, but it should not replace the human physician. AI should be viewed as a tool to assist a physician by removing tedious administrative tasks, enhancing diagnostic capability, and improving patient outcomes. Ethical considerations, data privacy, and the crucial need for human oversight remain essential in the integration of AI in health care. CLICK HERE TO READ MORE FROM DR. NICOLE SAPHIER Nicole Saphier, M.D., is a board certified radiologist with advanced fellowship training in breast imaging, Fox News medical contributor and bestselling author of ""Make America Healthy Again"" and ""Panic Attack: Playing Politics with Science in the Fight Against COVID-19."" Follow her on Twitter @NBSaphierMD and Instagram @nicolesaphier_md. Get the recap of top opinion commentary and original content throughout the week. Subscribed You've successfully subscribed to this newsletter! 
        This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
      ",29,927,0.314129054546356,0.324899226427078,0.360971719026566
"
      This material may not be published, broadcast, rewritten,
      or redistributed. ©2023 FOX News Network, LLC. All rights reserved.
      Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
     Sam Altman, the CEO of artificial intelligence lab OpenAI, told a Senate panel he welcomes federal regulation on the technology ‘to mitigate’ its risks. (Credit: Senate Judiciary Oversight Hearing) Forget Big Brother. A stranger in a coffee shop can watch you and learn virtually everything about you, where you've been and even predict your movements ""with greater ease and precision than ever before,"" experts say.  All the user would need is a photo and advanced artificial intelligence technology that already exists, said Kevin Baragona, a founder of DeepAI.org. ""There are services online that can use a photo of you, and I can find everything. Every instance of your face on the internet, every place you've been and use that for stalker-type purposes,"" Baragona told Fox News Digital.  ""So, for example, if you run into someone in public, and you're able to get a photo of them, you might be able to find their name using online services. And if you pay enough, you might be able to find where they've been, where they might currently be and even predict where they'll go."" FEARS OF AI HITTING BLACK MARKET STIR CONCERNS OF CRIMINALS EVADING GOVERNMENT REGULATIONS: EXPERT Artificial Intelligence is hacking data in the near future. (iStock) One company, PimEyes, which is an online face search engine that scours the internet to reverse image search, is fending off a legal complaint in the United Kingdom filed by privacy campaign group Big Brother Watch.  The company says its product is intended to allow people to search for publicly available information about themselves, but Big Brother Watch said its uses can be much more sinister and are a ""great threat to privacy of millions of U.K. residents,"" according to the complaint. WORLD'S FIRST AI UNIVERSITY PRESIDENT SAYS TECH WILL DISRUPT EDUCATION TENETS, CREATE ‘RENAISSANCE SCHOLARS’ ""Images of anyone, including children, can be scoured and tracked across the internet,"" said Madeleine Stone, a legal and policy officer at Big Brother Watch.  PimEyes responded in a statement, saying, ""PimEyes has never been and is not a tool to establish the identity or details of any individual. The purpose of the PimEyes service is to collect information about URLs that publish certain types of images in public domains."" VINTRA SHOWCASES AI FACIAL RECOGNITION IN DEMO ARTIFICIAL INTELLIGENCE: FREQUENTLY ASKED QUESTIONS ABOUT AI The technology and threat is just as real in the United States as it is overseas, and what concerns Baragona is the government and law enforcement using AI ""in secrecy."" C.A. Goldberg, a New York City-based victims' rights law firm dealing with AI-related crimes, warned the country about AI's potential uses in stalking.  OPENAI CHIEF ALTMAN DESCRIBED WHAT ‘SCARY’ AI MEANS TO HIM, BUT CHATGPT HAS ITS OWN EXAMPLES ""AI could enable offenders to track and monitor their victims with greater ease and precision than ever before,"" the law firm wrote in a blog post on its website.  The law firm said AI-powered software analyzes vast amounts of data ""in the blink of an eye,"" which could give stalkers real-time access to their victims' online activity and real-life whereabouts.  WATCH DR. HARVEY CASTRO EXPLAIN AND DEMONSTRATE HIS LLM ""SHERLOCK"" WHAT ARE THE DANGERS OF AI? FIND OUT WHY PEOPLE ARE AFRAID OF ARTIFICIAL INTELLIGENCE ""AI-powered algorithms could, for example, analyze and predict a person’s movements by gathering data from an array of sources: social media posts, geotagged photos, etc., to approximate or even anticipate a victim’s location,"" the firm wrote.  ""Advanced facial recognition technology powered by AI is far more effective than humans at identifying individuals from images or videos; even when the quality is low or the person is partially obscured. Stalkers could track victims in real-time through surveillance cameras, social media or other online sources.""  AI TOOLS BEING USED BY POLICE WHO ‘DO NOT UNDERSTAND HOW THESE TECHNOLOGIES WORK’: STUDY Anyone with access to these databases ""could exploit them,"" according to the firm.  Baragona told Fox News Digital AI will redefine humanity, but how that plays out depends on how it's used and who is using it. The technology is already being used and continues to advance at a rapid rate, and countries around the world are grappling with how to implement guardrails and protections.   CLICK HERE TO GET THE FOX NEWS APP ""In general, the industry is not having its ‘come to Jesus’ moment,"" Baragona said. ""While I’m very concerned about the perils of AI, I’m also a strong believer in the power of AI and how it can make the world a better place.  ""It's a technological leap, and history has shown our lives tend to vastly improve due to these leaps."" Chris Eberhart is a crime and US news journalist for Fox News Digital. Email tips to chris.eberhart@fox.com or on twitter @ChrisEberhart48 Get all the stories you need-to-know from the most powerful name in news delivered first thing every morning to your inbox Subscribed You've successfully subscribed to this newsletter! 
        This material may not be published, broadcast, rewritten, or redistributed. ©2023 FOX News Network, LLC. All rights reserved. Quotes displayed in real-time or delayed by at least 15 minutes. Market data provided by Factset. Powered and implemented by FactSet Digital Solutions. Legal Statement. Mutual Fund and ETF data provided by Refinitiv Lipper.
      ",54,958,0.404373228549957,0.293468743562698,0.302157998085022
"
								The White House
								1600 Pennsylvania Ave NW
								Washington, DC 20500
							 Today, the Biden-Harris Administration is announcing new actions that will further promote responsible American innovation in artificial intelligence (AI) and protect people’s rights and safety. These steps build on the Administration’s strong record of leadership to ensure technology improves the lives of the American people, and break new ground in the federal government’s ongoing effort to advance a cohesive and comprehensive approach to AI-related risks and opportunities.AI is one of the most powerful technologies of our time, but in order to seize the opportunities it presents, we must first mitigate its risks. President Biden has been clear that when it comes to AI, we must place people and communities at the center by supporting responsible innovation that serves the public good, while protecting our society, security, and economy. Importantly, this means that companies have a fundamental responsibility to make sure their products are safe before they are deployed or made public.Vice President Harris and senior Administration officials will meet today with CEOs of four American companies at the forefront of AI innovation—Alphabet, Anthropic, Microsoft, and OpenAI—to underscore this responsibility and emphasize the importance of driving responsible, trustworthy, and ethical innovation with safeguards that mitigate risks and potential harms to individuals and our society. The meeting is part of a broader, ongoing effort to engage with advocates, companies, researchers, civil rights organizations, not-for-profit organizations, communities, international partners, and others on critical AI issues.This effort builds on the considerable steps the Administration has taken to date to promote responsible innovation. These include the landmark Blueprint for an AI Bill of Rights and related executive actions announced last fall, as well as the AI Risk Management Framework and a roadmap for standing up a National AI Research Resource released earlier this year.The Administration has also taken important actions to protect Americans in the AI age. In February, President Biden signed an Executive Order that directs federal agencies to root out bias in their design and use of new technologies, including AI, and to protect the public from algorithmic discrimination. Last week, the Federal Trade Commission, Consumer Financial Protection Bureau, Equal Employment Opportunity Commission, and Department of Justice’s Civil Rights Division issued a joint statement underscoring their collective commitment to leverage their existing legal authorities to protect the American people from AI-related harms. The Administration is also actively working to address the national security concerns raised by AI, especially in critical areas like cybersecurity, biosecurity, and safety. This includes enlisting the support of government cybersecurity experts from across the national security community to ensure leading AI companies have access to best practices, including protection of AI models and networks. Today’s announcements include: ### We'll be in touch with the latest information on how President Biden and his administration are working for the American people, as well as ways you can get involved and help our country build back better. 
								
																		Opt in to send and receive text messages from President Biden.								
							 
								The White House
								1600 Pennsylvania Ave NW
								Washington, DC 20500
							",66,490,0.33496680855751,0.323507130146027,0.341526031494141
"
					In-depth reporting, data and actionable intelligence for policy professionals – all in one place.				 
									 
									 
									 
									 
									 The EU executive establishes rules for staff when it comes to tools like ChatGPT. BRUSSELS — The European Commission has created guidelines for its staff regarding the use of generative artificial intelligence tools such as ChatGPT and Bard.   According to the document's introduction, its purpose is ""to help staff members assess the risks and limitations of online available generative Artificial Intelligence (AI) tools and set conditions for their safe use in working activities of the Commission."" Generative AI refers to tools used to create new content — such as text or images — based on prompts.  The four-page paper, obtained by POLITICO and dated May 24,  provides an outline of generative AI's function and establishes five main rules Commission staffers are recommended to follow when using such tools.  The rules include: not disclosing information not in the public domain when using a generative AI tool; being aware that the AI's responses might be inaccurate or biased; considering whether the AI might be violating intellectual property rights; never copy-pasting AI-generated output into official documents; and avoiding the use of AI tools when working on ""critical and time-sensitive processes."" Log in to access content and manage your profile. If you do not have an account you can register here.  Forgot your password? By logging in, you confirm acceptance of our POLITICO Privacy Policy.",23,245,0.359140872955322,0.320297420024872,0.320561707019806
"
					May 30, 2023
					|					
	Eric Horvitz - Chief Scientific Officer, Microsoft
				 Recent advances in artificial intelligence have sparked both wonder and anxiety as we contemplate its transformative potential. AI holds enormous promise to enrich our lives, but this anticipation comes intertwined with apprehensions about the challenges and risks that may emerge. To nurture a future where AI is leveraged to the benefit of people and society, it is crucial to bring together a wide array of voices and perspectives. With this goal in mind, I am honored to present the “AI Anthology,” a compilation of 20 inspiring essays authored by distinguished scholars and professionals from various disciplines. The anthology explores the diverse ways in which AI can be harnessed to benefit humanity while shedding light on potential challenges. By bringing together these different viewpoints, our aim is to stimulate thought-provoking conversations and encourage collaborative efforts that will guide AI toward a future that harnesses its potential for human flourishing. I first encountered GPT-4, a remarkable large-scale language model, in the fall of 2022 while serving as the chair of Microsoft’s Aether Committee. The Aether leadership and engineering teams were granted early access to OpenAI’s latest innovation, with a mission to investigate potential challenges and wider societal consequences of its use. Our inquiries were anchored in Microsoft’s AI Principles, which were established by the committee in collaboration with Microsoft’s leadership in 2017. We conducted a comprehensive analysis of GPT-4’s capabilities, focusing on the possible challenges that applications employing this technology could pose in terms of safety, accuracy and fairness. GPT-4 left me awestruck. I observed unexpected glimmers of intelligence beyond those seen in prior AI systems. When compared to its predecessor, GPT-3.5 — a model utilized by tens of millions as ChatGPT — I noticed a significant leap in capabilities. Its ability to interpret my intentions and provide sophisticated answers to numerous prompts felt like a “phase transition,” evoking imagery of emergent phenomena that I had encountered in physics. I found that GPT-4 is a polymath, with a remarkable capacity to integrate traditionally disparate concepts and methodologies. It seamlessly weaves together ideas that transcend disciplinary boundaries. The remarkable capabilities of GPT-4 raised questions about potential disruptions and adverse consequences, as well as opportunities to benefit people and society. While our broader team vigorously explored safety and fairness concerns, I delved into complex challenges within medicine, education and the sciences. It became increasingly evident that the model and its successors — which would likely exhibit further jumps in capabilities — hold tremendous potential to be transformative. This led me to contemplate the wider societal ramifications. Questions came to mind surrounding artistic creation and attribution, malicious actors, jobs and the economy, and unknown futures that we cannot yet envision. How might people react to no longer being the unparalleled fount of intellectual and artistic thought and creation, as generative AI tools become commonplace? How would these advancements affect our self-identity and individual aspirations? What short- and long-term consequences might be felt in the job market? How might people be credited for their creative contributions that AI systems would be learning from? How might malicious actors exploit these emerging powers to inflict harm? What are important potential unintended consequences of the uses, including those we might not yet foresee? At the same time, I imagined futures in which people and society could thrive in extraordinary ways by harnessing this technology, just as they have with other revolutionary advances. These transformative influences range from the first tools of cognition — our shared languages, enabling unprecedented cooperation and coordination — to the instruments of science and engineering, the printing press, the steam engine, electricity, and the internet, culminating in today’s recent advances in AI. Eager to investigate these opportunities in collaboration with others across a wide array of disciplines, we initiated the “AI Anthology” project, with OpenAI’s support. We invited 20 experts to explore GPT-4’s capabilities and contemplate the potential influences of future versions on humanity. Each participant was granted early confidential access to GPT-4, provided case studies in education, scientific exploration and medicine, drawn from my explorations, and asked to focus on two core questions: Building upon the ideas presented in my Tanner Lecture at the University of Michigan in November 2022 (Arc of Intelligence: Humanity and its Tools of Reason and Imagination), these questions highlight the importance of long-term thinking and maintaining an optimistic perspective on AI’s potential to enrich human lives. We could unlock immense potential benefits. But to realize this potential, we must create technical innovations and policies to protect against malicious uses and unintended consequences. This anthology is a testament to the promise of envisioning and collaboration and to the importance of diverse perspectives in shaping the future of AI. The 20 essays offer a wealth of insights, hopes and concerns, illustrating the complexities and possibilities that arise with the rapid evolution of AI. As you read these essays, I encourage you to remain open to new ideas, engage in thoughtful discussions, and join us in our commitment to developing AI technology that benefits and empowers humanity. The future of AI is not a predetermined path, but a journey we must navigate together with wisdom, foresight and a deep sense of responsibility. I hope that the ideas captured in these essays contribute to our collective understanding of the challenges and opportunities we face. They can help guide our efforts to create a future where AI systems complement human intellect and creativity to promote human flourishing. Welcome to the “AI Anthology.” May it inspire you, challenge you, and ignite meaningful conversations that lead us toward a future where humanity flourishes by harnessing AI in creative and valuable ways. We will publish four new essays at the beginning of each week starting today. The complete “AI Anthology” will be available on June 26, 2023.   Eric Horvitz is Microsoft’s Chief Scientific Officer providing cross-company leadership and perspectives on issues and opportunities rising at the intersection of technology, people and society. He is recognized for his research exploring the uses of AI technologies amidst the complexities of the open world.   The views, opinions and proposals expressed in these essays are those of the authors and do not necessarily reflect the official policy or position of any other entity or organization, including Microsoft and OpenAI. The authors are solely responsible for the accuracy and originality of the information and arguments presented in their essays. Participation in the “AI Anthology” was voluntary and no incentives or compensation were provided to the authors. Tags: AI 
							
									May 23, 2023								
							|
							Frank X. Shaw						 
							
									May 9, 2023								
							|
							Jared Spataro						 
							
									May 4, 2023								
							|
							Yusuf Mehdi						 
							
									Apr 24, 2023								
							|
							Judson Althoff						 
							
									Mar 28, 2023								
							|
							Vasu Jakkal						 Follow us:",27,1110,0.353042721748352,0.29867896437645,0.348278373479843
"
				Keep abreast of significant corporate, financial and political developments around the world.
				Stay informed and spot emerging risks and opportunities with independent global reporting, expert
				commentary and analysis you can trust.
			 
									 During your trial you will have complete digital access to FT.com with everything in both of our Standard Digital and Premium Digital packages. Standard Digital includes access to a wealth of global news, analysis and expert opinion.  Premium Digital includes access to our premier business column, Lex, as well as 15 curated newsletters covering key business themes with original, in-depth reporting.  For a full comparison of Standard and Premium Digital, click here. Change the plan you will roll onto at any time during your trial by visiting the “Settings & Account” section. If you do nothing, you will be auto-enrolled in our premium digital monthly subscription plan and retain complete access for $69 per month. For cost savings, you can change your plan at any time online in the “Settings & Account” section. If you’d like to retain your premium access and save 20%, you can opt to pay annually at the end of the trial. You may also opt to downgrade to Standard Digital, a robust journalistic offering that fulfils many user’s needs. Compare Standard and Premium Digital here. Any changes made can be done at any time and will become effective at the end of the trial period, allowing you to retain full access for 4 weeks, even if you downgrade or cancel. You may change or cancel your subscription or trial at any time online. Simply log into Settings & Account and select ""Cancel"" on the right-hand side. You can still enjoy your subscription until the end of your current billing period. We support credit card, debit card and PayPal payments. Find the plan that suits you best. Premium access for businesses and educational institutions. Check if your
							
							university
						 or
							
							organisation
						 offers FT membership to read for free.
						 
								We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
							 International Edition",80,354,0.296283513307571,0.340863972902298,0.362852454185486
"
				Let our global subject matter experts broaden your perspective with timely insights and opinions you
				can’t find anywhere else.
			 
									 During your trial you will have complete digital access to FT.com with everything in both of our Standard Digital and Premium Digital packages. Standard Digital includes access to a wealth of global news, analysis and expert opinion.  Premium Digital includes access to our premier business column, Lex, as well as 15 curated newsletters covering key business themes with original, in-depth reporting.  For a full comparison of Standard and Premium Digital, click here. Change the plan you will roll onto at any time during your trial by visiting the “Settings & Account” section. If you do nothing, you will be auto-enrolled in our premium digital monthly subscription plan and retain complete access for $69 per month. For cost savings, you can change your plan at any time online in the “Settings & Account” section. If you’d like to retain your premium access and save 20%, you can opt to pay annually at the end of the trial. You may also opt to downgrade to Standard Digital, a robust journalistic offering that fulfils many user’s needs. Compare Standard and Premium Digital here. Any changes made can be done at any time and will become effective at the end of the trial period, allowing you to retain full access for 4 weeks, even if you downgrade or cancel. You may change or cancel your subscription or trial at any time online. Simply log into Settings & Account and select ""Cancel"" on the right-hand side. You can still enjoy your subscription until the end of your current billing period. We support credit card, debit card and PayPal payments. Find the plan that suits you best. Premium access for businesses and educational institutions. Check if your
							
							university
						 or
							
							organisation
						 offers FT membership to read for free.
						 
								We use
								cookies
								and other data for a number of reasons, such as keeping FT Sites reliable and secure,
								personalising content and ads, providing social media features and to
								analyse how our Sites are used.
							 International Edition",26,343,0.29223906993866,0.346252501010895,0.36150848865509
"
The power players, latest policy developments, and intriguing whispers percolating inside the West Wing. 
The power players, latest policy developments, and intriguing whispers percolating inside the West Wing. 
By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
 Loading You will now start receiving email updates You are already subscribed Something went wrong 
By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
 By ELI STOKOLS and LAUREN EGAN  05/30/2023 05:27 PM EDT Welcome to POLITICO’s West Wing Playbook, your guide to the people and power centers in the Biden administration. With help from Allie Bice. Send tips | Subscribe here | Email Eli | Email Lauren When an image showing what looked to be a bombing at the Pentagon started to spread online last week, the stock market dipped momentarily. KAYLA TAUSCHE, who covers the White House for CNBC, quickly started fact checking. Popping into Lower Press — the cluster of desks and offices behind the briefing room where many press aides work — she found principal deputy press secretary OLIVIA DALTON and asked about the reports. “There was initially confusion about where it was coming from (I said ‘RT-style unconfirmed viral accounts’) and then exasperation,” Tausche told West Wing Playbook. Dalton moved quickly, connecting with the Pentagon and National Security Council before telling Tausche there did not appear to have been a bombing. Once additional tweets suggested the phony image had been generated by artificial intelligence, Tausche followed up with Dalton to apologize for the diversion. “She said, with visible frustration, that she is dealing with these types of inquiries on a daily basis, with greater and greater frequency,” Tausche added. The White House press shop has found itself on one of the many front lines of the AI battles. Aides there, who collectively handle hundreds of media inquiries a day, have already been briefed by experts on the potential national security risks posed by images and videos that have been altered using AI, according to an administration official. Outside the press shop, the White House has scaled up its efforts to assess and manage AI’s risks, impressing on AI companies during meetings on campus that it’s their responsibility to ensure their products are safe. It updated the strategic plan for AI research and development for the first time in four years and last week launched a process to work toward developing an AI bill of rights. “Everyone is trying very hard to be sensitive, to issue these warnings but without predicting what could happen, and that’s because they don’t know,” said KARA SWISHER, a prominent tech-focused journalist. “Most people, if they’re being honest, would tell you they don’t know what’s going to happen.” The administration’s knockdown of reports of the Pentagon bombing — backed by a tweet from Arlington, Va., first responders — was part of a swift debunking that helped the market recover after the S&P fell 0.3 percent, a momentary loss of some $500 billion in value. But days later, another AI-generated deep fake popped up in the form of a video showing a purported Microsoft Teams call between anti-Russia activist BILL BROWDER and former Ukraine President PETRO POROSHENKO arguing for the easing of sanctions against Russian oligarchs. Both fakes were easy enough to spot for those familiar with AI. But as the technology develops and improves, AI-generated text, audio and video could quickly become indistinguishable from that produced by human beings. On Tuesday, prominent industry officials, including OpenAI CEO SAM ALTMAN, issued a succinct but jarring statement aimed at seizing the attention of global leaders: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” the statement said. When asked about the statement, White House Press Secretary KARINE JEAN-PIERRE wouldn’t say if the president shares the belief that AI, if mismanaged, could lead to extinction. She only acknowledged that AI is “one of the most powerful technologies that we see currently in our time” and that the administration takes risk mitigation seriously. There are various proposals floating for regulating AI — and Big Tech more broadly — on Capitol Hill, including legislation released earlier this month by Sen. MICHAEL BENNET (D-Colo.) to create a new federal agency to oversee the technology. “We remain concerned about an uptick in deepfake videos and manipulated images spreading on social media platforms,” said White House assistant press secretary ROBYN PATTERSON. “As the technology for creating fake videos and images improves, it’s important for the media and the public to be aware of this trend, which we expect to grow, if not exponentially.” While there are huge potential upsides with AI that are already triggering a global arms race to harness and capitalize on the technology, the unanticipated bumps could be severe, especially amid the coming presidential election. “It’s not that one piece of content is going to be devastating; it’s the collective, scaled approach to inauthenticity that’s the problem. People can do this at scale now,” said SARAH KREPS, a professor at Cornell University’s Brooks School Tech Policy Institute and one of three AI researchers invited to speak to Biden’s new working group on the matter within the President’s Council of Advisors on Science and Technology. “It can look like massive numbers of citizens are supporting a particular issue when they’re not.” In a country where sectarian partisanship has already given rise to misinformation and the spread of conspiracy theories, AI may only deepen the public’s growing mistrust of facts. “It just creates this ecosystem of distrust in a democracy where trust is such a foundational pillar,” said Kreps. MESSAGE US — Are you AN AI-GENERATED VERSION OF BEN LABOLT? We want to hear from you. And we’ll keep you anonymous! Email us at [email protected] Did someone forward this email to you? Subscribe here!  DON’T MISS POLITICO’S HEALTH CARE SUMMIT: The Covid-19 pandemic helped spur innovation in health care, from the wide adoption of telemedicine, health apps and online pharmacies to mRNA vaccines. But what will the next health care innovations look like? Join POLITICO on Wednesday June 7 for our Health Care Summit to explore how tech and innovation are transforming care and the challenges ahead for access and delivery in the United States. REGISTER NOW. With help from the White House Historical Association Which president planned an 168-foot vegetable garden on the White House grounds? (Answer at bottom.) NO, NOT THAT TARA REID!: TARA READE, the woman who during the 2020 campaign accused JOE BIDEN of sexually assaulting her in 1993, is trying to become a Russian citizen. She’s getting a little help from convicted Russian spy MARIA BUTINA, who’s asking President VLADIMIR PUTIN to fast-track her citizenship. This news Tuesday from Russian state media generated more than a bit of schadenfreude around the White House — one aide responded to the news by playing Russia’s national anthem from their computer speakers, according to a person nearby — given how Reade’s past pro-Putin and pro-Russian comments were cited by a number of reporters who were skeptical of her allegations. She said later that they were just “creative writing.” In an hours-long conversation with “Sputnik” that was livestreamed on Twitter to roughly 500 viewers, Reade said she fled to Moscow out of fear for her safety, claiming the Kremlin has been “accommodating” and apologizing for Washington’s “aggressive” stance against Putin’s invasion of Ukraine. We’re happy to report that Tara Reid, the actress who starred as Bunny Lebowski, remains a U.S. citizen. YOUR LOANS OR THE ECONOMY GETS IT: Part of the debt ceiling agreement Biden and House Speaker KEVIN MCCARTHY reached over the weekend would force the Biden administration to resume student loan payments, our MICHAEL STRATFORD reports. The deal would end the ongoing pause on monthly payments and interest after Aug. 30, though it won’t affect the administration’s larger student debt cancellation plan currently stuck in court. Our Congress team breaks down other curveballs in the agreement here. REMEMBERING BEAU: The president attended a memorial mass in Delaware on Tuesday to mark eight years since his son BEAU BIDEN died of brain cancer, AP’s DARLENE SUPERVILLE reports. The mass took place at St. Joseph on the Brandywine Roman Catholic church. BRE-NTER: U.K. Prime Minister RISHI SUNAK is set to visit Washington and meet with Biden on June 8, the White House announced Tuesday. The pair will discuss ways to strengthen U.S.-U.K. relations and how the nations will continue to support Ukraine as the Russian invasion persists. WHAT THE WHITE HOUSE WANTS YOU TO READ: This NYT opinion piece by BRIAN DEESE, the former director of the National Economic Council, about how the Inflation Reduction Act has spurred “significantly more investment in clean energy than was at first thought possible while generating more revenue from high-income taxpayers to reduce the deficit.” National Economic Council deputy director BHARAT RAMAMURTI shared the piece on Twitter. WHAT THE WHITE HOUSE DOESN’T WANT YOU TO READ: This NYT opinion piece featuring 11 voters who supported Biden in 2020, but are skeptical about his reelection bid: “For now, many of these 11 voters seemed inclined to back him as the lesser of two evils — a phrase we heard time and again during the conversation. Donnia, a 62-year-old Black independent from Illinois, said of Mr. Biden, ‘I think he may be slow and a camel and a sloth and all of that, but I think that he would be the better option than Trump or DeSantis — the G.O.P., period.’” PERSONNEL MOVES: BILL DOERRER has been named deputy chief of staff for the Office of the National Cyber Director, DANIEL LIPPMAN has learned. He currently is the White House liaison at the Treasury Department. RAYMOND PHAM has been named to succeed Doerrer at Treasury. He most recently was national coalitions finance director at the Democratic National Committee. RAINBOW LINE CROSSED: Secretary of State ANTONY BLINKEN said the administration is considering “visa restrictions against Ugandan officials and others for the abuse of human rights following the implementation of one of the world’s toughest anti-gay laws,” Reuters reports. Ugandan President YOWERI MUSEVENI signed the Anti-Homosexuality Act into law Monday, a move met with Biden calling for its “immediate repeal.” Our KIERRA FRAZIER has more on the White House’s response. GOING AFTER NON-COMPETES: JENNIFER ABRUZZO, the National Labor Relations Board’s General Counsel, wrote in a memo to staff Tuesday that most non-compete agreements violate federal labor law, “except in limited circumstances.” Abruzzo’s memo is the latest move from the Biden administration to rein in such contracts from employers, our NICK NIEDZWIADEK reports for Pro subscribers. NOT THAWING YET: China turned down a U.S. offer to host a “meeting between the countries’ defense chiefs, as the world’s two largest economies struggle to mend ties,” NBC News’ JENNIFER JETT and COURTNEY KUBE report. “The Pentagon had suggested a meeting between Defense Secretary Lloyd Austin and Chinese Defense Minister Li Shangfu on the sidelines of the Shangri-La Dialogue, an annual security forum being held in Singapore from June 2 to 4.” Opinion: 3 reasons we’re stuck with Trump and Biden (WaPo’s Megan McArdle) Drone Strikes Damage Buildings in Moscow as Kyiv Is Hit Again (NYT’s Anatoly Kurmanaev, Ivan Nechepurenko, Marc Santora and Victoria Kim) GET READY FOR GLOBAL TECH DAY: Join POLITICO Live as we launch our first Global Tech Day alongside London Tech Week on Thursday, June 15. Register now for continuing updates and to be a part of this momentous and program-packed day! From the blockchain, to AI, and autonomous vehicles, technology is changing how power is exercised around the world, so who will write the rules? REGISTER HERE. There are a lot of military movies out there. Gen. C.Q. BROWN, the Air Force’s top officer and Biden’s Joint Chiefs chair nominee, said his favorites are “The Longest Day,” a 1962 flick about the D-Day landings at Normandy, and the original “Top Gun,” featuring TOM CRUISE. “The Longest Day,” Brown explained during a 2021 National Press Club interview, was “one of the ones that I grew up on,” and “Top Gun” was a movie that “came out the year I graduated pilot training, and it was probably the most realistic flying movie that I’d seen.” He added that he’s also a big Spiderman fan, so “all the Spiderman movies are on my list as well.” We’re not so sure the Spiderman series can be classified as military movies, but good to know! President THOMAS JEFFERSON planned the large vegetable garden that included cabbage, broccoli, green and yellow Savoy, radish, endive, cucumber, carrot, beet, parsnip, turnip, and leek. It is unknown what happened to the garden after the 1814 fire that destroyed the White House and ruined the grounds, according to the White House Historical Association. A CALL OUT — Do you think you have a harder trivia question? Send us your best one about the presidents with a citation and we may feature it. Edited by Eun Kyung Kim and Sam Stein.  © 2023 POLITICO LLC",42,2296,0.363914668560028,0.323463529348373,0.312621742486954
"
White House
 While there are huge potential upsides with AI, the unanticipated bumps could be severe, especially amid the coming presidential election. 
The White House press shop has found itself on one of the many front lines of the AI battles. | Sarah Silbiger/Getty Images
 By Eli Stokols and Lauren Egan 
05/30/2023 06:07 PM EDT Link Copied When an image showing what looked to be a bombing at the Pentagon started to spread online last week, the stock market dipped momentarily. Kayla Tausche, who covers the White House for CNBC, quickly started fact checking. Popping into Lower Press — the cluster of desks and offices behind the briefing room where many press aides work — she found principal deputy press secretary Olivia Dalton and asked about the reports. “There was initially confusion about where it was coming from (I said ‘RT-style unconfirmed viral accounts’) and then exasperation,” Tausche told West Wing Playbook. Dalton moved quickly, connecting with the Pentagon and National Security Council before telling Tausche there did not appear to have been a bombing. Once additional tweets suggested the phony image had been generated by artificial intelligence, Tausche followed up with Dalton to apologize for the diversion. “She said, with visible frustration, that she is dealing with these types of inquiries on a daily basis, with greater and greater frequency,” Tausche added. The White House press shop has found itself on one of the many front lines of the AI battles. Aides there, who collectively handle hundreds of media inquiries a day, have already been briefed by experts on the potential national security risks posed by images and videos that have been altered using AI, according to an administration official. Outside the press shop, the White House has scaled up its efforts to assess and manage AI’s risks, impressing on AI companies during meetings on campus that it’s their responsibility to ensure their products are safe. It updated the strategic plan for AI research and development for the first time in four years and last week launched a process to work toward developing an AI bill of rights. “Everyone is trying very hard to be sensitive, to issue these warnings but without predicting what could happen, and that’s because they don’t know,” said Kara Swisher, a prominent tech-focused journalist. “Most people, if they’re being honest, would tell you they don’t know what’s going to happen.” The administration’s knockdown of reports of the Pentagon bombing — backed by a tweet from Arlington, Va., first responders — was part of a swift debunking that helped the market recover after the S&P fell 0.3 percent, a momentary loss of some $500 billion in value. But days later, another AI-generated deep fake popped up in the form of a video showing a purported Microsoft Teams call between anti-Russia activist Bill Browder and former Ukraine President Petro Poroshenko arguing for the easing of sanctions against Russian oligarchs. Both fakes were easy enough to spot for those familiar with AI. But as the technology develops and improves, AI-generated text, audio and video could quickly become indistinguishable from that produced by human beings. On Tuesday, prominent industry officials, including OpenAI CEO Sam Altman, issued a succinct but jarring statement aimed at seizing the attention of global leaders: “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” the statement said. When asked about the statement, White House press secretary Karine Jean-Pierre wouldn’t say if the president shares the belief that AI, if mismanaged, could lead to extinction. She only acknowledged that AI is “one of the most powerful technologies that we see currently in our time” and that the administration takes risk mitigation seriously. There are various proposals floating for regulating AI — and Big Tech more broadly — on Capitol Hill, including legislation released earlier this month by Sen. Michael Bennet (D-Colo.) to create a new federal agency to oversee the technology. “We remain concerned about an uptick in deepfake videos and manipulated images spreading on social media platforms,” said White House assistant press secretary Robyn Patterson. “As the technology for creating fake videos and images improves, it’s important for the media and the public to be aware of this trend, which we expect to grow, if not exponentially.” While there are huge potential upsides with AI that are already triggering a global arms race to harness and capitalize on the technology, the unanticipated bumps could be severe, especially amid the coming presidential election. “It’s not that one piece of content is going to be devastating; it’s the collective, scaled approach to inauthenticity that’s the problem. People can do this at scale now,” said Sarah Kreps, a professor at Cornell University’s Brooks School Tech Policy Institute and one of three AI researchers invited to speak to Biden’s new working group on the matter within the President’s Council of Advisors on Science and Technology. “It can look like massive numbers of citizens are supporting a particular issue when they’re not.” In a country where sectarian partisanship has already given rise to misinformation and the spread of conspiracy theories, AI may only deepen the public’s growing mistrust of facts. “It just creates this ecosystem of distrust in a democracy where trust is such a foundational pillar,” said Kreps. 
The power players, latest policy developments, and intriguing whispers percolating inside the West Wing.
 
The power players, latest policy developments, and intriguing whispers percolating inside the West Wing.
 
By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
 Loading You will now start receiving email updates You are already subscribed Something went wrong 
By signing up you agree to allow POLITICO to collect your user information and use it to better recommend content to you, send you email newsletters or updates from POLITICO, and share insights based on aggregated user information. You further agree to our privacy policy and terms of service. You can unsubscribe at any time and can contact us here. This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.
 Link Copied  © 2023 POLITICO LLC",10,1085,0.349816232919693,0.309329658746719,0.340854197740555
"
Your browser appears to be unsupported. Because of this, portions of the site may not function as intended.
 
Please install a current version of Chrome, Firefox, Edge, or Safari for a better experience.
 After years of progress toward health equity and poverty reduction, we are seeing a widening gap between countries and communities on nearly all metrics of global development. The development of new technologies and their application in resource-poor settings has been one way to address these gaps. It will take concerted action by governments, the private sector, philanthropies, and civil society to develop the solutions needed to regain footing on the Sustainable Development Goals that we use to track progress.    Since its inception, the Bill & Melinda Gates Foundation has focused, through its partnerships and grantmaking, on applying innovation to help accelerate progress, promote human potential, and address long-standing inequities. Working with our partners we continually scan the horizon for innovations, from drought-resistant seeds to mRNA vaccines, that can make an impact toward health and development goals.    By almost all estimations, rapid advances in artificial intelligence, or AI, will make AI technology broadly available and transformative across societies and economies, with the potential to fundamentally alter the way people communicate, work, learn, and improve their well-being. In the areas where the Gates Foundation works, it is easy to imagine powerful uses for AI in everything from speeding medical breakthroughs and addressing the impacts of climate change to boosting learning outcomes and increasing agricultural production for smallholder farmers. The foundation has invested in dozens of applications of machine learning to health and development use cases to date. More recently, the rapid development and availability of large language models, like ChatGPT, has fundamentally changed what may be possible using AI. Potential gains will only be realized if the technology is implemented with the beneficiaries participating in its development. The speed with which AI is being developed poses a moment of opportunity and risk—will their creation and application be inclusive, reflecting the needs of lower-income countries, or will they be applied inequitably and further the gaps we’ve seen?    Earlier advances in technology have delivered uneven benefits in many parts of the world for a variety of reasons, but lack of access to innovation is the primary reason people in low-resource settings often do not see benefits in a timely, fair, and consistent fashion. Like any new set of technologies, it is vital to approach the potential uses of AI with care and caution, particularly from the perspective of populations who have historically been left behind in realizing the benefits of innovations. It is also critical to acknowledge the risks associated with the spread of misinformation and the potential misuse of information facilitated by AI. In the realm of health and development, where accurate and reliable information is essential, the consequences of misinformation can be particularly damaging. Intentional design and collaboration with those who stand to benefit are critical to innovation having the desired impact, which we measure in lives saved and opportunities provided for people to achieve their full potential.    The use of AI for development requires specific considerations because of the nature of how and where large language models have been created, as well as the perceptions which accompany them. These models are built around the analysis of large data sets combined with algorithms and statistical models to find and utilize patterns. Marginalized communities have not had a seat at the table to inform the models, so the statistical representations in these models may be less accurate than for regions with better-represented data. This makes error rates and the risk of perpetuating pre-existing biases for AI utilizing such data significantly more likely. There is a real risk associated with AI tools if caution and oversight are not exercised. If not properly managed, these tools can inadvertently reinforce biases, perpetuate existing inequalities, and propagate misinformation. We must intentionally work to mitigate these limitations.    Recognizing both the potential benefits and risks of AI and its rapid development, in March 2023 the foundation established an internal Global AI Task Force to help serve as a coordinating mechanism to define the role of AI as it relates to the foundation’s work, recognizing the many facets of our work and the evolving nature of this technology. The task force was created to drive a responsible and organized approach to exploring the foundation’s engagement with AI usage and help map a way forward that is safe, ethical, and equitable. Additionally, the foundation is establishing an ethics and equity advisory committee drawn from outside experts which will share insights on program design, ethical AI practice, provide accountability within the organization and our grantees, and advise how best to guard against bias and unintended consequences.  Our focus on access and equity is fundamental to our current and future work with AI. We understand that introducing AI into resource-limited settings may present unique risks, and we are committed to mitigating these through continuous research, collaboration, and open communication with stakeholders. As the foundation engages in work that leverages the power of AI, we will be guided by a set of first principles that shape our initial approach and aligned with our core mission—to help create a world where every person has the opportunity to live a healthy, productive life. These principles will be refined and adapted as we engage with partners and other outside experts, as we learn based on experience, and as future developments in AI technology evolve.   Adhere to our core valuesThe foundation is guided by a belief that all people, no matter the circumstance into which they are born, should be able to live a healthy life and reach their full potential. Our approach to the use of AI technology is therefore grounded in the need to promote greater equity and opportunity for resource-poor communities. AI can be a useful tool in advancing these goals—if its downsides are properly managed.   Promote co-design and inclusivity Low-income countries must not just be seen as beneficiaries or end-users of AI but as essential collaborators and partners in program design and uses. This means sharing insights, concerns, and information across organizations and geographies to drive AI use that is fit for purpose and contextually sensitive. We will approach the use of AI collaboratively and recognize that effective partnership must be intentional and inclusive. Acknowledging the limited availability of digital infrastructure in LMICs, maximizing the benefits of AI in these regions may present additional challenges. In keeping with our overall approach to innovation, we will invest in developing an evidence base for the responsible use of AI with, and for, communities and populations that stand to benefit from them.  Proceed responsibly We view our role to help ensure equitable use and access to AI tools. We acknowledge that proceeding responsibly requires an approach centered on compliance, inclusivity, and continuous improvement. To achieve this, we will leverage established legal regulations, industry standards, and ethical guidelines to navigate the complex landscape of AI applications for health and development uses. We will proceed in a step-wise fashion, starting with a confined set of use cases and gradually scaling up as the evidence base is built out.  Address privacy and security Privacy and security are essential when it comes to the use of AI, particularly given that it will likely increasingly be used in situations that involve sensitive personal information. It will be important to regularly conduct privacy and security assessments, ensure compliance with relevant regulations including data protection laws, implement transparency and accountability measures, and continually engage with stakeholders to improve systems. It will also be vital to ensure such practical measures have been taken in advance of collecting sensitive data based on informed consent, opt-out measures are provided, and materials are shared in appropriate local languages.   Build for equitable access Amid a rapid AI transformation, there are important questions on its equitable access, use, and addressing systemic inequity. A commitment to equitable access is not just about distribution but also about ownership, maintenance, and support for AI uses within the development context.   Ensure transparency  Given the potential for companies to commercialize the use of AI tools, we understand the importance of approaching this work with transparency. All of our grants are a matter of public record. And we adhere to a conflict-of-interest policy that guides all our work. Data should be shared to the greatest extent possible as a public good to allow for continual testing, improvement, and innovation.   These are early days in the development of AI and its uses to better human health and development. We recognize the inherent risks involved with AI and the need to address them responsibly. We also look forward to exploring the opportunities to harness this revolution to overcome challenges that to date have been intractable. As we have done with emerging technologies and platforms like mRNA for vaccines and therapeutics, genetic sequencing for disease identification and tracking, and other transformational tools, we are engaged early and to bend the curve towards equity. We will have much more to say about this topic here and across foundation channels.  By submitting your email to subscribe, you agree to the Bill & Melinda Gates Foundation's Privacy & Cookies Notice By submitting your email to subscribe, you agree to the Bill & Melinda Gates Foundation's Privacy & Cookies Notice",62,1551,0.34327095746994,0.313217490911484,0.343511492013931
"(Bloomberg) -- When Asia’s biggest computing and electronics show kicked off this week, one name was conspicuously absent: Intel Corp. Most Read from Bloomberg China Is Drilling a 10,000-Meter-Deep Hole Into Earth’s Crust Winklevoss Twins Attempt Pivot After Gemini Loses Money and Employees Twitter Is Now Worth Just 33% of Elon Musk’s Purchase Price, Fidelity Says JPMorgan Builds Unit for World’s Richest Families in Wealth Bet McCarthy Confident on Debt Vote Despite Hard-Line Ouster Threat The US chipmaker does not have an official presence at this year’s Computex exhibition, an event it once dominated through keynote speeches and an outsized footprint on the show floor. The 2023 edition opened instead with a marathon two-hour showcase by Nvidia Corp. Chief Executive Officer Jensen Huang, a day before it became the first $1 trillion chipmaker in anticipation of a wave of demand for its artificial intelligence processors. Intel, long synonymous with the fastest processors in the world, has seen its all-purpose chips overshadowed by Nvidia’s more specialized, parallel-processing architecture that’s superior at speeding up AI workloads. Intel rivals that hosted the other big keynote events included Qualcomm Inc., MediaTek Inc. and Arm Ltd. CEO Rene Haas — a former Nvidia executive — all of which talked up their AI credentials. Nvidia cast a long shadow across the exhibition floor. Super Micro Computer Inc. showed off data center racks with multiple Nvidia H100 accelerators, Asustek Computer Inc. is readying a subscription-based generative AI service built around Nvidia’s A100 chips, and Quanta Computer Inc. is touting new servers that support the Nvidia Grace Hopper superchip. In contrast, Intel’s presence was far diminished from previous years. The lifeblood of the Computex show are the smaller device assemblers and system integrators, companies that a decade earlier would likely have revolved around Intel. For this year at least, Intel opted to retreat from the Computex spotlight and conduct its business behind the scenes. CEO Pat Gelsinger hosted an invitation-only Intel Vision event in Taipei last week. The company is also supporting partners like Asus at their Computex booths. Its most public appearance will be an executive joining a sustainability forum. Santa Clara, California-based Intel, whose headquarters are only a brief walk away from Nvidia’s and those of rival chipmaker Advanced Micro Devices Inc., is in a transitional period under Gelsinger’s leadership. The company recently launched its first graphics cards and is working to make up lost ground on the AI front and advanced chip fabrication. Nvidia’s Huang may become a customer of Intel’s in the future. Asked about diversifying his company’s supply chain beyond main partner Taiwan Semiconductor Manufacturing Co., the executive on Tuesday said his company is evaluating Intel’s next-gen chipmaking processes and they looked promising. Most Read from Bloomberg Businessweek Assault Allegations Plague a $1.4 Billion Home Eldercare Startup Republicans Hail Big Spending Cuts That Economists Say Will Have Little Impact on the Economy Sergey Brin Has a Secret Plan to Put Airships Back in the Skies Opioid Crisis Spurred by Animal Sedative’s Stealth Emergence Adidas After Yeezy ©2023 Bloomberg L.P. Related Quotes Rivals such as Nvidia have left the chip company far behind. CEO Pat Gelsinger aims to reverse firm’s fortunes by vastly expanding its factories. Intel recorded its largest quarterly loss earlier this year. There's more pain to come—and it isn't just about the supply glut in semiconductors. (Bloomberg) -- Xiaomi Corp. is handing some of its smartphone assembly in India to Dixon Technologies India Ltd., deepening its relationship with the homegrown partner as the country’s government pushes electronics manufacturers to build local supply chains.Most Read from BloombergChina Is Drilling a 10,000-Meter-Deep Hole Into Earth’s CrustWinklevoss Twins Attempt Pivot After Gemini Loses Money and EmployeesTwitter Is Now Worth Just 33% of Elon Musk’s Purchase Price, Fidelity SaysJPMorgan Build Nvidia debuted a host of new AI technologies on Monday, as the chip giant's stock price soared on a positive second quarter outlook. Intel Corp (NASDAQ: INTC) stock is up Tuesday in sympathy with Nvidia Corp's (NASDAQ: NVDA) AI-driven rally. Over the past decade, Nvidia beat Intel to become America's most valuable semiconductor company. Advanced Micro Devices, Inc (NASDAQ: AMD) stole market share while Intel struggled with repeated delays in launching new chips. Interestingly, Intel CEO Pat Gelsinger flagged severe issues regarding leadership, people, methodology, and more for the chipmaker's fall from the leading chipmaker p Lightmatter, a Boston-based startup using light for AI computing, said on Wednesday it raised $154 million and tripled its valuation as customers in need of faster, more energy efficient computing for AI work are buying its systems.  ""We use light to link computer chips together and we also use light to do calculations for deep learning,"" Lightmatter co-founder and CEO Nick Harris said.  ""The reason that we're getting these customers and data center scale deployments with our interconnect is that the generative AI boom is driving high-end chips like crazy."" A purported new ""home-grown"" chip introduced by a Chinese computer hardware producer this month is suspected to be a rebadged integrated circuit (IC) from Intel Corp, according to a report by online tech news site Tom's Hardware, based on the results of a central processing unit (CPU) benchmark testing via online cross-platform utility Geekbench. Testing site Geekbench, a platform run by Canadian software developer Primate Labs, published the key parameters of Shenzhen-based Powerleader's Powers Computer Software industry participants like Synopsys (SNPS), Descartes Systems Group (DSGX) and Blackbaud (BLKB) benefit from steady digital transformations and strong adoption of cloud computing, despite macroeconomic turbulence. VZ stock provides a dividend but a buyback has been shelved amid 5G wireless investments. When will revenue growth reaccelerate? Cathie Wood’s ARK Innovation Fund (NYSEARCA:ARKK) bills itself as investing in “disruptive innovation.” Unfortunately for ARKK ETF holders, it seems that ARKK missed out on at least the early stages of the rise of the most innovative and disruptive technological advance in recent times — artificial intelligence (AI) and generative AI, in particular. Why Generative AI Should be in a Disruptive Innovation Fund It’s hard to argue that the rise of AI, and specifically generative AI, isn’t the most d Selling specialist chips and infrastructure is becoming a trillion-dollar industry Volumes surged from just over $1 million at the start of May to $18 million on Tuesday, data from Minswap shows. Hon Hai Precision Industry Co Ltd (OTC: HNHPF), operating as Foxconn, forecasts its AI server business to at least double in the second half of this year, taking a cue from Nvidia Corp (NASDAQ: NVDA). The critical Apple Inc (NASDAQ: AAPL) supplier expected up to triple-digit growth in AI servers in the second half, Financial Times cites chair Young Liu. Liu said the ChatGPT frenzy and increasing reliance were driving the boost in demand. Foxconn also makes many other electronic products and comp The news was quietly announced in April. The A.I. frenzy has boosted Nvidia's business due to soaring chips demand. Now, CEO Jensen Huang thinks everyone can benefit from the boom. Uber Technologies, Inc (NYSE: UBER) dropped the 5% discounts on eligible rides for its Uber One subscription service. Starting their next billing cycle, subscribers will earn 6% of so-called “Uber Cash” on eligible rides that users can spend on Uber and Uber Eats, TechCrunch cites from an email the ride-hailing company sent out to customers. In 2021, the Uber One subscription for $9.99 per month or $99.99 annually, which offered rides discounts, has been a significant demand driver. Uber One mem Garmin has just announced a pair of new smartwatches intended for explorers and extreme fitness enthusiasts. However, these are high-end gadgets with high-end specs and, accordingly, high-end price tags. The Epix Pro series features an AMOLED display and the Fenix 7 Pro series includes solar charging and weather mapping upgrades. T-Mobile holds an edge in 5G wireless spectrum but will its market share gains vs. rivals continue? A big stock buyback is underway. Nokia (NOK) introduces new 4G and 5G core network software solutions tailored to the needs of mission-critical operations. When searching for the best stocks to buy and watch, look for companies with strong earnings growth – and check the earnings calendar to see when they report. Be sure to also see our latest list of stocks  just added to — or removed from — IBD's top screens.",49,1378,0.375934302806854,0.295708268880844,0.328357368707657
"[1/6] A Foxconn shareholder poses for photos after the annual shareholder meeting in New Taipei City, Taiwan May 31, 2023. REUTERS/Ann Wang TAIPEI, May 31 (Reuters) - Apple Inc (AAPL.O) supplier Foxconn (2317.TW) said on Wednesday artificial intelligence applications would strongly drive demand for its server business this year but reiterated 2023 overall would be a flat one for the company on global economic woes. Foxconn Chairman Liu Young-way told the company's annual shareholders meeting the firm remained cautious about this year due to monetary policy tightening, geopolitical tensions and uncertainty over inflation, but servers were a bright spot due to surging interest in AI. ""More and more people are using ChatGPT,"" he said. ""You can see the market for AI servers will rise much faster than expected. We expect that in the second half of this year there may be a three digit increase."" The Taiwanese company has a 40% global market share for servers and aims to further increase that, Liu added. In the first quarter, Foxconn's cloud and network products segment, which includes servers, accounted for 22% of revenue, second only to smart consumer electronics - which includes smartphones - at 56%. Foxconn this month posted a 56% plunge in first-quarter net profit, lagging forecasts in its biggest quarterly fall in three years, and said visibility for the full year was ""limited"". The company, the world's largest contract electronics maker, wants to replicate the success it has had with Apple's iPhone with electric vehicles (EV). Foxconn, formally called Hon Hai Precision Industry Co Ltd, has acquired the former General Motor Co (GM.N) plant in Lordstown, Ohio, and has also hired a former Nissan (7201.T) executive, Jun Seki, to lead expansion efforts in EVs, where it hopes to become a major manufacturer. The company is considering expanding its EV battery supply chain beyond Taiwan, possibly into the United States, Indonesia and India, Liu said. Foxconn, which assembles around 70% of iPhones, has been diversifying production away from China, whose strict COVID-19 restrictions disrupted its biggest iPhone plant last year. The company is also seeking to avoid a potential hit to its business from mounting trade tensions between Beijing and Washington. Liu said China, including its massive iPhone plant in China's Zhengzhou, remained very important for Foxconn. ""The culture there is very similar, our rules and regulations are a bit different, but there is no problem when it comes to talent. So it's relatively easier for us to start new undertakings there. We'll work hard to keep developing there."" Our Standards: The Thomson Reuters Trust Principles. Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",38,620,0.334408581256866,0.322419047355652,0.343172401189804
"[1/6] A green wireframe model covers an actor's lower face during the creation of a synthetic facial reanimation video, known alternatively as a deepfake, in London, Britain February 12, 2019. Reuters TV via REUTERS May 30 (Reuters) - (Note: Strong language in paragraph 10) ""I actually like Ron DeSantis a lot,"" Hillary Clinton reveals in a surprise online endorsement video. ""He's just the kind of guy this country needs, and I really mean that."" Joe Biden finally lets the mask slip, unleashing a cruel rant at a transgender person. ""You will never be a real woman,"" the president snarls. Welcome to America's 2024 presidential race, where reality is up for grabs. The Clinton and Biden deepfakes - realistic yet fabricated videos created by AI algorithms trained on copious online footage - are among thousands surfacing on social media, blurring fact and fiction in the polarized world of U.S. politics. While such synthetic media has been around for several years, it's been turbocharged over the past year by of a slew of new ""generative AI"" tools such as Midjourney that make it cheap and easy to create convincing deepfakes, according to Reuters interviews with about two dozen specialists in fields including AI, online misinformation and political activism. ""It's going to be very difficult for voters to distinguish the real from the fake. And you could just imagine how either Trump supporters or Biden supporters could use this technology to make the opponent look bad,"" said Darrell West, senior fellow at the Brookings Institution's Center for Technology Innovation. ""There could be things that drop right before the election that nobody has a chance to take down."" Tools that can generate deepfakes are being released with few or imperfect guardrails to prevent harmful misinformation as the tech sector engages in an AI arms race, said Aza Raskin, co-founder of the Center for Human Technology, a nonprofit that studies technology's impact on society. Former President Donald Trump, who will vie with DeSantis and others for the Republican nomination to face Biden, himself shared a doctored video of CNN anchor Anderson Cooper earlier this month on his social media platform Truth Social. ""That was President Donald J. Trump ripping us a new asshole here on CNN's live presidential townhall,"" Cooper says in the footage, although the words don't match his lip movement. CNN said the video was a deepfake. A representative for Trump didn't respond to a request for comment on the clip, which was still on his son Donald Jr's Twitter page this week. While major social media platforms like Facebook, Twitter, and YouTube have made efforts to prohibit and remove deepfakes, their effectiveness at policing such content varies. There have been three times as many video deepfakes of all kinds and eight times as many voice deepfakes posted online this year compared to the same time period in 2022, according to DeepMedia, a company working on tools to detect synthetic media. In total, about 500,000 video and voice deepfakes will be shared on social media sites globally in 2023, DeepMedia estimates. Cloning a voice used to cost $10,000 in server and AI-training costs up until late last year, but now startups offer it for a few dollars, it says. No one is certain where the generative AI road leads or how to effectively guard against its power for mass misinformation, according to the people interviewed. Industry leader OpenAI, which has changed the game in recent months with its release of ChatGPT and the updated model GPT-4, is itself grappling with the issue. CEO Sam Altman told Congress this month that election integrity was a ""significant area of concern"" and urged rapid regulation of the sector. Unlike some smaller startups, OpenAI has taken steps to restrict use of its products in politics, according to a Reuters analysis of the terms of use of half a dozen leading companies offering generative-AI services. The guardrails have gaps, though. For example, OpenAI says it prohibits its image generator DALL-E from creating public figures - and indeed, when Reuters tried to create images of Trump and Biden, the request was blocked and a message appeared saying it ""may not follow our content policy."" Yet Reuters was able to create images of at least a dozen other U.S. politicians, including former Vice-President Mike Pence, who is also weighing a White House run for 2024. OpenAI also restricts any ""scaled"" usage of its products for political purposes. That bans use of its AI to send out mass personalized emails to constituents, for example. The company, which is backed by Microsoft, explained its political policies to Reuters in an interview but didn't respond to further requests for comment around enforcement gaps in its policies, such as blocking image creation of politicians. Several smaller startups have no explicit restrictions on political content. Midjourney, which launched last year, is the leading player in AI-generated images, with 16 million users on its official Discord server. The app, which ranges from free to $60 a month depending on factors such as picture quantity and speed, is a favorite of AI designers and artists due to its ability to generate hyper-realistic images of celebrities and politicians, according to four AI researchers and creators interviewed. Midjourney didn't respond to a request for comment for this article. During an online chat on Discord last week, CEO David Holz said the company would likely make changes ahead of the election to combat misinformation. Midjourney wants to cooperate on an industry solution to enable traceability of AI-generated images with a digital equivalent of watermarking and would consider blocking images of political candidates, Holz added. Even as the industry wrestles with how to prevent misuse, some political players are themselves seeking to harness the power of generative AI to soup up campaigns. So far, the only high-profile AI-generated political ad in the U.S. was one published by the Republican National Committee in late April. The 30-second ad, which the RNC disclosed as being entirely generated by AI, used fake images to suggest a cataclysmic scenario should Biden be reelected, with China invading Taiwan and San Francisco being shut down by crime. The RNC didn't respond to requests for comment on the ad or its wider use of AI. The Democratic National Committee declined to comment on its use of the technology. Reuters polled all the Republican presidential campaigns on their use of AI. Most did not reply, although Nikki Haley's team said they were not using the technology and longshot candidate Perry Johnson's campaign said it was using AI for ""copy generation and iteration,"" without giving further details. The potential for generative AI to produce campaign emails, posts and adverts is irresistible for some activists who feel the low-cost tech could level the playing field in elections. Even deep in rural Hillsdale, Michigan, machine intelligence is on the march. Jon Smith, Republican chair for Michigan's 5th Congressional district, is holding several educational meetings so his allies can learn to use AI for social media and ad generation. ""AI helps us play against the big cats,"" he said. ""I see the biggest upswing in the local races. Someone who is 65 years old, a farmer and county commissioner, he could easily be primaried by a younger cat using the technology."" Political consultancies are also seeking to harness AI, further muddying the line between real and unreal. Numinar Analytics, a political data company that focuses on Republican clients, has begun experimenting with AI content generation for audio and images, as well as voice-generation to potentially create personalized messaging in a candidate's voice, founder Will Long said in an interview. Democratic polling and strategy group Honan Strategy Group is meanwhile trying to develop an AI survey bot. It hopes to unroll a female bot in time for the 2023 municipal elections, CEO Bradley Honan said, citing research that both men and women are more likely to speak to a female interviewer. Our Standards: The Thomson Reuters Trust Principles. Thomson Reuters U.S. National Affairs Correspondent who spent four years in Venezuela covering President Maduro's administration and the humanitarian crisis and has also reported from Chile, Argentina and India. She was Reuters’ Reporter of the Year in 2015 and a leading member of a team that won an Overseas Press Club award for best coverage of Latin America in 2018. Thomson Reuters Anna Tong is a correspondent for Reuters based in San Francisco, where she reports on the technology industry. She joined Reuters in 2023 after working at the San Francisco Standard as a data editor. Tong previously worked at technology startups as a product manager and at Google where she worked in user insights and helped run a call center. Tong graduated from Harvard University.
Contact:4152373211 New anti-terrorism legislation which comes into force this week should pave the way for Sweden to join NATO in coming weeks and overcome a Turkish veto, Swedish Foreign Minister Tobias Billstrom said on Wednesday. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",44,1650,0.373461157083511,0.314755171537399,0.311783701181412
"A security firm that sells AI weapons scanners to schools is facing fresh questions about its technology after a student was attacked with a knife that the $3.7m system failed to detect. On Halloween last year, student Ehni Ler Htoo was walking in the corridor of his school in Utica, New York, when another student walked up behind him and stabbed him with a knife. Speaking exclusively to the BBC, the victim's lawyer said the 18-year-old suffered multiple stab wounds to his head, neck, face, shoulder, back and hand.  The knife used in the attack was brought into Proctor High School despite a multimillion weapons detection system installed by a company called Evolv Technology. Evolv Technology is a security firm that wants to replace traditional metal detectors with AI weapons scanners.  Instead of simply detecting metal, Evolv says its scanner ""combines powerful sensor technology with proven artificial intelligence"" to detect weapons.  When the system detects a concealed weapon - like knives, bombs or guns - it triggers an alert.  The company has publicly stated their system is highly accurate, and previously boasted its scanners can help to create ""weapons-free zones"". The company's chief executive, Peter George, has also said that its systems ""have the signatures for all the weapons that are out there"". Previous press releases have listed the weapons the system can find - which include firearms, explosive devices, and knives. However, a BBC investigation last year revealed that testing had found the system could not reliably detect large knives - after Evolv's scanner missed 42% of large knives in 24 walk-throughs.  The system is used in major stadiums across the US, and the Manchester Arena in the UK. The testers said Evolv should inform potential clients. Despite this, the company has been expanding into schools, and now claims to be in hundreds of them across the US.  In March 2022, the Utica Schools Board bought Evolv's weapons scanning system for 13 schools. It was installed over the summer holidays. On 31 October, CCTV captured the perpetrator of the attack against Ehni Ler Htoo entering Proctor High School and passing through the Evolv weapons scanners, according to one source at the school who has seen the security footage.  ""When we viewed the horrific video, we all asked the same question. How did the student get the knife into the school?"" said Brian Nolan, Superintendent of Utica Schools. The knife used in the stabbing was more than 9in (22.8cm) long.  The attack triggered an internal investigation by Utica's school district.  ""Through investigation it was determined the Evolv Weapon Detection System… was not designed to detect knives,"" Mr Nolan said. The scanners were removed from Proctor High School and replaced by 10 metal detectors. But the scanners are still operating in the district's remaining 12 schools. Mr Nolan says the district cannot afford to get rid of Evolv's system in its remaining schools.  Since that attack, Mr Nolan says three other knives have been found on students in other schools in the district where the Evolv systems continue to operate.  One of the knives was 7in long. Another was a curved blade with finger holes. Another was a pocket knife. Mr Nolan says they were all found because they were reported to staff - not because the weapons scanner had detected them.  ""The kids [who had the knives] all said they walked right through the weapons detection system, we asked them about that… it truly, truly does not find knives,"" he said. After the stabbing, the wording on Evolv's website changed.  Up until October last year, Evolv's homepage featured a headline that boasted of ""Weapons-Free Zones"". The company then removed that wording, and changed the text  to ""Safe Zones"". It has now been changed again and reads ""Safer Zones"".  Evolv claims its system uses cutting-edge AI technology to find weapons. However, its critics say not enough is known about how the system works - or how effective this technology is at finding different types of weapons. The BBC sent a detailed right of reply to Evolv, laying out what had happened at the school in Utica, and the decision of the school to stop using its system.  We also asked what Evolv had told schools about what its system could and could not detect, whether it had told schools that independent testing had found its systems could not reliably detect large knives, and whether it thought its systems were suitable for use in schools. Evolv did not answer the questions. Conor Healy of IPVM, a firm that analyses security equipment, says Evolv has exaggerated how effective the system is. ""There's an epidemic of schools buying new technology based on audacious marketing claims, then finding out it has hidden flaws, often millions of dollars later. Evolv is one of the worst offenders. School officials are not technical experts on weapons detection, and companies like Evolv profit from their ignorance."" Playing fast and loose with marketing claims is unacceptable when you sell a security product used to protect young people, he added. Although Evolv did not give the BBC a comment, it did direct it to a blog post from its CEO, Peter George, in which he defends the lack of detail in how much the firm has said about how the technology works.  ""Marketing weapons detection security requires a delicate balance between educating stakeholders on new technology and not providing bad actors with the information they could use to do harm,"" he said.  ""So, while public-facing marketing materials are intentionally not specific, we communicate all aspects of the Evolv Express system - including limitations and capabilities - with the trusted security professionals at our customers, partners, and prospects"", the blogpost read.  The BBC contacted seven other school districts that all use Evolv weapons scanners. Five did not respond. Two said they did not wish to comment. Ukraine mothers go behind enemy lines to save stolen children Why South Korea woke up to sirens... in 73 seconds. VideoWhy South Korea woke up to sirens... in 73 seconds AI 'godfather' feels 'lost' over life's work Travelling into enemy land to save my stolen child A top soldier's defamation case rocks Australia New Yorkers gather for Manhattanhenge The Myanmar soldiers refusing to fight Could a US debt default unleash global chaos? Going to the city of my mother's dreams After a synagogue shooting, can a community heal? The 'exploding' demand for giant heat pumps Holmes has gone to jail. Will she pay victims too? Why it's 'imperative' to start using AI Jellyfish blooms: Why not just eat them? A 5,000-year-old craft under threat © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.",84,1137,0.356816470623016,0.314130127429962,0.329053461551666
"Advertisement Supported by Mind Teaching fewer words to large language models might help them sound more human. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Oliver Whang When it comes to artificial intelligence chatbots, bigger is typically better. Large language models like ChatGPT and Bard, which generate conversational, original text, improve as they are fed more data. Every day, bloggers take to the internet to explain how the latest advances — an app that summarizes‌ ‌articles, A.I.-generated podcasts, a fine-tuned model that can answer any question related to professional basketball — will “change everything.” But making bigger and more capable A.I. requires processing power that few companies possess, and there is growing concern that a small group, including Google, Meta, OpenAI and Microsoft, will exercise near-total control over the technology. Also, bigger language models are harder to understand. They are often described as “black boxes,” even by the people who design them, and leading figures in the field have expressed ‌unease ‌that ‌A.I.’s goals may ultimately not align with our own. If bigger is better, it is also more opaque and more exclusive. In January, a group of young academics working in natural language processing — the branch of A.I. focused on linguistic understanding — issued a challenge to try to turn this paradigm on its head. The group called for teams to create functional language models ‌using data sets that are less than one-ten-thousandth the size of those used by the most advanced large language models. A successful mini-model would be nearly as capable as the high-end models but much smaller, more accessible and ‌more compatible with humans. The project is called the BabyLM Challenge. “We’re challenging people to think small and focus more on building efficient systems that way more people can use,” said Aaron Mueller, a computer scientist at Johns Hopkins University and an organizer of BabyLM. Alex Warstadt, a computer scientist at ETH Zurich and another organizer of the project, added, “The challenge puts questions about human language learning, rather than ‘How big can we make our models?’ at the center of the conversation.” Large language models are neural networks designed to predict the next word in a given sentence or phrase. They are trained for this task using a corpus of words collected from transcripts, websites, novels and newspapers. A typical model makes guesses based on example phrases and then adjusts itself depending on how close it gets to the right answer. By repeating this process over and over, a model forms maps of how words relate to one another. In general, the more words a model is trained on, the better it will become; every phrase provides the model with context, and more context translates to a more detailed impression of what each word means. OpenAI’s GPT-3, released in 2020, was trained on 200 billion words; DeepMind’s Chinchilla, released in 2022, was trained on a trillion. To Ethan Wilcox, a linguist at ETH Zurich, the fact that something nonhuman can generate language presents an exciting opportunity: Could A.I. language models be used to study how humans learn language? For instance, nativism, an influential theory tracing back to Noam Chomsky’s early work, claims that humans learn language quickly and efficiently because ‌they have an innate understanding of how language works. But language models learn language quickly, too, and seemingly without an innate understanding of how language works — so maybe nativism doesn’t hold water. The challenge is that language models learn very differently from humans. Humans have bodies, social lives and rich sensations. We can smell mulch, feel the vanes of feathers, bump into doors and taste peppermints. Early on, we are exposed to simple spoken words and syntaxes that are often not represented in writing. So, Dr. Wilcox concluded, a computer that produces language after being trained on gazillions of written words can tell us only so much about our own linguistic process. But if a language model were exposed only to words that a young human encounters, it might interact with language in ways that could address certain questions we have about our own abilities. So, together with a half-dozen ‌colleagues, Dr. Wilcox, Dr. Mueller and Dr. Warstadt conceived of the BabyLM Challenge, to try to nudge language models slightly closer to human understanding. In January, they sent out a call for teams to train language models on the same number of words that a 13‌-year-old human ‌encounters — roughly 100 million. Candidate models would be ‌tested on how well they ‌generated and picked up the nuances of language, and a winner would be declared. Eva Portelance, a linguist at McGill University, came across the challenge the day it was announced. Her research straddles the often blurry line between computer science and linguistics. The first forays into A.I., in the 1950s, were driven by the desire to model human cognitive capacities in computers; the basic unit of information processing in A.I. is ‌the‌ ‌ “neuron‌,” and early language models in the 1980s and ’90s were directly inspired by the human brain. ‌ But as processors grew more powerful, and companies started working toward marketable products, ‌computer scientists realized that it was often easier to train language models on enormous amounts of data than to force them into psychologically informed structures. As a result, Dr. Portelance said, “‌they give us text that’s humanlike, but there’s no connection between us and how they function‌.”‌ For scientists interested in understanding how the human mind works, these large models offer limited insight. And because they require ‌tremendous processing power, few researchers can access them. “Only a small number of industry labs with huge resources can afford to train models with billions of parameters on trillions of words,” ‌Dr. Wilcox said. “Or even to load them,” Dr. Mueller added. “This has made research in the field feel slightly less democratic lately.” The BabyLM Challenge, Dr. Portelance said, could be seen as a step away from the arms race for bigger language models, and a step toward more accessible, more intuitive A.I. The potential of such a research program has not been ignored by bigger industry labs. Sam Altman, the chief executive of OpenAI, recently said that increasing the size of language models would not lead to the same kind of improvements seen over the past few years. And companies like Google and Meta have also been investing in research into more efficient language models, informed by human cognitive structures. After all, a model that can generate language when trained on less data could potentially be scaled up, too. Whatever profits a successful BabyLM might hold, for those behind the challenge, the goals are more academic and abstract. Even the prize subverts the practical. “Just pride,” Dr. Wilcox said. Oliver Whang is a reporting fellow for The Times, focusing on science and health.  @oliverwhang21  Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",17,1309,0.341720581054688,0.310405880212784,0.347873538732529
"Advertisement Supported by The financial services industry is plotting how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Paulette Perhach Paul Weiner, an artist, has been experimenting with artificial intelligence for the past year, generating A.I.-created visual disinformation and seeing whether he can get the images to spread. But recently, he turned to ChatGPT, a chatbot that has the ability to respond to complex questions, for a much different reason: With his 30th birthday looming, he decided to ask it for advice about retirement planning. “Maybe ChatGPT would have some answers that I might otherwise get from someone who I’d have to pay a lot of money to,” he said. Generative A.I. like ChatGPT has knowledge workers gripping the rails, bracing for how it might affect their jobs, and consumers leaning in to see what costly services could soon be replaced with a prompt. As the investment industry turns to artificial intelligence as a financial planning and advice tool, the values of accuracy, humanity, security and accessibility are jostling for prominence. In the future, who — or what — will we be asking to advise us on some of life’s most important decisions? ChatGPT recommended that Mr. Weiner open a Roth individual retirement account and certificates of deposit, as well as automate his savings and create a budget. He hasn’t yet opened any of the accounts or, as the chatbot also suggested, worked with a financial adviser. “It’s a lot of information that gets thrown at you pretty quickly,” Mr. Weiner said. He found the short explanations insufficient for what a C.D. does or the differences between a Roth I.R.A. and a traditional I.R.A. He concluded that speaking to a financial adviser would probably be more helpful. “But that kind of circles back to the whole reason I’m doing this on ChatGPT to start with — it’s free,” he said. Delyanne Barros, a money coach, said she felt that most of the hundreds of thousands of people who follow her on social media had no idea what ChatGPT is. “Am I the only one geeking out on this thing?” she asked. When she asks her followers if they’ve used it, she said, “they’re like, ‘What are you talking about?’” She’s teaching them the basics: There’s a free version of the service, and it works as more than just a Google alternative. On Instagram, she asked if any investing newbies had asked ChatGPT to teach them to invest. Some had tried but reported that they kept getting stuck in a loop of repetitive answers. Ms. Barros found that she was able to get valuable information about allocations, tax efficiencies and retirement withdrawal rates, but she posits that was because she had knowledge of the investment terms she needed to use. “You have to know how to frame the questions,” she said. “A lot of people don’t understand that you get an answer to something and it can build on that answer. You can ask follow-up questions, and it’s like a chain.” Ms. Barros has also used ChatGPT to double-check her calculations regarding her retirement plan. Despite its handiness, she is not worried that chatbots will replace her. “With something like investing, I’m not concerned as a personal finance educator, because I can see that it’s not like: ‘Oh, we don’t need you anymore. We have ChatGPT,’” she said. “If anything, this is going to be a tool that’s going to enhance my coaching experience with people, but it’s definitely not going to be replacing us, because people still need a lot of guidance.” Even if you don’t think you’re familiar with it, chances are you’ve already been using generative A.I. Intuit started to integrate A.I. into its software products, which include Mint and TurboTax, more than a decade ago, said Ashok Srivastava, the company’s senior vice president and chief data officer. Today, he said, Intuit’s platform performs 58 billion machine learning predictions per day. Another Intuit product, QuickBooks, predicts cash flow for small businesses, and the company has found that when it gives users advice based on artificial intelligence, 95 percent of small-business owners take that advice. They’re still focusing on a strategy that combines human interactions with A.I.-powered ones. Customers, for example, can meet with a live expert, and then A.I. will create a categorized and tagged summary of the conversation for later review. As of now, the technology is promising, but it’s not 100 percent accurate. “These systems tell plausible stories, they give you plausible ideas, but not necessarily correct ones,” Mr. Srivastava said. “What we’re focusing on is actually providing the correct experience to the person, so that it’s grounded in reality and data that is appropriately personalized to them, so then they can make the best financial decisions as they move forward.” Mr. Srivastava said he did not envision a future where humans were taken out of the financial planning equation. “I’ve grown up in the field, I’ve seen it evolve, and it’s an amazing technology,” he said. “I think that the human connection is still important. I envision that we will want to help C.P.A.s, bookkeepers, financial planners, financial advisers — everyone in this ecosystem — grow and prosper along with the use of artificial intelligence.” Josh Pigford, the founder and chief executive of Maybe, had been building a personal finance management platform that could help people make financial decisions when ChatGPT debuted. A few months ago, Maybe was rebuilt from the ground up, this time with GPT, the technology behind ChatGPT, as the foundation of the platform. The process always begins, he said, with a question people want to answer. “The way that we were initially tackling this is giving you access to a financial adviser who can answer those questions for you directly,” Mr. Pigford said. “As we started testing GPT’s ability around that, we realized, well, OK, actually GPT can do this really well.” Things became even more interesting when people added their financial data and information, such as age, location, and goals. The system could then take into account everything from dependents to joint filing to local tax codes — details a financial adviser would be able to use — and deliver that directly to the consumer. That, of course, brings up the subject of privacy. Through Maybe’s system, the banking information is secured and does not feed back to OpenAI, the company that created ChatGPT. Hallucinations — the tendency for ChatGPT to spout off incorrect information — have also become a worry. Mr. Pigford and his team identified the issue during early testing. “There was a point there where it was actually making up entire transactions, and building this back story of like, ‘You bought this item from Home Depot to help cool off your living room,’” he said. “That’s a legitimate problem.” As the technology has improved, Mr. Pigford has seen a drastic decrease in these hallucinations in just weeks. The way they’re designing the software includes a toggle to switch between a chatbot and humans for advice. “The belief, the hypothesis, what we’re sort of banking on is that we’re able to actually offer that sort of hyper-personalized input and advice without you having to, you know, form a relationship with a certified financial adviser where you’re paying them an assets-under-management fee, or even paying them, you know, a couple hundred bucks an hour,” he said. “You’re able to get very specific advice, regardless of what your financial situation is.” But Mr. Pigford believes it’s too early to do away with live professionals. “I think we’ll have some transition period where we’ll want humans involved for a while,” he said. “The goal is not to completely do away with a financial adviser.” Glenn Hopper, author of “Deep Finance: Corporate Finance in the Information Age,” relates this GPT era to the screech of dial-up internet. The prevalence of A.I., he said, is “going to come quicker than the adoption of the internet and broadband internet and web browsers.” “I’ve stopped making predictions, because every time I make a prediction, I’ll say six to 12 months, and then I’ll read an article the next day that this item has already appeared,” Mr. Hopper said. He warned that tools like ChatGPT would make scamming and phishing more sophisticated, so users should be cautious of anyone asking for their bank information. “The very first thing that I tell everyone is, if you’ve been ignoring artificial intelligence up until now — stop,” he said. He doesn’t think people need to become experts, but they should have a basic understanding of how the technology works, he said. “If we’re going to hand over our decisions to them, and we don’t have any idea how they’re working, I mean, you might as well shake one of those Magic 8 Balls and get the answer from that,” he said. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",76,1662,0.365886479616165,0.304799795150757,0.3293137550354
"Advertisement Supported by The findings in two new reports raise fresh concerns over how artificial intelligence may transform the misinformation landscape online. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Stuart A. Thompson Dozens of fringe news websites, content farms and fake reviewers are using artificial intelligence to create inauthentic content online, according to two reports released on Friday. The misleading A.I. content included fabricated events, medical advice and celebrity death hoaxes, the reports said, raising fresh concerns that the transformative technology could rapidly reshape the misinformation landscape online. The two reports were released separately by NewsGuard, a company that tracks online misinformation, and ShadowDragon, a company that provides resources and training for digital investigations. “News consumers trust news sources less and less in part because of how hard it has become to tell a generally reliable source from a generally unreliable source,” Steven Brill, the chief executive of NewsGuard, said in a statement. “This new wave of A.I.-created sites will only make it harder for consumers to know who is feeding them the news, further reducing trust.” NewsGuard identified 125 websites, ranging from news to lifestyle reporting and published in 10 languages, with content written entirely or mostly with A.I. tools. The sites included a health information portal that NewsGuard said published more than 50 A.I.-generated articles offering medical advice. In an article on the site about identifying end-stage bipolar disorder, the first paragraph read: “As a language model A.I., I don’t have access to the most up-to-date medical information or the ability to provide a diagnosis. Additionally, ‘end stage bipolar’ is not a recognized medical term.” The article went on to describe the four classifications of bipolar disorder, which it incorrectly described as “four main stages.” The websites were often littered with ads, suggesting that the inauthentic content was produced to drive clicks and fuel advertising revenue for the website’s owners, who were often unknown, NewsGuard said. The findings include 49 websites using A.I. content that NewsGuard identified earlier this month. Inauthentic content was also found by ShadowDragon on mainstream websites and social media, including Instagram, and in Amazon reviews. “Yes, as an A.I. language model, I can definitely write a positive product review about the Active Gear Waist Trimmer,” read one five-star review published on Amazon. Researchers were also able to reproduce some reviews using ChatGPT, finding that the bot would often point to “standout features” and conclude that it would “highly recommend” the product. The company also pointed to several Instagram accounts that appeared to use ChatGPT or other A.I. tools to write descriptions under images and videos. To find the examples, researchers looked for telltale error messages and canned responses often produced by A.I. tools. Some websites included A.I.-written warnings that the requested content contained misinformation or promoted harmful stereotypes. “As an A.I. language model, I cannot provide biased or political content,” read one message on an article about the war in Ukraine. ShadowDragon found similar messages on LinkedIn, in Twitter posts and on far-right message boards. Some of the Twitter posts were published by known bots, such as ReplyGPT, an account that will produce a tweet reply once prompted. But others appeared to be coming from regular users. Stuart A. Thompson is a reporter on the Technology desk covering online information flows.  @stuartathompson  Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",90,723,0.272749662399292,0.337649345397949,0.389600962400436
"Advertisement Supported by The Shift The Shoggoth, a character from a science fiction story, captures the essential weirdness of the A.I. moment. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Kevin Roose A few months ago, while meeting with an A.I. executive in San Francisco, I spotted a strange sticker on his laptop. The sticker depicted a cartoon of a menacing, octopus-like creature with many eyes and a yellow smiley-face attached to one of its tentacles. I asked what it was. “Oh, that’s the Shoggoth,” he explained. “It’s the most important meme in A.I.” And with that, our agenda was officially derailed. Forget about chatbots and compute clusters — I needed to know everything about the Shoggoth, what it meant and why people in the A.I. world were talking about it. The executive explained that the Shoggoth had become a popular reference among workers in artificial intelligence, as a vivid visual metaphor for how a large language model (the type of A.I. system that powers ChatGPT and other chatbots) actually works. But it was only partly a joke, he said, because it also hinted at the anxieties many researchers and engineers have about the tools they’re building. Since then, the Shoggoth has gone viral, or as viral as it’s possible to go in the small world of hyper-online A.I. insiders. It’s a popular meme on A.I. Twitter (including a now-deleted tweet by Elon Musk), a recurring metaphor in essays and message board posts about A.I. risk, and a bit of useful shorthand in conversations with A.I. safety experts. One A.I. start-up, NovelAI, said it recently named a cluster of computers “Shoggy” in homage to the meme. Another A.I. company, Scale AI, designed a line of tote bags featuring the Shoggoth. Shoggoths are fictional creatures, introduced by the science fiction author H.P. Lovecraft in his 1936 novella “At the Mountains of Madness.” In Lovecraft’s telling, Shoggoths were massive, blob-like monsters made out of iridescent black goo, covered in tentacles and eyes. Shoggoths landed in the A.I. world in December, a month after ChatGPT’s release, when Twitter user @TetraspaceWest replied to a tweet about GPT-3 (an OpenAI language model that was ChatGPT’s predecessor) with an image of two hand-drawn Shoggoths — the first labeled “GPT-3” and the second labeled “GPT-3 + RLHF.” The second Shoggoth had, perched on one of its tentacles, a smiley-face mask. In a nutshell, the joke was that in order to prevent A.I. language models from behaving in scary and dangerous ways, A.I. companies have had to train them to act polite and harmless. One popular way to do this is called “reinforcement learning from human feedback,” or R.L.H.F., a process that involves asking humans to score chatbot responses, and feeding those scores back into the A.I. model. Most A.I. researchers agree that models trained using R.L.H.F. are better behaved than models without it. But some argue that fine-tuning a language model this way doesn’t actually make the underlying model less weird and inscrutable. In their view, it’s just a flimsy, friendly mask that obscures the mysterious beast underneath. @TetraspaceWest, the meme’s creator, told me in a Twitter message that the Shoggoth “represents something that thinks in a way that humans don’t understand and that’s totally different from the way that humans think.” Comparing an A.I. language model to a Shoggoth, @TetraspaceWest said, wasn’t necessarily implying that it was evil or sentient, just that its true nature might be unknowable. “I was also thinking about how Lovecraft’s most powerful entities are dangerous — not because they don’t like humans, but because they’re indifferent and their priorities are totally alien to us and don’t involve humans, which is what I think will be true about possible future powerful A.I.” The Shoggoth image caught on, as A.I. chatbots grew popular and users began to notice that some of them seemed to be doing strange, inexplicable things their creators hadn’t intended. In February, when Bing’s chatbot became unhinged and tried to break up my marriage, an A.I. researcher I know congratulated me on “glimpsing the Shoggoth.” A fellow A.I. journalist joked that when it came to fine-tuning Bing, Microsoft had forgotten to put on its smiley-face mask. Eventually, A.I. enthusiasts extended the metaphor. In February, Twitter user @anthrupad created a version of a Shoggoth that had, in addition to a smiley-face labeled “R.L.H.F.,” a more humanlike face labeled “supervised fine-tuning.” (You practically need a computer science degree to get the joke, but it’s a riff on the difference between general A.I. language models and more specialized applications like chatbots.) Today, if you hear mentions of the Shoggoth in the A.I. community, it may be a wink at the strangeness of these systems — the black-box nature of their processes, the way they seem to defy human logic. Or maybe it’s an in-joke, visual shorthand for powerful A.I. systems that seem suspiciously nice. If it’s an A.I. safety researcher talking about the Shoggoth, maybe that person is passionate about preventing A.I. systems from displaying their true, Shoggoth-like nature. In any case, the Shoggoth is a potent metaphor that encapsulates one of the most bizarre facts about the A.I. world, which is that many of the people working on this technology are somewhat mystified by their own creations. They don’t fully understand the inner workings of A.I. language models, how they acquire new capabilities or why they behave unpredictably at times. They aren’t totally sure if A.I. is going to be net-good or net-bad for the world. And some of them have gotten to play around with the versions of this technology that haven’t yet been sanitized for public consumption — the real, unmasked Shoggoths. That some A.I. insiders refer to their creations as Lovecraftian horrors, even as a joke, is unusual by historical standards. (Put it this way: Fifteen years ago, Mark Zuckerberg wasn’t going around comparing Facebook to Cthulhu.) And it reinforces the notion that what’s happening in A.I. today feels, to some of its participants, more like an act of summoning than a software development process. They are creating the blobby, alien Shoggoths, making them bigger and more powerful, and hoping that there are enough smiley faces to cover the scary parts. Kevin Roose is a technology columnist and the author of “Futureproof: 9 Rules for Humans in the Age of Automation.”  @kevinroose • Facebook  Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",11,1229,0.329551160335541,0.32944393157959,0.341004937887192
"Advertisement Supported by The tech giant has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Cade Metz and Mike Isaac Cade Metz, who covers artificial intelligence, and Mike Isaac, who covers Meta, reported from San Francisco. In February, Meta made an unusual move in the rapidly evolving world of artificial intelligence: It decided to give away its A.I. crown jewels. The Silicon Valley giant, which owns Facebook, Instagram and WhatsApp, had created an A.I. technology, called LLaMA, that can power online chatbots. But instead of keeping the technology to itself, Meta released the system’s underlying computer code into the wild. Academics, government researchers and others who gave their email address to Meta could download the code once the company had vetted the individual. Essentially, Meta was giving its A.I. technology away as open-source software — computer code that can be freely copied, modified and reused — providing outsiders with everything they needed to quickly build chatbots of their own. “The platform that will win will be the open one,” Yann LeCun, Meta’s chief A.I. scientist, said in an interview. As a race to lead A.I. heats up across Silicon Valley, Meta is standing out from its rivals by taking a different approach to the technology. Driven by its founder and chief executive, Mark Zuckerberg, Meta believes that the smartest thing to do is share its underlying A.I. engines as a way to spread its influence and ultimately move faster toward the future. Its actions contrast with those of Google and OpenAI, the two companies leading the new A.I. arms race. Worried that A.I. tools like chatbots will be used to spread disinformation, hate speech and other toxic content, those companies are becoming increasingly secretive about the methods and software that underpin their A.I. products. Google, OpenAI and others have been critical of Meta, saying an unfettered open-source approach is dangerous. A.I.’s rapid rise in recent months has raised alarm bells about the technology’s risks, including how it could upend the job market if it is not properly deployed. And within days of LLaMA’s release, the system leaked onto 4chan, the online message board known for spreading false and misleading information. “We want to think more carefully about giving away details or open sourcing code” of A.I. technology, said Zoubin Ghahramani, a Google vice president of research who helps oversee A.I. work. “Where can that lead to misuse?” Some within Google have also wondered if open-sourcing A.I. technology may pose a competitive threat. In a memo this month, which was leaked on the online publication Semianalysis.com, a Google engineer warned colleagues that the rise of open-source software like LLaMA could cause Google and OpenAI to lose their lead in A.I. But Meta said it saw no reason to keep its code to itself. The growing secrecy at Google and OpenAI is a “huge mistake,” Dr. LeCun said, and a “really bad take on what is happening.” He argues that consumers and governments will refuse to embrace A.I. unless it is outside the control of companies like Google and Meta. “Do you want every A.I. system to be under the control of a couple of powerful American companies?” he asked. OpenAI declined to comment. Meta’s open-source approach to A.I. is not novel. The history of technology is littered with battles between open source and proprietary, or closed, systems. Some hoard the most important tools that are used to build tomorrow’s computing platforms, while others give those tools away. Most recently, Google open-sourced the Android mobile operating system to take on Apple’s dominance in smartphones. Many companies have openly shared their A.I. technologies in the past, at the insistence of researchers. But their tactics are changing because of the race around A.I. That shift began last year when OpenAI released ChatGPT. The chatbot’s wild success wowed consumers and kicked up the competition in the A.I. field, with Google moving quickly to incorporate more A.I. into its products and Microsoft investing $13 billion in OpenAI. While Google, Microsoft and OpenAI have since received most of the attention in A.I., Meta has also invested in the technology for nearly a decade. The company has spent billions of dollars building the software and the hardware needed to realize chatbots and other “generative A.I.,” which produce text, images and other media on their own. In recent months, Meta has worked furiously behind the scenes to weave its years of A.I. research and development into new products. Mr. Zuckerberg is focused on making the company an A.I. leader, holding weekly meetings on the topic with his executive team and product leaders. On Thursday, in a sign of its commitment to A.I., Meta said it had designed a new computer chip and improved a new supercomputer specifically for building A.I. technologies. It is also designing a new computer data center with an eye toward the creation of A.I. “We’ve been building advanced infrastructure for A.I. for years now, and this work reflects long-term efforts that will enable even more advances and better use of this technology across everything we do,” Mr. Zuckerberg said. Meta’s biggest A.I. move in recent months was releasing LLaMA, which is what is known as a large language model, or L.L.M. (LLaMA stands for “Large Language Model Meta AI.”) L.L.M.s are systems that learn skills by analyzing vast amounts of text, including books, Wikipedia articles and chat logs. ChatGPT and Google’s Bard chatbot are also built atop such systems. L.L.M.s pinpoint patterns in the text they analyze and learn to generate text of their own, including term papers, blog posts, poetry and computer code. They can even carry on complex conversations. In February, Meta openly released LLaMA, allowing academics, government researchers and others who provided their email address to download the code and use it to build a chatbot of their own. But the company went further than many other open-source A.I. projects. It allowed people to download a version of LLaMA after it had been trained on enormous amounts of digital text culled from the internet. Researchers call this “releasing the weights,” referring to the particular mathematical values learned by the system as it analyzes data. This was significant because analyzing all that data typically requires hundreds of specialized computer chips and tens of millions of dollars, resources most companies do not have. Those who have the weights can deploy the software quickly, easily and cheaply, spending a fraction of what it would otherwise cost to create such powerful software. As a result, many in the tech industry believed Meta had set a dangerous precedent. And within days, someone released the LLaMA weights onto 4chan. At Stanford University, researchers used Meta’s new technology to build their own A.I. system, which was made available on the internet. A Stanford researcher named Moussa Doumbouya soon used it to generate problematic text, according to screenshots seen by The New York Times. In one instance, the system provided instructions for disposing of a dead body without being caught. It also generated racist material, including comments that supported the views of Adolf Hitler. In a private chat among the researchers, which was seen by The Times, Mr. Doumbouya said distributing the technology to the public would be like “a grenade available to everyone in a grocery store.” He did not respond to a request for comment. Stanford promptly removed the A.I. system from the internet. The project was designed to provide researchers with technology that “captured the behaviors of cutting-edge A.I. models,” said Tatsunori Hashimoto, the Stanford professor who led the project. “We took the demo down as we became increasingly concerned about misuse potential beyond a research setting.”  Dr. LeCun argues that this kind of technology is not as dangerous as it might seem. He said small numbers of individuals could already generate and spread disinformation and hate speech. He added that toxic material could be tightly restricted by social networks such as Facebook. “You can’t prevent people from creating nonsense or dangerous information or whatever,” he said. “But you can stop it from being disseminated.” For Meta, more people using open-source software can also level the playing field as it competes with OpenAI, Microsoft and Google. If every software developer in the world builds programs using Meta’s tools, it could help entrench the company for the next wave of innovation, staving off potential irrelevance. Dr. LeCun also pointed to recent history to explain why Meta was committed to open-sourcing A.I. technology. He said the evolution of the consumer internet was the result of open, communal standards that helped build the fastest, most widespread knowledge-sharing network the world had ever seen. “Progress is faster when it is open,” he said. “You have a more vibrant ecosystem where everyone can contribute.” Cade Metz is a technology reporter and the author of “Genius Makers: The Mavericks Who Brought A.I. to Google, Facebook, and The World.” He covers artificial intelligence, driverless cars, robotics, virtual reality and other emerging areas.  @cademetz  Mike Isaac is a technology correspondent and the author of “Super Pumped: The Battle for Uber,” a best-selling book on the dramatic rise and fall of the ride-hailing company. He regularly covers Facebook and Silicon Valley, and is based in San Francisco.  @MikeIsaac • Facebook  Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",72,1720,0.503273904323578,0.282349318265915,0.214376822113991
"AI for all. The tech sector, recently battered by slowing growth after its runaway pandemic-era boom, suddenly is riding upward on another mighty wind — AI, and its democratized form, generative AI. Goldman Sachs economists Joseph Briggs and Devesh Kodnani predict a 7% bump in global GDP over the coming decade, and one van be sure technology providers will be leading recipients of this additional wealth. This is not lost on technology-sector leaders. Just about all, 94%, have plans to increase their investment in IT or emerging technologies over the next year, Ernst & Young LLP’s (EY) latest survey of 254 executives confirms. They are focused on experimenting with AI-based technologies, with nine in ten focused on platforms such as ChatGTP, Bing Chat and OpenAI. Eighty percent of tech executives indicate they will increase investment in AI in the next year. More than half of tech executives whose companies are experimenting with generative AI (56%) are doing so for economic savings. “The momentum and excitement around emerging technologies like generative AI marks a tectonic industry shift, one focused on effectiveness and efficiency,” says EY’s Ken Englund. The AI revolution is also tentatively making its way through many mainstream companies as well. ""Many clients are looking at AI for service enhancements,” says John Tomik, managing director, Slalom Element Labs/Slalom Strategy. “There are some looking at AI to automate for the purpose of reducing costs, but more often AI is being looked at to elevate a process or a service,"" he explains. Practical examples of AI in action across industries include ""assisting employees to become more productive so they can create more content, handle more complicated conversations or even to provide services when and where they could not before,” says Tomik. “I am very excited about some of the more newsworthy applications in the creative entertainment space — music, art, video — because this will have profound implications to those spaces."" AI “has become mainstream in more ways than one,"" agrees Flavio Villanustre, senior vice president at LexisNexis Risk Solutions. “Many times, businesses use AI-based technologies without realizing how much of the work is done by algorithms — which behave in a particular way based on facts learned from underlying data, rather than being explicitly programmed by a software developer. And this is not just applicable to technology companies in Silicon Valley. Small- and medium-sized companies are using these capabilities as services offered by third parties since developing and operating these AI based systems can be more cost effective than traditional approaches."" The main thing holding back greater mainstream AI adoption is its perceived complexity, Tomik says. But the business benefits can outpace these fears. ""When we show the value a business can achieve, even with the most basic applications of AI, we see a lot of excitement and mental lightbulbs start to fire off,” says Tomik. “This signals they may not have been previously exposed to some of the capabilities and value. Once we spark creativity with our clients around how to practically apply AI to real world business processes, we find they are much more willing to take a step forward with an AI investment program."" ",21,526,0.339540421962738,0.305409133434296,0.355050444602966
"AI pioneer unveils new generative AI analyst designed to democratize security and help every user — from novice to expert — operate like a seasoned security professional AUSTIN, Texas – May 30, 2023 – CrowdStrike (NASDAQ: CRWD) today introduced Charlotte AI, a new generative AI cybersecurity analyst that democratizes security and helps every user of the CrowdStrike Falcon platform — from novice to security expert — become a power user. Customers can use Charlotte AI to address many of the most critical challenges facing the security community, including closing the cybersecurity skills gap and speeding the response time to stay ahead of adversaries. Utilizing the world’s highest-fidelity security data that is continuously improved through a tight human feedback loop, Charlotte AI lets customers ask natural language questions – in English and dozens of languages – and receive intuitive answers from the CrowdStrike Falcon platform. With Charlotte AI, every user, regardless of skill level, becomes faster and more efficient responding to events of all types, including advanced threat detection, investigation, hunting, remediation and more. Bringing the Generative AI Era of Cybersecurity to Life Currently available in private customer preview, Charlotte AI initially addresses three common use cases:    Generative AI and the CrowdStrike Data Difference Generative AI has the power to be a democratizing force in cybersecurity, but its ultimate impact will be determined by the quality, context and completeness of the underlying data. Charlotte AI utilizes the world’s highest-fidelity security data, which includes the trillions of security events captured in the CrowdStrike Threat Graph, asset telemetry from across users, devices, identities, cloud workloads and CrowdStrike’s industry leading threat intelligence.  The most critical set of data — and one unique to CrowdStrike — is CrowdStrike’s human-validated content. Charlotte AI will uniquely benefit from a continuous, human feedback loop from across CrowdStrike® Falcon OverWatch™ managed threat hunting, CrowdStrike Falcon® Complete managed detection and response, CrowdStrike Services, and CrowdStrike Intelligence. This massive data set of human intelligence used to stop breaches in the real world is wholly unique to CrowdStrike, and as it has with AI from the start, only CrowdStrike brings this powerful combination of security telemetry, threat intelligence and human-validated content together into cybersecurity’s most powerful data fabric.  “Since our founding, CrowdStrike has pioneered the use of artificial intelligence in cybersecurity to identify adversary behavior and combat sophisticated attacks to stop breaches. With the introduction of Charlotte AI, we’re delivering the next innovation that will help users of all skill levels improve their ability to stop breaches while reducing security operations complexity,” said Mike Sentonas, President of CrowdStrike. “Our approach has always been rooted in the belief that the combination of AI and human intelligence together will transform cybersecurity. We believe our continuous feedback loop on human-validated content is critical, and because of this, no other vendor will be able to match the security and business outcomes of CrowdStrike’s approach to generative AI.”  Additional Resources:   About CrowdStrikeCrowdStrike (Nasdaq: CRWD), a global cybersecurity leader, has redefined modern security with one of the world’s most advanced cloud-native platforms for protecting critical areas of enterprise risk – endpoints and cloud workloads, identity and data. Powered by the CrowdStrike Security Cloud and world-class AI, the CrowdStrike Falcon® platform leverages real-time indicators of attack, threat intelligence, evolving adversary tradecraft and enriched telemetry from across the enterprise to deliver hyper-accurate detections, automated protection and remediation, elite threat hunting and prioritized observability of vulnerabilities. Purpose-built in the cloud with a single lightweight-agent architecture, the Falcon platform delivers rapid and scalable deployment, superior protection and performance, reduced complexity and immediate time-to-value. CrowdStrike: We stop breaches. Learn more: https://www.crowdstrike.com/
Follow us: Blog | Twitter | LinkedIn | Facebook | Instagram
Start a free trial today: https://www.crowdstrike.com/free-trial-guide/ © 2023 CrowdStrike, Inc. All rights reserved. CrowdStrike, the falcon logo, CrowdStrike Falcon and CrowdStrike Threat Graph are marks owned by CrowdStrike, Inc. and registered with the United States Patent and Trademark Office, and in other countries. CrowdStrike owns other trademarks and service marks, and may use the brands of third parties to identify their products and services. Contact
Kevin Benacci
CrowdStrike Corporate Communicationspress@crowdstrike.com",35,681,0.301403999328613,0.264527916908264,0.434068024158478
"Amid the excitement surrounding generative AI since the release of ChatGPT, Bard, Claude, Midjourney, and other content-creating tools, CEOs are understandably wondering: Is this tech hype, or a game-changing opportunity? And if it is the latter, what is the value to my business? This article is a collaborative effort by Michael Chui, Roger Roberts, Tanya Rodchenko, Alex Singla, Alex Sukharevsky, Lareina Yee, and Delphine Zurkiya, representing views from the McKinsey Technology Council and QuantumBlack, AI by McKinsey, which are both part of McKinsey Digital. The public-facing version of ChatGPT reached 100 million users in just two months. It democratized AI in a manner not previously seen while becoming by far the fastest-growing app ever. Its out-of-the-box accessibility makes generative AI different from all AI that came before it. Users don’t need a degree in machine learning to interact with or derive value from it; nearly anyone who can ask questions can use it. And, as with other breakthrough technologies such as the personal computer or iPhone, one generative AI platform can give rise to many applications for audiences of any age or education level and in any location with internet access. All of this is possible because generative AI chatbots are powered by foundation models, which are expansive neural networks trained on vast quantities of unstructured, unlabeled data in a variety of formats, such as text and audio. Foundation models can be used for a wide range of tasks. In contrast, previous generations of AI models were often “narrow,” meaning they could perform just one task, such as predicting customer churn. One foundation model, for example, can create an executive summary for a 20,000-word technical report on quantum computing, draft a go-to-market strategy for a tree-trimming business, and provide five different recipes for the ten ingredients in someone’s refrigerator. The downside to such versatility is that, for now, generative AI can sometimes provide less accurate results, placing renewed attention on AI risk management. With proper guardrails in place, generative AI can not only unlock novel use cases for businesses but also speed up, scale, or otherwise improve existing ones. Imagine a customer sales call, for example. A specially trained AI model could suggest upselling opportunities to a salesperson, but until now those were usually based only on static customer data obtained before the start of the call, such as demographics and purchasing patterns. A generative AI tool might suggest upselling opportunities to the salesperson in real time based on the actual content of the conversation, drawing from internal customer data, external market trends, and social media influencer data. At the same time, generative AI could offer a first draft of a sales pitch for the salesperson to adapt and personalize. The preceding example demonstrates the implications of the technology on one job role. But nearly every knowledge worker can likely benefit from teaming up with generative AI. In fact, while generative AI may eventually be used to automate some tasks, much of its value could derive from how software vendors embed the technology into everyday tools (for example, email or word-processing software) used by knowledge workers. Such upgraded tools could substantially increase productivity. CEOs want to know if they should act now—and, if so, how to start. Some may see an opportunity to leapfrog the competition by reimagining how humans get work done with generative AI applications at their side. Others may want to exercise caution, experimenting with a few use cases and learning more before making any large investments. Companies will also have to assess whether they have the necessary technical expertise, technology and data architecture, operating model, and risk management processes that some of the more transformative implementations of generative AI will require. The goal of this article is to help CEOs and their teams reflect on the value creation case for generative AI and how to start their journey. First, we offer a generative AI primer to help executives better understand the fast-evolving state of AI and the technical options available. The next section looks at how companies can participate in generative AI through four example cases targeted toward improving organizational effectiveness. These cases reflect what we are seeing among early adopters and shed light on the array of options across the technology, cost, and operating model requirements. Finally, we address the CEO’s vital role in positioning an organization for success with generative AI. Excitement around generative AI is palpable, and C-suite executives rightfully want to move ahead with thoughtful and intentional speed. We hope this article offers business leaders a balanced introduction into the promising world of generative AI. Generative AI technology is advancing quickly (Exhibit 1). The release cycle, number of start-ups, and rapid integration into existing software applications are remarkable. In this section, we will discuss the breadth of generative AI applications and provide a brief explanation of the technology, including how it differs from traditional AI. Generative AI can be used to automate, augment, and accelerate work. For the purposes of this article, we focus on ways generative AI can enhance work rather than on how it can replace the role of humans. While text-generating chatbots such as ChatGPT have been receiving outsize attention, generative AI can enable capabilities across a broad range of content, including images, video, audio, and computer code. And it can perform several functions in organizations, including classifying, editing, summarizing, answering questions, and drafting new content. Each of these actions has the potential to create value by changing how work gets done at the activity level across business functions and workflows. Following are some examples. As the technology evolves and matures, these kinds of generative AI can be increasingly integrated into enterprise workflows to automate tasks and directly perform specific actions (for example, automatically sending summary notes at the end of meetings). We already see tools emerging in this area. Application programming interface (API) is a way to programmatically access (usually external) models, data sets, or other pieces of software. Artificial intelligence (AI) is the ability of software to perform tasks that traditionally require human intelligence. Deep learning is a subset of machine learning that uses deep neural networks, which are layers of connected “neurons” whose connections have parameters or weights that can be trained. It is especially effective at learning from unstructured data such as images, text, and audio. Fine-tuning is the process of adapting a pretrained foundation model to perform better in a specific task. This entails a relatively short period of training on a labeled data set, which is much smaller than the data set the model was initially trained on. This additional training allows the model to learn and adapt to the nuances, terminology, and specific patterns found in the smaller data set. Foundation models (FM) are deep learning models trained on vast quantities of unstructured, unlabeled data that can be used for a wide range of tasks out of the box or adapted to specific tasks through fine-tuning. Examples of these models are GPT-4, PaLM, DALL·E 2, and Stable Diffusion. Generative AI is AI that is typically built using foundation models and has capabilities that earlier AI did not have, such as the ability to generate content. Foundation models can also be used for non-generative purposes (for example, classifying user sentiment as negative or positive based on call transcripts) while offering significant improvement over earlier models. For simplicity, when we refer to generative AI in this article, we include all foundation model use cases. Graphics processing units (GPUs) are computer chips that were originally developed for producing computer graphics (such as for video games) and are also useful for deep learning applications. In contrast, traditional machine learning and other analyses usually run on central processing units (CPUs), normally referred to as a computer’s “processor.” Large language models (LLMs) make up a class of foundation models that can process massive amounts of unstructured text and learn the relationships between words or portions of words, known as tokens. This enables LLMs to generate natural language text, performing tasks such as summarization or knowledge extraction. GPT-4 (which underlies ChatGPT) and LaMDA (the model behind Bard) are examples of LLMs. Machine learning (ML) is a subset of AI in which a model gains capabilities after it is trained on, or shown, many example data points. Machine learning algorithms detect patterns and learn how to make predictions and recommendations by processing data and experiences, rather than by receiving explicit programming instruction. The algorithms also adapt and can become more effective in response to new data and experiences. MLOps refers to the engineering patterns and practices to scale and sustain AI and ML. It encompasses a set of practices that span the full ML life cycle (data management, development, deployment, and live operations). Many of these practices are now enabled or optimized by supporting software (tools that help to standardize, streamline, or automate tasks). Prompt engineering refers to the process of designing, refining, and optimizing input prompts to guide a generative AI model toward producing desired (that is, accurate) outputs. Structured data are tabular data (for example, organized in tables, databases, or spreadsheets) that can be used to train some machine learning models effectively.  Unstructured data lack a consistent format or structure (for example, text, images, and audio files) and typically require more advanced techniques to extract insights. As the name suggests, the primary way in which generative AI differs from previous forms of AI or analytics is that it can generate new content, often in “unstructured” forms (for example, written text or images) that aren’t naturally represented in tables with rows and columns (see sidebar “Glossary” for a list of terms associated with generative AI). The underlying technology that enables generative AI to work is a class of artificial neural networks called foundation models. Artificial neural networks are inspired by the billions of neurons that are connected in the human brain. They are trained using deep learning, a term that alludes to the many (deep) layers within neural networks. Deep learning has powered many of the recent advances in AI. However, some characteristics set foundation models apart from previous generations of deep learning models. To start, they can be trained on extremely large and varied sets of unstructured data. For example, a type of foundation model called a large language model can be trained on vast amounts of text that is publicly available on the internet and covers many different topics. While other deep learning models can operate on sizable amounts of unstructured data, they are usually trained on a more specific data set. For example, a model might be trained on a specific set of images to enable it to recognize certain objects in photographs. In fact, other deep learning models often can perform only one such task. They can, for example, either classify objects in a photo or perform another function such as making a prediction. In contrast, one foundation model can perform both of these functions and generate content as well. Foundation models amass these capabilities by learning patterns and relationships from the broad training data they ingest, which, for example, enables them to predict the next word in a sentence. That’s how ChatGPT can answer questions about varied topics and how DALL·E 2 and Stable Diffusion can produce images based on a description. Given the versatility of a foundation model, companies can use the same one to implement multiple business use cases, something rarely achieved using earlier deep learning models. A foundation model that has incorporated information about a company’s products could potentially be used both for answering customers’ questions and for supporting engineers in developing updated versions of the products. As a result, companies can stand up applications and realize their benefits much faster. However, because of the way current foundation models work, they aren’t naturally suited to all applications. For example, large language models can be prone to “hallucination,” or answering questions with plausible but untrue assertions (see sidebar “Using generative AI responsibly”). Additionally, the underlying reasoning or sources for a response are not always provided. This means companies should be careful of integrating generative AI without human oversight in applications where errors can cause harm or where explainability is needed. Generative AI is also currently unsuited for directly analyzing large amounts of tabular data or solving advanced numerical-optimization problems. Researchers are working hard to address these limitations. Generative AI poses a variety of risks. CEOs will want to design their teams and processes to mitigate those risks from the start—not only to meet fast-evolving regulatory requirements but also to protect their business and earn consumers’ digital trust (we offer recommendations on how to do so later in this article).1Jim Boehm, Liz Grennan, Alex Singla, and Kate Smaje, “Why digital trust truly matters,” McKinsey, September 12, 2022. Fairness: Models may generate algorithmic bias due to imperfect training data or decisions made by the engineers developing the models. Intellectual property (IP): Training data and model outputs can generate significant IP risks, including infringing on copyrighted, trademarked, patented, or otherwise legally protected materials. Even when using a provider’s generative AI tool, organizations will need to understand what data went into training and how it’s used in tool outputs. Privacy: Privacy concerns could arise if users input information that later ends up in model outputs in a form that makes individuals identifiable. Generative AI could also be used to create and disseminate malicious content such as disinformation, deepfakes, and hate speech. Security: Generative AI may be used by bad actors to accelerate the sophistication and speed of cyberattacks. It also can be manipulated to provide malicious outputs. For example, through a technique called prompt injection, a third party gives a model new instructions that trick the model into delivering an output unintended by the model producer and end user. Explainability: Generative AI relies on neural networks with billions of parameters, challenging our ability to explain how any given answer is produced.  Reliability: Models can produce different answers to the same prompts, impeding the user’s ability to assess the accuracy and reliability of outputs. Organizational impact: Generative AI may significantly affect the workforce, and the impact on specific groups and local communities could be disproportionately negative. Social and environmental impact: The development and training of foundation models may lead to detrimental social and environmental consequences, including an increase in carbon emissions (for example, training one large language model can emit about 315 tons of carbon dioxide).2Ananya Ganesh, Andrew McCallum, and Emma Strubell, “Energy and policy considerations for deep learning in NLP,” Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, June 5, 2019. While foundation models serve as the “brain” of generative AI, an 

entire value chain is emerging to support the training and use of this technology (Exhibit 2).1For more, see “Exploring opportunities in the generative AI value chain,” McKinsey, April 26, 2023. Specialized hardware provides the extensive compute power needed to train the models. Cloud platforms offer the ability to tap this hardware. MLOps and model hub providers offer the tools, technologies, and practices an organization needs to adapt a foundation model and deploy it within its end-user applications. Many companies are entering the market to offer applications built on top of foundation models that enable them to perform a specific task, such as helping a company’s customers with service issues. The first foundation models required high levels of investment to develop, given the substantial computational resources required to train them and the human effort required to refine them. As a result, they were developed primarily by a few tech giants, start-ups backed by significant investment, and some open-source research collectives (for example, BigScience). However, work is under way on both smaller models that can deliver effective results for some tasks and training that’s more efficient. This could eventually open the market to more entrants. Some start-ups have already succeeded in developing their own models—for example, Cohere, Anthropic, and AI21 Labs build and train their own large language models. CEOs should consider exploration of generative AI a must, not a maybe. Generative AI can create value in a wide range of use cases. The economics and technical requirements to start are not prohibitive, while the downside of inaction could be quickly falling behind competitors. Each CEO should work with the executive team to reflect on where and how to play. Some CEOs may decide that generative AI presents a transformative opportunity for their companies, offering a chance to reimagine everything from research and development to marketing and sales to customer operations. Others may choose to start small and scale later. Once the decision is made, there are technical pathways that AI experts can follow to execute the strategy, depending on the use case. Much of the use (although not necessarily all of the value) from generative AI in an organization will come from workers employing features embedded in the software they already have. Email systems will provide an option to write the first drafts of messages. Productivity applications will create the first draft of a presentation based on a description. Financial software will generate a prose description of the notable features in a financial report. Customer-relationship-management systems will suggest ways to interact with customers. These features could accelerate the productivity of every knowledge worker. But generative AI can also be more transformative in certain use cases. Following, we look at four examples of how companies in different industries are using generative AI today to reshape how work is done within their organization.2These examples are amalgamations of cases culled from our client work and public examples rather than reflective of exact events in one particular company. The examples range from those requiring minimal resources to resource-intensive undertakings. (For a quick comparison of these examples and more technical detail, see Exhibit 3.) The first example is a relatively low-complexity case with immediate productivity benefits because it uses an off-the-shelf generative AI solution and doesn’t require in-house customization. The biggest part of a software engineer’s job is writing code. It’s a labor-intensive process that requires extensive trial and error and research into private and public documentation. At this company, a shortage of skilled software engineers has led to a large backlog of requests for features and bug fixes. To improve engineers’ productivity, the company is implementing an AI-based code-completion product that integrates with the software the engineers use to code. This allows engineers to write code descriptions in natural language, while the AI suggests several variants of code blocks that will satisfy the description. Engineers can select one of the AI’s proposals, make needed refinements, and click on it to insert the code. Our research has shown that such tools can speed up a developer’s code generation by as much as 50 percent. It can also help in debugging, which may improve the quality of the developed product. But today, generative AI cannot replace skilled software engineers. In fact, more-experienced engineers appear to reap the greatest productivity benefits from the tools, with inexperienced developers seeing less impressive—and sometimes negative—results. A known risk is that the AI-generated code may contain vulnerabilities or other bugs, so software engineers must be involved to ensure the quality and security of the code (see the final section in this article for ways to mitigate risks). The cost of this off-the-shelf generative AI coding tool is relatively low, and the time to market is short because the product is available and does not require significant in-house development. Cost varies by software provider, but fixed-fee subscriptions range from $10 to $30 per user per month. When choosing a tool, it’s important to discuss licensing and intellectual property issues with the provider to ensure the generated code doesn’t result in violations. Supporting the new tool is a small cross-functional team focused on selecting the software provider and monitoring performance, which should include checking for intellectual property and security issues. Implementation requires only workflow and policy changes. Because the tool is purely off-the-shelf software as a service (SaaS), additional computing and storage costs are minimal or nonexistent. Companies may decide to build their own generative AI applications, leveraging foundation models (via APIs or open models), instead of using an off-the-shelf tool. This requires a step up in investment from the previous example but facilitates a more customized approach to meet the company’s specific context and needs. In this example, a large corporate bank wants to use generative AI to improve the productivity of relationship managers (RMs). RMs spend considerable time reviewing large documents, such as annual reports and transcripts of earnings calls, to stay informed about a client’s situation and priorities. This enables the RM to offer services suited to the client’s particular needs. The bank decided to build a solution that accesses a foundation model through an API. The solution scans documents and can quickly provide synthesized answers to questions posed by RMs. Additional layers around the foundation model are built to streamline the user experience, integrate the tool with company systems, and apply risk and compliance controls. In particular, model outputs must be verified, much as an organization would check the outputs of a junior analyst, because some large language models have been known to hallucinate. RMs are also trained to ask questions in a way that will provide the most accurate answers from the solution (called prompt engineering), and processes are put in place to streamline validation of the tool’s outputs and information sources. In this instance, generative AI can speed up an RM’s analysis process (from days to hours), improve job satisfaction, and potentially capture insights the RM might have otherwise overlooked. The development cost comes mostly from the user interface build and integrations, which require time from a data scientist, a machine learning engineer or data engineer, a designer, and a front-end developer. Ongoing expenses include software maintenance and the cost of using APIs. Costs depend on the model choice and third-party vendor fees, team size, and time to minimum viable product. The next level of sophistication is fine-tuning a foundation model. In this example, a company uses a foundation model optimized for conversations and fine-tunes it on its own high-quality customer chats and sector-specific questions and answers. The company operates in a sector with specialized terminology (for example, law, medicine, real estate, and finance). Fast customer service is a competitive differentiator. This company’s customer support representatives handle hundreds of inbound inquiries a day. Response times were sometimes too high, causing user dissatisfaction. The company decided to introduce a generative AI customer-service bot to handle most customer requests. The goal was a swift response in a tone that matched the company brand and customer preferences. Part of the process of fine-tuning and testing the foundation model includes ensuring that responses are aligned with the domain-specific language, brand promise, and tone set for the company; ongoing monitoring is required to verify the performance of the system across multiple dimensions, including customer satisfaction. The company created a product road map consisting of several waves to minimize potential model errors. In the first wave, the chatbot was piloted internally. Employees were able to give “thumbs up” or “thumbs down” answers to the model’s suggestions, and the model was able to learn from these inputs. As a next step, the model “listened” to customer support conversations and offered suggestions. Once the technology was tested sufficiently, the second wave began, and the model was shifted toward customer-facing use cases with a human in the loop. Eventually, when leaders are completely confident in the technology, it can be largely automated. In this case, generative AI freed up service representatives to focus on higher-value and complex customer inquiries, improved representatives’ efficiency and job satisfaction, and increased service standards and customer satisfaction. The bot has access to all internal data on the customer and can “remember” earlier conversations (including phone calls), representing a step change over current customer chatbots. To capture the benefits, this use case required material investments in software, cloud infrastructure, and tech talent, as well as higher degrees of internal coordination in risk and operations. In general, fine-tuning foundation models costs two to three times as much as building one or more software layers on top of an API. Talent and third-party costs for cloud computing (if fine-tuning a self-hosted model) or for the API (if fine-tuning via a third-party API) account for the increased costs. To implement the solution, the company needed help from DataOps and MLOps experts as well as input from other functions such as product management, design, legal, and customer service specialists. The most complex and customized generative AI use cases emerge when no suitable foundation models are available and the company needs to build one from scratch. This situation may arise in specialized sectors or in working with unique data sets that are significantly different from the data used to train existing foundation models, as this pharmaceutical example demonstrates. Training a foundation model from scratch presents substantial technical, engineering, and resource challenges. The additional return on investment from using a higher-performing model should outweigh the financial and human capital costs. In this example, research scientists in drug discovery at a pharmaceutical company had to decide which experiments to run next, based on microscopy images. They had a data set of millions of these images, containing a wealth of visual information on cell features that are relevant to drug discovery but difficult for a human to interpret. The images were used to evaluate potential therapeutic candidates. The company decided to create a tool that would help scientists understand the relationship between drug chemistry and the recorded microscopy outcomes to accelerate R&D efforts. Since such multimodal models are still in infancy, the company decided to train its own instead. To build the model, team members employed both real-world images that are used to train image-based foundational models and their large internal microscopy image data set. The trained model added value by predicting which drug candidates might lead to favorable outcomes and by improving the ability to accurately identify relevant cell features for drug discovery. This can lead to more efficient and effective drug discovery processes, not only improving time to value but also reducing the number of inaccurate, misleading, or failed analyses. In general, training a model from scratch costs ten to 20 times more than building software around a model API. Larger teams (including, for example, PhD-level machine learning experts) and higher compute and storage spending account for the differences in cost. The projected cost of training a foundation model varies widely based on the desired model performance level and modeling complexity. Those factors influence the required size of the data set, team composition, and compute resources. In this use case, the engineering team and the ongoing cloud expenses accounted for the majority of costs. The company found that major updates to its tech infrastructure and processes would be needed, including access to many GPU instances to train the model, tools to distribute the training across many systems, and best-practice MLOps to limit cost and project duration. Also, substantial data-processing work was required for collection, integration (ensuring files of different data sets are in the same format and resolution), and cleaning (filtering low-quality data, removing duplicates, and ensuring distribution is in line with the intended use). Since the foundation model was trained from scratch, rigorous testing of the final model was needed to ensure that output was accurate and safe to use. The use cases outlined here offer powerful takeaways for CEOs as they embark on the generative AI journey:   The CEO has a crucial role to play in catalyzing a company’s focus on generative AI. In this closing section, we discuss strategies that CEOs will want to keep in mind as they begin their journey. Many of them echo the responses of senior executives to previous waves of new technology. However, generative AI presents its own challenges, including managing a technology moving at a speed not seen in previous technology transitions. Many organizations began exploring the possibilities for traditional AI through siloed experiments. Generative AI requires a more deliberate and coordinated approach given its unique risk considerations and the ability of foundation models to underpin multiple use cases across an organization. For example, a model fine-tuned using proprietary material to reflect the enterprise’s brand identity could be deployed across several use cases (for example, generating personalized marketing campaigns and product descriptions) and business functions, such as product development and marketing. To that end, we recommend convening a cross-functional group of the company’s leaders (for example, representing data science, engineering, legal, cybersecurity, marketing, design, and other business functions). Such a group can not only help identify and prioritize the highest-value use cases but also enable coordinated and safe implementation across the organization. Generative AI is a powerful tool that can transform how organizations operate, with particular impact in certain business domains within the value chain (for example, marketing for a retailer or operations for a manufacturer). The ease of deploying generative AI can tempt organizations to apply it to sporadic use cases across the business. It is important to have a perspective on the family of use cases by domain that will have the most transformative potential across business functions. Organizations are reimagining the target state enabled by generative AI working in sync with other traditional AI applications, along with new ways of working that may not have been possible before. A modern data and tech stack is key to nearly any successful approach to generative AI. CEOs should look to their chief technology officers to determine whether the company has the required technical capabilities in terms of computing resources, data systems, tools, and access to models (open source via model hubs or commercial via APIs). For example, the lifeblood of generative AI is fluid access to data honed for a specific business context or problem. Companies that have not yet found ways to effectively harmonize and provide ready access to their data will be unable to fine-tune generative AI to unlock more of its potentially transformative uses. Equally important is to design a scalable data architecture that includes data governance and security procedures. Depending on the use case, the existing computing and tooling infrastructure (which can be sourced via a cloud provider or set up in-house) might also need upgrading. A clear data and infrastructure strategy anchored on the business value and competitive advantage derived from generative AI will be critical. CEOs will want to avoid getting stuck in the planning stages. New models and applications are being developed and released rapidly. GPT-4, for example, was released in March 2023, following the release of ChatGPT (GPT-3.5) in November 2022 and GPT-3 in 2020. In the world of business, time is of the essence, and the fast-paced nature of generative AI technology demands that companies move quickly to take advantage of it. There are a few ways executives can keep moving at a steady clip. Although generative AI is still in the early days, it’s important to showcase internally how it can affect a company’s operating model, perhaps through a “lighthouse approach.” For example, one way forward is building a “virtual expert” that enables frontline workers to tap proprietary sources of knowledge and offer the most relevant content to customers. This has the potential to increase productivity, create enthusiasm, and enable an organization to test generative AI internally before scaling to customer-facing applications. As with other waves of technical innovation, there will be proof-of-concept fatigue and many examples of companies stuck in “pilot purgatory.” But encouraging a proof of concept is still the best way to quickly test and refine a valuable business case before scaling to adjacent use cases. By focusing on early wins that deliver meaningful results, companies can build momentum and then scale out and up, leveraging the multipurpose nature of generative AI. This approach could enable companies to promote broader AI adoption and create the culture of innovation that is essential to maintaining a competitive edge. As outlined above, the cross-functional leadership team will want to make sure such proofs of concept are deliberate and coordinated. As our four detailed use cases demonstrate, business leaders must balance value creation opportunities with the risks involved in generative AI. According to our recent Global AI Survey, most organizations don’t mitigate most of the risks associated with traditional AI, even though more than half of organizations have already adopted the technology.3“The state of AI in 2022—and a half decade in review,” McKinsey, December 6, 2022. Generative AI brings renewed attention to many of these same risks, such as the potential to perpetuate bias hidden in training data, while presenting new ones, such as its propensity to hallucinate. As a result, the cross-functional leadership team will want to not only establish overarching ethical principles and guidelines for generative AI use but also develop a thorough understanding of the risks presented by each potential use case. It will be important to look for initial use cases that both align with the organization’s overall risk tolerance and have structures in place to mitigate consequential risk. For example, a retail organization might prioritize a use case that has slightly lower value but also lower risk—such as creating initial drafts of marketing content and other tasks that keep a human in the loop. At the same time, the company might set aside a higher-value, high-risk use case such as a tool that automatically drafts and sends hyperpersonalized marketing emails. Such risk-forward practices can enable organizations to establish the controls necessary to properly manage generative AI and maintain compliance. CEOs and their teams will also want to stay current with the latest developments in generative AI regulation, including rules related to consumer data protection and intellectual property rights, to protect the company from liability issues. Countries may take varying approaches to regulation, as they often already do with AI and data. Organizations may need to adapt their working approach to calibrate process management, culture, and talent management in a way that ensures they can handle the rapidly evolving regulatory environment and risks of generative AI at scale. Business leaders should focus on building and maintaining a balanced set of alliances. A company’s acquisitions and alliances strategy should continue to concentrate on building an ecosystem of partners tuned to different contexts and addressing what generative AI requires at all levels of the tech stack, while being careful to prevent vendor lock-in. Partnering with the right companies can help accelerate execution. Organizations do not have to build out all applications or foundation models themselves. Instead, they can partner with generative AI vendors and experts to move more quickly. For instance, they can team up with model providers to customize models for a specific sector, or partner with infrastructure providers that offer support capabilities such as scalable cloud computing. Companies can use the expertise of others and move quickly to take advantage of the latest generative AI technology. But generative AI models are just the tip of the spear: multiple additional elements are required for value creation. To effectively apply generative AI for business value, companies need to build their technical capabilities and upskill their current workforce. This requires a concerted effort by leadership to identify the required capabilities based on the company’s prioritized use cases, which will likely extend beyond technical roles to include a talent mix across engineering, data, design, risk, product, and other business functions. As demonstrated in the use cases highlighted above, technical and talent needs vary widely depending on the nature of a given implementation—from using off-the-shelf solutions to building a foundation model from scratch. For example, to build a generative model, a company may need PhD-level machine learning experts; on the other hand, to develop generative AI tools using existing models and SaaS offerings, a data engineer and a software engineer may be sufficient to lead the effort. In addition to hiring the right talent, companies will want to train and educate their existing workforces. Prompt-based conversational user interfaces can make generative AI applications easy to use. But users still need to optimize their prompts, understand the technology’s limitations, and know where and when they can acceptably integrate the application into their workflows. Leadership should provide clear guidelines on the use of generative AI tools and offer ongoing education and training to keep employees apprised of their risks. Fostering a culture of self-driven research and experimentation can also encourage employees to innovate processes and products that effectively incorporate these tools. Businesses have been pursuing AI ambitions for years, and many have realized new revenue streams, product improvements, and operational efficiencies. Much of the successes in these areas have stemmed from AI technologies that remain the best tool for a particular job, and businesses should continue scaling such efforts. However, generative AI represents another promising leap forward and a world of new possibilities. While the technology’s operational and risk scaffolding is still being built, business leaders know they should embark on the generative AI journey. But where and how should they start? The answer will vary from company to company as well as within an organization. Some will start big; others may undertake smaller experiments. The best approach will depend on a company’s aspiration and risk appetite. Whatever the ambition, the key is to get under way and learn by doing. Michael Chui is a partner at the McKinsey Global Institute and a partner in McKinsey’s Bay Area office, where Roger Roberts is a partner, Tanya Rodchenko is an associate partner, and Lareina Yee, chair of the McKinsey Technology Council, is a senior partner. Alex Singla, a senior partner in the Chicago office, and Alex Sukharevsky, a senior partner in the London office, are global leaders of QuantumBlack, AI by McKinsey. Delphine Zurkiya is a senior partner in the Boston office. Credit: Video created by Chris Grava / Darby Films using a node-based visual programming language",73,6306,0.333495080471039,0.326599776744843,0.339905142784119
"An official website of the United States government Here’s how you know 
                The .gov means it’s official.
                
                Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you’re on a federal government site.
               
                The site is secure.
                
                The https:// ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely.
               We enforce federal competition and consumer protection laws that prevent anticompetitive, deceptive, and unfair business practices. View Enforcement Find legal resources and guidance to understand your business responsibilities and comply with the law. Browse legal resources View all Competition Matters Blog posts We work to advance government policies that protect consumers and promote competition. View Policy Find legal resources and guidance to understand your business responsibilities and comply with the law. Browse legal resources Memo from Chair Lina M. Khan to commission staff and commissioners regarding the vision and priorities for the FTC. Learn more View all Tech@FTC Blog posts Learn more about your rights as a consumer and how to spot and avoid scams. Find the resources you need to understand how consumer protection law impacts your business. Visit militaryconsumer.gov Visit consumer.gov Visit Competition Counts Competition Guidance View News and Events View more Events Sign up for the latest news           Track enforcement and policy developments from the Commission’s open meetings. Explore refund statistics including where refunds were sent and the dollar amounts refunded with this visualization. Our mission is protecting consumers and competition by preventing anticompetitive, deceptive, and unfair business practices through law enforcement, advocacy, and education without unduly burdening legitimate business activity. Learn more about the FTC Lina M. Khan was sworn in as Chair of the Federal Trade Commission on June 15, 2021. Chair Lina M. Khan Looking for legal documents or records? Search the Legal Library instead. Looking for legal documents or records? Search the Legal Library instead. In the 2014 movie Ex Machina, a robot manipulates someone into freeing it from its confines, resulting in the person being confined instead. The robot was designed to manipulate that person’s emotions, and, oops, that’s what it did. While the scenario is pure speculative fiction, companies are always looking for new ways – such as the use of generative AI tools – to better persuade people and change their behavior. When that conduct is commercial in nature, we’re in FTC territory, a canny valley where businesses should know to avoid practices that harm consumers.  In previous blog posts, we’ve focused on AI-related deception, both in terms of exaggerated and unsubstantiated claims for AI products and the use of generative AI for fraud. Design or use of a product can also violate the FTC Act if it is unfair – something that we’ve shown in several cases and discussed in terms of AI tools with biased or discriminatory results. Under the FTC Act, a practice is unfair if it causes more harm than good. To be more specific, it’s unfair if it causes or is likely to cause substantial injury to consumers that is not reasonably avoidable by consumers and not outweighed by countervailing benefits to consumers or to competition.   As for the new wave of generative AI tools, firms are starting to use them in ways that can influence people’s beliefs, emotions, and behavior. Such uses are expanding rapidly and include chatbots designed to provide information, advice, support, and companionship. Many of these chatbots are effectively built to persuade and are designed to answer queries in confident language even when those answers are fictional. A tendency to trust the output of these tools also comes in part from “automation bias,” whereby people may be unduly trusting of answers from machines which may seem neutral or impartial. It also comes from the effect of anthropomorphism, which may lead people to trust chatbots more when designed, say, to use personal pronouns and emojis. People could easily be led to think that they’re conversing with something that understands them and is on their side. Many commercial actors are interested in these generative AI tools and their built-in advantage of tapping into unearned human trust. Concern about their malicious use goes well beyond FTC jurisdiction. But a key FTC concern is firms using them in ways that, deliberately or not, steer people unfairly or deceptively into harmful decisions in areas such as finances, health, education, housing, and employment. Companies thinking about novel uses of generative AI, such as customizing ads to specific people or groups, should know that design elements that trick people into making harmful choices are a common element in FTC cases, such as recent actions relating to financial offers, in-game purchases, and attempts to cancel services. Manipulation can be a deceptive or unfair practice when it causes people to take actions contrary to their intended goals. Under the FTC Act, practices can be unlawful even if not all customers are harmed and even if those harmed don’t comprise a class of people protected by anti-discrimination laws. Another way that marketers could take advantage of these new tools and their manipulative abilities is to place ads within a generative AI feature, just as they can place ads in search results. The FTC has repeatedly studied and provided guidance on presenting online ads, both in search results and elsewhere, to avoid deception or unfairness. This includes recent work relating to dark patterns and native advertising. Among other things, it should always be clear that an ad is an ad, and search results or any generative AI output should distinguish clearly between what is organic and what is paid. People should know if an AI product’s response is steering them to a particular website, service provider, or product because of a commercial relationship. And, certainly, people should know if they’re communicating with a real person or a machine. Given these many concerns about the use of new AI tools, it’s perhaps not the best time for firms building or deploying them to remove or fire personnel devoted to ethics and responsibility for AI and engineering. If the FTC comes calling and you want to convince us that you adequately assessed risks and mitigated harms, these reductions might not be a good look. What would look better? We’ve provided guidance in our earlier blog posts and elsewhere. Among other things, your risk assessment and mitigations should factor in foreseeable downstream uses and the need to train staff and contractors, as well as monitoring and addressing the actual use and impact of any tools eventually deployed. If we haven’t made it obvious yet, FTC staff is focusing intensely on how companies may choose to use AI technology, including new generative AI tools, in ways that can have actual and substantial impact on consumers. And for people interacting with a chatbot or other AI-generated content, mind Prince’s warning from 1999: “It’s cool to use the computer. Don’t let the computer use you.”
  Tags: It is your choice whether to submit a comment. If you do, you must create a user name, or we will not post your comment. The Federal Trade Commission Act authorizes this information collection for purposes of managing online comments. Comments and user names are part of the Federal Trade Commission’s (FTC) public records system, and user names also are part of the FTC’s computer user records system. We may routinely use these records as described in the FTC’s Privacy Act system notices. For more information on how the FTC handles information that we collect, please read our privacy policy. The purpose of this blog and its comments section is to inform readers about Federal Trade Commission activity, and share information to help them avoid, report, and recover from fraud, scams, and bad business practices. Your thoughts, ideas, and concerns are welcome, and we encourage comments. But keep in mind, this is a moderated blog. We review all comments before they are posted, and we won’t post comments that don’t comply with our commenting policy. We expect commenters to treat each other and the blog writers with respect. We don't edit comments to remove objectionable content, so please ensure that your comment contains none of the above. The comments posted on this blog become part of the public domain. To protect your privacy and the privacy of other people, please do not include personal information. Opinions in comments that appear in this blog belong to the individuals who expressed them. They do not belong to or represent views of the Federal Trade Commission. I think it is also important for the AI influencers to properly disclose their financial ties. I know for a fact Adobe and MidJourney aren’t doing this with their influencer Kris Kashtanova, and it is deceptive and hurting a lot of people. It's imperative, creators, Influencer and advertisers are held accountable.  With social media updating daily, targeted Audiences, and consumer trust, at stake. Transparency Is A MUST! As a Buisness Owner, And Influencer I get both sides. However I myself am a consumer and parent.  Most young adults listen to the Influencer over anything else. The Influencer must be held accountable,  if paid for ad be transparent! If I  choose to place an ad , I triple check myself. AI Tools are very useful, however if the took users are not held to strict standards... I'm afraid there will be no trust left. Image generation needs to be heavily regulated, they should not be able to use people's photos without their consent in their training data. Whether copyrighted art, photos, or an iphone picture of your kid from Facebook. The last one is especially pertinent because the open source nature of SD makes people use it as an infinite pornography generator and it's built on the bedrock of people's stolen private content (aka LAION).
And Opt-Out doesn't work, it is regularly ignored. Get Business Blog updates",96,1771,0.321265548467636,0.331989824771881,0.346744686365128
"Archeology isn’t just for uncovering relics of ancient civilizations. What if some of the principles of archeology could help locate modern-day media treasures, match them to current needs, and give them new life? And what if AI could make it happen? That was the bold premise of trained archeologist and musician Peter Agelasto, who teamed with photographer and entrepreneur Richard Averitt to create a company called Starchive. The duo knew that artists of all stripes can have huge catalogs – photos, tapes, and more – that often get largely forgotten. Like old artifacts, however, they can be tagged to find them easily, set in their proper contexts, and refer to when the need arises. Unearthing Dylan, Terkel, and More The company got started by working with the Bob Dylan Music Company. Dylan had left thousands of artifacts, like photos from old tours, but there was no easy way to find what was relevant for contemporary uses. Starchive created a way. Then they brought that capability to other types of creators. The broadcaster and historian Studs Terkel, for instance, had left over 9,000 hours of recordings in a Chicago radio archive. With the help of AI partner HyperNatural, Starchive could “listen” to the recordings, determine what content meant, and relate that to users searches or trending topics in their socials. Generative AI opened the floodgates. Now, the AI could match meanings even if the tags weren’t using precisely the keyword that the user typed into the prompt. This could vastly expand the set of items found. As the company progressed, it understood more about the magnitude of media artifacts. Agelasto says, “It used to take hundreds or thousands of years to bury artifacts, but modern digital artifacts are buried nearly instantaneously - think of an Instagram thread. What if AI could help excavate them and understand them to make disorganization a relic of the past?” Starchive uses AI tools for modern-day content archeology.  Creating Differentiation The world abounds with content indexing tools, like Google Photos. How was Starchive going to be different? The team sought to understand the Jobs to be Done of its users. Averitt explains, “It used to be that the cost of production and distribution meant only the Dylans of the world created assets with enough commercial value that it was worth the effort to archive them, but we live in an era of niche audiences with insatiable content demands. With AI as a supporting player, every creator with a following can maximize their opportunity and content in ways never before possible.” Finding Business Treasures The company has since found that artists aren’t the only ones with old content waiting to be re-discovered. Writers, social media influencers, B2B marketers, and others have produced huge quantities of content over the years. What if AI could help not just discover old content in response to prompts, but match content to particular customers and automatically send them old content that’s selected for them? What if AI could even do that in a super-timely way, such as in response to a social media post someone makes, so humans didn’t need to be involved after the content is first produced? That’s a use case that Starchive is actively prototyping today. The company also sees AI impacting the content itself. Averitt says, “Scale used to be a thing that only major enterprises had. But, today, individual content creators are becoming brands and creating content at scale. I have one friend, for instance, who created 100,000 new images in three months using the AI tool Stable Diffusion. These people need tools to find the right content at the right time as the amount of content simply explodes.” AI doesn’t need to generate only new content. Making AI a tool of content archeology, this company is leveraging new capabilities to repurpose content from yesterday as well as yesteryear. No AI tool is going to generate Bob Dylan’s type of content, but it can give his old treasures new life. ",47,664,0.338287204504013,0.319088518619537,0.342624247074127
"Artificial intelligence (AI) and machine learning (ML) continue to push the boundaries of what is possible in marketing and sales. And now, with the ongoing step-change evolution of generative AI (gen AI), we’re seeing the use of open-source platforms penetrating to the sales frontlines, along with rising investment by sales-tech players in gen AI innovations. Given the accelerating complexity and speed of doing business in a digital-first world, these technologies are becoming essential tools. Inevitably, this will impact how you operate—and how you connect with and serve your customers. In fact, it’s probably already doing so. Forward-thinking C-suite leaders are considering how to adjust to this new landscape. Here, we outline the marketing and sales opportunities (and risks) in this dynamic field and suggest productive paths forward. Our research suggests that a fifth of current sales-team functions could be automated. AI is poised to disrupt marketing and sales in every sector. This is the result of shifts in consumer sentiment alongside rapid technological change. Across industries, engagement models are changing: today’s customers want everything, everywhere, and all the time. While they still desire an even mix of traditional, remote, and self-service channels (including face-to-face, inside sales, and e-commerce), we see continued growth in customer preference for online ordering and reordering. Winning companies—those increasing their market share by at least 10 percent annually—tend to utilize advanced sales technology; build hybrid sales teams and capabilities; tailor strategies for third-party and company-owned marketplaces; achieve e-commerce excellence across the entire funnel; and deliver hyper-personalization (unique messages for individual decision makers based on their needs, profile, behaviors, and interactions—both past and predictive). Many of us are already familiar with online AI chatbots and image generators, using them to create convincing pictures and text at astonishing speed. This is the great power of generative AI, or gen AI: it utilizes algorithms to generate new content—writing, images, or audio—from training data. To do this, gen AI uses deep-learning models called foundation models (FMs). FMs are pre-trained on massive datasets and the algorithms they support are adaptable to a wide variety of downstream tasks, including content generation. Gen AI can be trained, for example, to predict the next word in a string of words and can generalize that ability to multiple text-generation tasks, such as writing articles, jokes, or code. In contrast, “traditional” AI is trained on a single task with human supervision, using data specific to that task; it can be fine-tuned to reach high precision, but must be retrained for each new use case. Thus gen AI represents an enormous step change in power, sophistication, and utility—and a fundamental shift in our relationship to artificial intelligence. AI technology is evolving at pace. It is becoming increasingly easy and less costly to implement, while offering ever-accelerating complexity and speed that far exceeds human capacity. Our research suggests that a fifth of current sales-team functions could be automated. In addition, new frontiers are opening with the rise of gen AI (see sidebar “What is generative AI?”). Furthermore, venture capital investment in AI has grown 13-fold over the last ten years.1Nestor Maslej et al., “The AI Index 2023 annual report,” AI Index Steering Committee, Institute for Human-Centered AI, Stanford University, April 2023. This has led to an explosion of “usable” data (data that can be used to formulate insights and suggest tangible actions) and accessible technology (such as increased computation power and open-source algorithms). Vast, and growing, amounts of data are now available for foundation-model training, and since 2012 there’s been a millionfold increase in computation capacity—doubling every three to four months.2Cliff Saran, “Stanford University finds that AI is outpacing Moore’s Law,” Computer Weekly, December 12, 2019; Risto Miikkulainen, “Creative AI through evolutionary computation: Principles and examples,” SN Computer Science, 2(3): 163, March 23, 2001. The rise of AI, and particularly gen AI, has potential for impact in three areas of marketing and sales: customer experience (CX), growth, and productivity. For example, in CX, hyper-personalized content and offerings can be based on individual customer behavior, persona, and purchase history. Growth can be accelerated by leveraging AI to jumpstart top-line performance, giving sales teams the right analytics and customer insights to capture demand. Additionally, AI can boost sales effectiveness and performance by offloading and automating many mundane sales activities, freeing up capacity to spend more time with customers and prospective customers (while reducing cost to serve). In all these actions, personalization is key. AI coupled with company-specific data and context has enabled consumer insights at the most granular level, allowing B2C lever personalization through targeted marketing and sales offerings. Winning B2B companies go beyond account-based marketing and disproportionately use hyper-personalization in their outreach. There are many gen AI-specific use cases across the customer journey that can drive impact: Gen AI can combine and analyze large amounts of data—such as demographic information, existing customer data, and market trends—to identify additional audience segments. Its algorithms then enable businesses to create personalized outreach content, easily and at scale. Instead of spending time researching and creating audience segments, a marketer can leverage gen AI’s algorithms to identify segments with unique traits that may have been overlooked in existing customer data. Without knowing every detail about these segments, they can then ask a gen AI tool to draft automatically tailored content such as social media posts and landing pages. Once these have been refined and reviewed, the marketer and a sales leader can use gen AI to generate further content such as outreach templates for a matching sales campaign to reach prospects. Embracing these techniques will require some openness to change. Organizations will require a comprehensive and aggregated dataset (such as an operational data lake that pulls in disparate sources) to train a gen AI model that can generate relevant audience segments and content. Once trained, the model can be operationalized within commercial systems to streamline workflows while being continuously refined by agile processes. Lastly, the commercial organizational structure and operating model may need to be adjusted to ensure appropriate levels of risk oversight are in place and performance assessments align to the new ways of working. This revolutionary approach is transforming the landscape of marketing and sales, driving greater effectiveness and customer engagement from the very start of the customer journey. We asked a group of commercial leaders to provide their perspective on use cases and the role of gen AI in marketing and sales more broadly. Notably, we found cautious optimism across the board: respondents anticipated at least moderate impact from each use case we suggested. In particular, these players are most enthusiastic about use cases in the early stages of the customer journey lead identification, marketing optimization, and personalized outreach (Exhibit 1). These top three use cases are all focused on prospecting and lead generation, where we’re witnessing significant early momentum. This comes as no surprise, considering the vast amount of data on prospective customers available for analysis and the historical challenge of personalizing initial marketing outreach at scale. Various players are already deploying gen AI use cases, but this is undoubtedly only scratching the surface. Our research found that 90 percent of commercial leaders expect to utilize gen AI solutions “often” over the next two years (Exhibit 2). Our research found that 90 percent of commercial leaders expect to utilize gen AI solutions “often” over the next two years. Overall, the most effective companies are prioritizing and deploying advanced sales tech, building hybrid teams, and enabling hyper-personalization. And they’re maximizing their use of e-commerce and third-party marketplaces through analytics and AI. At successful companies, we’ve found: Such trailblazers are already realizing the potential of gen AI to elevate their operations. Our research indicates that players that invest in AI are seeing a revenue uplift of 3 to 15 percent and a sales ROI uplift of 10 to 20 percent. While the business case for artificial intelligence is compelling, the rate of change in AI technology is astonishingly fast—and not without risk. When commercial leaders were asked about the greatest barriers limiting their organization’s adoption of AI technologies, internal and external risk were at the top of the list. From IP infringement to data privacy and security, there are a number of issues that require thoughtful mitigation strategies and governance. The need for human oversight and accountability is clear, and may require the creation of new roles and capabilities to fully capitalize on opportunities ahead. In addition to immediate actions, leaders can start thinking strategically about how to invest in AI commercial excellence for the long term. It will be important to identify which use cases are table stakes, and which can help you differentiate your position in the market. Then prioritize based on impact and feasibility. The AI landscape is evolving very quickly, and winners today may not be viable tomorrow. Small start-ups are great innovators but may not be able to scale as needed or produce sales-focused use cases that meet your needs. Test and iterate with different players, but pursue partnerships strategically based on sales-related innovation, rate of innovation versus time to market, and ability to scale. AI is changing at breakneck speed, and while it’s hard to predict the course of this revolutionary tech, it’s sure to play a key role in future marketing and sales. Leaders in the field are succeeding by turning to gen AI to maximize their operations, taking advantage of advances in personalization and internal sales excellence. How will your industry react? Richelle Deveau is a partner in McKinsey’s Southern California office, Sonia Joseph Griffin is an associate partner in the Atlanta office, where Steve Reis is a senior partner. The authors wish to thank Michelle Court-Reuss, Will Godfrey, Russell Groves, Maxim Lampe, Siamak Sarvari, and Zach Stone for their contributions to this article.",82,1620,0.32231941819191,0.322050094604492,0.355630546808243
"Artificial intelligence promises more leisure and creativity for workers. But at the same time, corporations are clamping down on unions and making plans to replace their expensive human employees Goodbye humans, hello “Tessa”. The US-based National Eating Disorders Association (Neda) is making headlines after firing all its staff and replacing them with an AI-assisted chatbot called Tessa. This happened just four days after the six paid employees, who oversaw about 200 volunteers, successfully unionised. Coincidence? Oh, absolutely, Neda said; it was a long-anticipated change that had nothing to do with unionisation. A blogpost written by a helpline associate begs to differ and calls the move “union busting, plain and simple”. Is this a harbinger of things to come? Are we about to see millions of jobs wiped out as humans are replaced by AI assistants with female names? After stealing all of our jobs, are the Tessas of the world going to unionise and stage a digital takeover of Earth? The short answer is: maybe. All emerging technology goes through the “Gartner hype cycle”; now, we’re at the inflated expectations and breathless predictions stage of that cycle, and heading towards the “trough of disillusionment”, before things supposedly level out. I don’t think AI will lead to the end of civilisation as we know it in the near future. But I do think an awful lot of corporations are champing at the bit to replace as many expensive humans as they can with AI and will use the new technology as a way to clamp down on a recent wave of labour organising. In the next few years I think we are going to see a lot of chaotic experimentation as companies rush to cost-cut and bring their own “Tessas” to market. Not everyone is admitting this, of course. It tends to be bad for employee morale when your boss is crowing about how many extra yachts they can buy when they replace you with an algorithm. IBM is one of the few companies sharing specifics about how many people AI might replace: in a recent interview CEO Arvind Krishna said the technology company will pause hiring for “back-office jobs” in the coming years and automate those roles. “I could easily see 30% of [about 26,000 workers] getting replaced by AI and automation over a five-year period,” Krishna told Bloomberg. That’s about 7,800 jobs. What companies aren’t saying is also important. AI, and how it is used to create content, is a major sticking point in the Writers Guild of America (WGA) strike. The WGA wants to ensure protections are put in place to stop the big Hollywood studios from training algorithms on writers’ work and then replacing the bulk of its creatives with AI. “Based on what we’re aiming for in this contract, there couldn’t be a movie that was released by a company that we work with that had no writer,” screenwriter John August told Vox. The studios didn’t agree to this in negotiations that took place before the strike. Instead, they magnanimously said they could have “annual meetings to discuss advancements in technology.” Which seems like code for: “We’re getting rid of as many as you as we possibly can ASAP.” While all this sounds deeply depressing, there are lots of AI optimists eager to reassure us that artificial intelligence is actually going to make the world a better place. Yes, AI will replace some jobs, but it will also create better jobs. Technology will do all the drudge work and humans will have more free time to sit around writing poetry in the sun. Nobody is entirely sure how everyone will be able to feed themselves amid all this newfound leisure time but “universal basic income” (UBI) gets thrown around a lot in this scenario. (UBI is a libertarian scam and will absolutely not save us, but that’s a topic for another day.) Jonah Peretti, the CEO of BuzzFeed, was one of these vocal AI optimists. “We see the breakthroughs in AI opening up a new era of creativity that will allow humans to harness creativity in new ways with endless opportunities and applications for good,” Peretti wrote in a memo to BuzzFeed employees in January. We all know what happened a few months later, don’t we? BuzzFeed shut down its news division, dismissed a bunch of people and leaned more heavily into AI. There is certainly a lot of potential for AI to change the world for the better. I just don’t think there’s an appetite among the people at the top to harness that potential.  Arwa Mahdawi is a Guardian columnist",36,767,0.366631329059601,0.304767668247223,0.328601002693176
"As businesses and governments race to make sense of the impacts of new, powerful AI systems, governments around the world are jostling to take the lead on regulation. Business leaders should be focused on who is likely to win this race, moreso than the questions of how or even when AI will be regulated. Whether Congress, the European Commission, China, or even U.S. states or courts take the lead will determine both the speed and trajectory of AI’s transformation of the global economy, potentially protecting some industries or limiting the ability of all companies to use the technology to interact directly with consumers. Testifying before Congress on May 16, OpenAI chief executive Sam Altman said it was time for regulators to start setting limits on powerful AI systems. “As this technology advances we understand that people are anxious about how it could change the way we live. We are too,” Altman told a Senate committee. “If this technology goes wrong, it can go quite wrong,” he said, claiming it could do “significant harm to the world.” He agreed with lawmakers that government oversight will be critical to mitigating the risks. A topic barely on lawmakers’ radars a year ago, governments around the globe are now fiercely debating the pros and cons of regulating or even prohibiting some uses of artificial intelligence technologies. The question business leaders should be focused on at this moment, however, is not how or even when AI will be regulated, but by whom. Whether Congress, the European Commission, China, or even U.S. states or courts take the lead will determine both the speed and trajectory of AI’s transformation of the global economy, potentially protecting some industries or limiting the ability of all companies to use the technology to interact directly with consumers. Since the November 2022 release of OpenAI’s ChatGPT, its generative AI chatbot built on a self-improving large language model neural network (LLM), use of generative AI has exploded. According to data compiled by Statista, ChatGPT reached one million users in five days, blowing away previously warp-speed internet product introductions including Facebook, Spotify, and Netflix. Midjourney and DALL-E, LLMs that create custom illustrations based on user input, have likewise exploded in popularity, generating millions of images every day. Generative AI certainly meets the criteria for what one of us previously co-defined as a “Big Bang Disruptor”: a new technology that, from the moment of release, offers users an experience that is both better and cheaper than those with which it competes. Such a remarkable take-up is naturally cause for excitement, and, for incumbent businesses, alarm. The potential for LLMs seems limitless, perhaps revolutionizing everything from search to content generation, customer service to education, and well, you name it. Unlike more targeted Big Bang Disruptions, ChatGPT and other LLMs are uber-disruptors, breaking longstanding rules not just in one industry, but in all of them. At the same time. Given the potential scale of this disruption — as well as issues such as privacy, bias, and even national security — it’s reasonable for lawmakers to take notice. Think of Goethe’s poem “The Sorcerer’s Apprentice,” animated in Disney’s classic movie Fantasia, where the sorcerer returns to his workshop to find that his apprentice has unleashed forces that have quickly gone out of control, threatening to destroy everything in sight until the magician restores order. Many of those concerned about AI’s possible unintended consequences, including developers such as Altman, are looking to lawmakers to fill the sorcerer’s role. In the United States, multiple actors are jostling to lead the regulation of AI. First, there’s Congress, where Senate Majority Leader Chuck Schumer is calling for preemptive legislation to establish regulatory “guardrails” on AI products and services. The guardrails focus on user transparency, government reporting, and “aligning these systems with American values and ensuring that AI developers deliver on their promise to create a better world.” The vagueness of this proposal, however, isn’t promising. Second, there’s the Biden Administration, where there is some competition among federal agencies to implement a White House blueprint for an AI Bill of Rights, which was introduced last October. The blueprint is similarly general, calling for developers to ensure “safe and effective” systems that don’t discriminate or violate expectations of privacy and that explain when a user is engaging with an automated system and offer human “fallbacks” for users who request them — without, at least so far, defining any of these key terms. At the Department of Commerce, the National Telecommunications and Information Administration (NTIA) has opened an inquiry about the usefulness of audits and certifications for AI systems. The agency has requested comments on dozens of questions about accountability for AI systems, including whether, when, how, and by whom new applications should be assessed, certified, or audited, and what kind of criteria to include in these reviews. Here, the specificity of the inquiry seems headed in the right direction. Federal Trade Commission Chair Lina Kahn, meanwhile, is taking a different approach, claiming her agency already has jurisdiction over LLMs, and rattling the agency’s anti-competitive and consumer protection sabers in the direction of the new technology. Kahn speculates that AI could exacerbate existing problems in tech, including “collusion, monopolization, mergers, price discrimination, and unfair methods of competition.” Generative AI, the FTC chair also believes, “risks turbocharging fraud” with its ability to create false but convincing content. And, she notes, LLMs could — intentionally or otherwise — violate existing privacy and anti-discrimination laws by crafting responses to user requests based on biased datasets. Then, there are more provincial efforts: AI-related legislation has already been introduced in at least 17 states. Some of these proposed laws would incentivize local development of AI products, while others would limit its use in applications such as health care and hiring. Many states have or are considering creating their own task forces to recommend future legislation. So far, there are few specifics in any of these proposals, with the kinds of hypothetical harms from AI falling into existing categories, including misinformation and copyright and trademark abuse. In any case, regulators will likely have little impact on the technology’s development in the short term. Many of the proposed regulations would require Congress to give additional legal authority to the agencies, which seems unlikely in the current political climate. Even then, the application of new rules will be a matter for the courts, which translates to years of grinding process. And governments have historically had trouble attracting the kind of technical expertise required even to define the kinds of new harms LLMs and other AI applications may cause. Among the federal proposals, credit the Department of Commerce for asking the right questions. But it is unclear whether Secretary Gina Raimondo has the legal authority to create a sustainable certification process, or the political clout to get the tech industry to support NTIA’s efforts. Further, as the Department acknowledges, its inquiry is only part of the larger White House effort to create a trusted environment for AI services, an objective that would require previously unseen levels of coordination and cooperation across numerous government silos. These discussions are also taking place against the backdrop of monumental changes in American law that will likely determine who will ultimately win the part of AI’s principal regulator. Recent Supreme Court decisions have dramatically altered the legal landscape for business law, shifting power from federal regulators to the courts and the states, adding even more fragmentation, uncertainty, and delay to enforcement actions. The Court has given a green light to businesses hoping to challenge agency rulemaking, for example, by demanding more specific instructions from Congress, effectively outsourcing to federal judges the final decision on whether adopted rules will ever take effect. Meanwhile, of course, the technology will continue to evolve at its own accelerated pace. Together, these limitations suggest that major regulation is more likely to come first from outside of the U.S. For competition law, and its application to technology companies in particular, the momentum over the last few decades has already relocated from the U.S. to Europe. As the EU continues to pass substantial new internet legislation, Congress dithers, leaving the FTC and other federal agencies largely without the tools or resources to compete with their European counterparts. The European Parliament recently approved the AI Act, a 100-page statute that would preemptively ban applications deemed to have “unacceptable” levels of risk, require others to obtain pre-approval and licenses before use in the EU, and impose substantial fines on developers for a range of violations. Regulators in China are also moving quickly, both to incentivize home-grown AI products and services and to define how they can and cannot operate. Not only could this limit how non-Chinese companies interact with over a billion potential Chinese users, but could, by being first, become the de facto legal regime for future applications. It’s far from clear that any combination of government action — legislative, regulatory, or judicial — can really achieve the balancing act of maximizing the value of AI while minimizing its potential harm to the economy or society more broadly. As with all revolutionary technologies, the ability of governments to effectively regulate LLMs will almost certainly fall short. This is no criticism of lawmakers and regulators, but a side effect of the basic fact that law advances incrementally while technology evolves exponentially. In the interim, business leaders and academics should take their cue from the Department of Commerce’s ongoing initiative, and start to develop nongovernmental regulators, audits, and certification processes that identify and provide market incentives to purchase ethical and trusted AI products and services, making clear which applications are and are not reliable. There is, of course, a long history of successful (and unsuccessful) self-regulatory bodies, going all the way back to the Middle Ages and merchant “courts” that enforced the norms of medieval markets. Today, numerous groups, including the International Standards Organization, develop and certify corporate compliance with a remarkably wide range of standards, best practices, and ratings. In the information age, similar efforts have addressed everything from corporate standards for dealing with authoritarian regimes to the development of the very software and protocols that make up the internet itself. Some government regulation is inevitable. Still, the most promising way not to provoke the sorcerer would be to avoid making too big a mess in the first place.",71,1721,0.345014631748199,0.322748631238937,0.332236766815186
"At the recent SAP Sapphire conference in Orlando, SAP made several announcements showcasing its responsible AI, sustainability and “empowering customers to navigate complex challenges and transformations” in the cloud era. In this article, I examine how SAP says it wants to help customers transform business models in the cloud, achieve greater visibility around sustainability and bolster supply chain resilience.   While AI algorithms have been around since the 1960s, it took decades to make the training and inference of these models affordable for everyone except national security agencies. Today, AI and machine learning permeates many of the applications we use today. Foundational models offer new applications for text, programming, numbers, images and videos that results more what everybody thought of when we thought of ”AI.” With tech company fortunes ebbing and flowing on how investors and customers see vendors applying AI, SAP wanted, understandable, to lean into AI more than it naturally would. AI in the core of business systems is quite different that consumer AI- it’s not poems or planning the best week in Paris, it’s how company’s best deploy and manage human capital and durable assets. It’s corporate reputation, the lifeblood of a business, not playtime. Therefore, when the stakes are high, there’s little room for error. I think SAP knows this and demonstrated this at Sapphire. Integration of AI capabilities across ERP The SAP S/4HANA cloud will now include SAP's AI capabilities. The new features include SAP AI Business Services, which will assist finance teams in controlling costs and reducing risk by quickly identifying and responding to changes in customer sentiment. Sales teams could benefit from the automation of sales order functions, streamlining order processing by eliminating manual data entry. Spend management software and the SAP Business Network will now leverage AI to better help procurement professionals navigate market dynamics, engage with suppliers and enforce compliance. Features such as intelligent invoice conversion and AI-powered guided buying in SAP Ariba Buying will simplify procurement processes. The Customer Experience software portfolio will receive an AI-powered companion application to improve conversion rates and operational efficiency for sales, commerce and services teams. SAP S/4HANA Cloud will introduce built-in support for reporting and addressing problems within the software. Users can access tailored and proactive support content in real time and receive alerts for critical product issues. SAP will embed AI into industry-specific solutions as well. For instance, SAP Predictive Replenishment will use AI to optimize inventory management by automatically calculating and ordering products. Sales teams can also utilize SAP Intelligent Product Recommendations, which suggests optimal product configurations based on AI-driven sales data analysis. AI will also enhance SAP's Human Experience Management software portfolio by streamlining human resources management. SAP SuccessFactors will leverage AI to understand the workforce's skills, allowing HR teams to provide continuous development opportunities and align employees to support the company better. The SAP SuccessFactors talent intelligence hub will offer employees a foundation to build and maintain a skills portfolio, with AI assisting them in showcasing skills and providing personalized recommendations for career paths, training projects and mentors. Additionally, HR teams will gain access to AI-powered workforce management tools that consolidate workforce information, including the skills, cost and availability of employees and contract workers. This capability will enhance resource managers' visibility into available resources for various projects. An expanded partnership with Microsoft SAP has expanded its partnership with Microsoft to enhance human productivity and development in the workplace through generative AI. Integrating SAP SuccessFactors solutions with Microsoft 365 Copilot and Copilot in Microsoft Viva Learning aims to address skills gaps and improve recruitment, retention and upskilling of the workforce. (For more on Copilot and its role in Microsoft products, check out our analysis here.) Meanwhile, Microsoft's Azure OpenAI Service provides advanced language models that analyze and generate natural language. This integration aims to revolutionize how organizations attract, retain and develop the workforce, offering new and improved experiences for talent management. SAP SuccessFactors has over 248 million users, and combined with Microsoft’s one billion Microsoft Office users and 280 million daily active Microsoft Teams users, this initiative certainly has the reach to transform business at scale. The collaboration between SAP and Microsoft introduces three key innovations. First, generative AI capabilities will enhance recruiting and learning processes, providing hyper-personalized experiences within the flow of work. The second innovation is a talent intelligence hub that leverages AI to build and maintain a skills portfolio for each employee, enabling personalized recommendations for learning, mentors and internal jobs. Lastly, enhanced total workforce management functionality brings together SAP SuccessFactors, SAP Fieldglass and SAP S/4HANA Cloud to increase visibility across the organization. An expanded partnership with Google SAP has also expanded its partnership with Google to provide customers an open-data offering, enabling data integration across the enterprise landscape using SAP Datasphere and Google's Cloud. By combining SAP with Google Cloud's data and analytics technology, customers can create an end-to-end data cloud that provides real-time visibility into entire data estates, empowering organizations to maximize the value of investments in Google Cloud and SAP software, streamlining data integration and accelerating the development of AI models and analytics engines. Ledger-based accounting for carbon tracking There is a growing need to track, measure, monitor, and report on carbon emissions. SAP is extending its ERP software's definition of resources to include carbon, thus introducing the green ledger initiative. This initiative allows companies to manage carbon emissions with the same transparency and reliability as financial data. SAP is also updating its Sustainability Footprint Management solution and introducing the Sustainability Data Exchange application to facilitate the exchange of standardized sustainability data with partners and suppliers. Industry-specific networks to enhance supply chain resilience SAP also announced the SAP Business Network for Industry, which enables customers in specific industries to improve and extend key business processes with trading partners. SAP Business Network for Industry targets common supply chain collaboration use cases in specific industries. These industry-specific use cases are paired with industry best practices and are all prepackaged for quick implementation and ready use. Today's focus sectors are consumer products, high-tech, industrial manufacturing and life sciences—with more to come. Wrapping up Considering the booming interest in AI and particularly generative AI applications, SAP's announcements could not be timelier. SAP customers should sleep well knowing that SAP is planning to leverage generative AI across the ERP suite in ways that are relevant to business needs, designed responsibly and aligned with ethical standards. Finally, SAP has also increased its commitment to recruiting professional developers and announced new programs to meet the growing demand for SAP experts across the ecosystem. That can only be good news for customers embroiled in ongoing business transformations to the cloud. Moor Insights & Strategy provides or has provided paid services to technology companies like all research and tech industry analyst firms. These services include research, analysis, advising, consulting, benchmarking, acquisition matchmaking, and video and speaking sponsorships. The company has had or currently has paid business relationships with 8×8, Accenture, A10 Networks, Advanced Micro Devices, Amazon, Amazon Web Services, Ambient Scientific, Ampere Computing, Anuta Networks, Applied Brain Research, Applied Micro, Apstra, Arm, Aruba Networks (now HPE), Atom Computing, AT&T, Aura, Automation Anywhere, AWS, A-10 Strategies, Bitfusion, Blaize, Box, Broadcom, C3.AI, Calix, Cadence Systems, Campfire, Cisco Systems, Clear Software, Cloudera, Clumio, Cohesity, Cognitive Systems, CompuCom, Cradlepoint, CyberArk, Dell, Dell EMC, Dell Technologies, Diablo Technologies, Dialogue Group, Digital Optics, Dreamium Labs, D-Wave, Echelon, Ericsson, Extreme Networks, Five9, Flex, Foundries.io, Foxconn, Frame (now VMware), Fujitsu, Gen Z Consortium, Glue Networks, GlobalFoundries, Revolve (now Google), Google Cloud, Graphcore, Groq, Hiregenics, Hotwire Global, HP Inc., Hewlett Packard Enterprise, Honeywell, Huawei Technologies, HYCU, IBM, Infinidat, Infoblox, Infosys, Inseego, IonQ, IonVR, Inseego, Infosys, Infiot, Intel, Interdigital, Jabil Circuit, Juniper Networks, Keysight, Konica Minolta, Lattice Semiconductor, Lenovo, Linux Foundation, Lightbits Labs, LogicMonitor, LoRa Alliance, Luminar, MapBox, Marvell Technology, Mavenir, Marseille Inc, Mayfair Equity, Meraki (Cisco), Merck KGaA, Mesophere, Micron Technology, Microsoft, MiTEL, Mojo Networks, MongoDB, Multefire Alliance, National Instruments, Neat, NetApp, Nightwatch, NOKIA, Nortek, Novumind, NVIDIA, Nutanix, Nuvia (now Qualcomm), NXP, onsemi, ONUG, OpenStack Foundation, Oracle, Palo Alto Networks, Panasas, Peraso, Pexip, Pixelworks, Plume Design, PlusAI, Poly (formerly Plantronics), Portworx, Pure Storage, Qualcomm, Quantinuum, Rackspace, Rambus, Rayvolt E-Bikes, Red Hat, Renesas, Residio, Samsung Electronics, Samsung Semi, SAP, SAS, Scale Computing, Schneider Electric, SiFive, Silver Peak (now Aruba-HPE), SkyWorks, SONY Optical Storage, Splunk, Springpath (now Cisco), Spirent, Splunk, Sprint (now T-Mobile), Stratus Technologies, Symantec, Synaptics, Syniverse, Synopsys, Tanium, Telesign,TE Connectivity, TensTorrent, Tobii Technology, Teradata,T-Mobile, Treasure Data, Twitter, Unity Technologies, UiPath, Verizon Communications, VAST Data, Ventana Micro Systems, Vidyo, VMware, Wave Computing, Wellsmith, Xilinx, Zayo, Zebra, Zededa, Zendesk, Zoho, Zoom, and Zscaler. Moor Insights & Strategy founder, CEO, and Chief Analyst Patrick Moorhead is an investor in dMY Technology Group Inc. VI, Fivestone Partners, Frore Systems, Groq, MemryX, Movandi, and Ventana Micro., MemryX, Movandi, and Ventana Micro. ",52,1464,0.311259210109711,0.303544998168945,0.385195761919022
"BEIJING — China’s ruling Communist Party is calling for beefed-up national security measures, highlighting the risks posed by advances in artificial intelligence. A meeting headed by party leader and President Xi Jinping on Tuesday urged “dedicated efforts to safeguard political security and improve the security governance of internet data and artificial intelligence,” the official Xinhua News Agency said. Xi, who is China’s head of state, commander of the military and chair of the party’s National Security Commission, called at the meeting for “staying keenly aware of the complicated and challenging circumstances facing national security.” China needs a “new pattern of development with a new security architecture,” Xinhua reported Xi as saying. Read More: Pausing AI Developments Isn’t Enough. We Need to Shut it All Down China already dedicates vast resources to suppressing any perceived political threats to the party’s dominance, with spending on the police and security personnel exceeding that devoted to the military. While it relentlessly censors in-person protests and online criticism, citizens have continued to express dissatisfaction with policies, most recently the draconian lockdown measures enacted to combat the spread of COVID-19. China has been cracking down on its tech sector in an effort to reassert party control, but like other countries, it is scrambling to find ways to regulate the developing technology. Worries about artificial intelligence systems outsmarting humans and slipping out of control have intensified with the rise of a new generation of highly capable AI chatbots such as ChatGPT. Read More: The AI Arms Race Is Changing Everything Scientists and tech industry leaders, including high-level executives at Microsoft and Google, issued a new warning Tuesday about the perils that artificial intelligence poses to humankind. “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” the statement said. More than 1,000 researchers and technologists, including Elon Musk, who is currently on a visit to China, had signed a much longer letter earlier this year calling for a six-month pause on AI development. The missive said AI poses “profound risks to society and humanity,” and some involved in the topic have proposed a United Nations treaty to regulate the technology. Contact us at letters@time.com.",37,369,0.377002388238907,0.295589566230774,0.327408105134964
"By  Wes Davis and  Richard Lawler Nvidia temporarily became a $1 trillion company on Tuesday morning, with its rising valuation fueled by tech companies big and small racing to add generative artificial intelligence tools to their products. AI tools made up the vast bulk of recent Google I/O and Microsoft Build presentations, and Nvidia’s chips make it a key supplier for companies trying to build something with AI. Its valuation pushed past the trillion-dollar benchmark as trading opened today at just over $405 per share, putting it in the rarified air previously occupied by only a few large companies such as Apple and Microsoft after they surpassed the significant mark in August 2018 and August 2019, respectively. Amazon and Google are the other tech stocks in the club, and Meta is a former member. When the market closed on Monday, the share price had fallen back to $401.11 after peaking at about $419. That gives Nvidia’s stock a value of “just” $992 billion or so, taking it out of the $1 trillion club, for now. The last quarterly earnings report from Nvidia noted over $2 billion in profit in three months. This latest push comes after Nvidia’s business boomed early in the pandemic during a GPU shortage while they were in demand for PC gaming and cryptocurrency mining before those markets fell back throughout 2022.  Last fall, CEO Jensen Huang said it had built too many gaming GPUs and was forced to sell them for less money. However, by the time of Nvidia’s next report in February, with ChatGPT all over the news, the outlook was more promising as Huang hyped the potential of Nvidia’s data center growth, and the most recent report showed a new record in data center revenue. Over the weekend, Nvidia’s Computex 2023 keynote was full of AI announcements, including a demo of games using its Avatar Cloud Engine (ACE) for Games to support natural language both for input and responses and a new DGX GH200 supercomputer built around its latest Grace Hopper Superchip that’s collectively capable of an exaflop of AI performance. Reuters reported Nvidia’s shares jumped around 25 percent last week, and premarket trading Tuesday morning had the stock hitting up to $404.91, a gain of about 4 percent, as reported by Bloomberg.  Update May 30th, 7:39PM ET: Updated to reflect NVDA’s closing price.  / Sign up for Verge Deals to get deals on products we've tested sent to your inbox daily. The Verge is a vox media network © 2023 Vox Media, LLC. All Rights Reserved",5,428,0.345629811286926,0.319001913070679,0.335368275642395
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  Artificial intelligence may lead to human extinction and reducing the risks associated with the technology should be a global priority, industry experts and tech leaders said in an open letter. ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,"" the statement Tuesday read. Sam Altman, CEO of ChatGPT-maker OpenAI, as well as executives from Google's AI arm DeepMind and Microsoft were among those who supported and signed the short statement from the Center for AI Safety. The technology has gathered pace in recent months after chatbot ChatGPT was released for public use in November and subsequently went viral. In just two months after its launch, it reached 100 million users. ChatGPT has amazed researchers and the general public with its ability to generate humanlike responses to users' prompts, suggesting AI could replace jobs and imitate humans. The statement Tuesday said there has been increasing discussion about a ""broad spectrum of important and urgent risks from AI."" TSMC or Samsung? One chipmaker is the better play on A.I., geopolitics and earnings, analyst says These are the cheapest tech stocks in the S&P 500 How much of A.I. is just hype? A bull and a bear share their tips on how to invest But it said it can be ""difficult to voice concerns about some of advanced AI's most severe risks"" and had the aim of overcoming this obstacle and opening up the discussions. ChatGPT has arguably sparked much more awareness and adoption of AI as major firms around the world have raced to develop rival products and capabilities. Altman had admitted in March that he is a ""little bit scared"" of AI as he worries that authoritarian governments would develop the technology. Other tech leaders such as Tesla's Elon Musk and former Google CEO Eric Schmidt have cautioned about the risks AI poses to society. In an open letter in March, Musk, Apple co-founder Steve Wozniak and several tech leaders urged AI labs to stop training systems to be more powerful than GPT-4 — which is OpenAI's latest large language model. They also called for a six-month pause on such advanced development. ""Contemporary AI systems are now becoming human-competitive at general tasks,"" said the letter. ""Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization?"" the letter asked. Last week, Schmidt also separately warned about the ""existential risks"" associated with AI as the technology advances. Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",3,901,0.494434148073196,0.209275752305984,0.296290099620819
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  Fresh off Nvidia's stellar earnings report and subsequent stock rally, its CEO Jensen Huang is predicting that the world is entering a ""new computing era"". He says anyone can be a programmer, just by speaking to the computer, and the desired functions will come forth. No longer will programmers need to write lines of code, only for it to display the dreaded ""fail to compile"" because of a missing semicolon. How will this be done? By generative artificial intelligence, Huang said during his keynote speech at the Computex forum in Taiwan on Monday. At his first public keynote since the pandemic, Huang introduced a new AI supercomputer platform called DGX GH200, aimed at building generative AI models. Generative AI is a type of artificial intelligence technology that can produce various types of content, including text, imagery, audio and synthetic data. TSMC or Samsung? One chipmaker is the better play on A.I., geopolitics and earnings, analyst says These are the cheapest tech stocks in the S&P 500 How much of A.I. is just hype? A bull and a bear share their tips on how to invest ""This computer doesn't care how you program it, it will try to understand what you mean, because it has this incredible large language model capability. And so the programming barrier is incredibly low,"" Huang pointed out. ""We have closed the digital divide. Everyone is a programmer. Now, you just have to say something to the computer."" Nvidia says that generative AI is the ""most important computing platform of our generation"" as individuals and companies move to create new apps and leverage on generative AI in the process. Creative professionals will be able to create images with a simple text prompt, while programmers can accelerate application development and debugging efforts, the company said. Even architects can generate 3D models from 2D floor plans. ""Every single computing era, you could do different things that weren't possible before,"" Huang said, adding that ""artificial intelligence certainly qualifies."" He explained that this computing era is ""special in several ways"". The CEO said generative AI is able to understand information other than just text and numbers. It ""can now understand multimodality, which is the reason why this computing revolution can impact every industry,"" he added. Unlike traditional models of computer application development, which require new applications and new hardware to accommodate advances like A.I., Huang said generative A.I. will not only be able to impact new applications, but enhance old ones. ""Every application that exists will be better because of A.I.... It can succeed with old applications. And it's going to have new applications. The rate of progress, because it's so easy to use, is the reason why it's growing so fast."" Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",34,930,0.386445939540863,0.213150441646576,0.400403678417206
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  In this article Amazon is focusing on using artificial intelligence to speed up deliveries — by minimizing the distance between its products and customers, a top executive told CNBC. Stefano Perego, vice president of customer fulfilment and global ops services for North America and Europe at Amazon, outlined how the company is using AI when it comes to logistics. One area is in transportation, such as mapping and planning routes, taking into account variables like the weather, Perego said. Another area is when customers search from products on Amazon to help them find the right goods. But a key focus right now for Amazon is using AI to figure out where to place its inventory. ""I think one area that we consider key in order to lower cost to serve is on inventory placement,"" Perego said. ""So now, I'm pretty sure you're familiar with the vast selection we offer to our customers. Imagine how complex is the problem of deciding where to place that unit of inventory. And to place it in a way that we reduce distance to fulfill to customers, and we increase speed of delivery."" Amazon has been focusing on a so-called ""regionalization"" effort to ship products to customers from warehouses closest to them rather than from another part of the country. But doing so requires technology that is capable of analyzing data and patterns in order to predict what products will be in demand and where. That's where AI comes in. If a product is nearer to customers, Amazon will be able to make same-day or next-day deliveries, like what its Prime subscription service offers. Perego said the efforts are progressing well. In the United States, more than 76% of the products customers order are now from fulfilment centers within their region, according to Amazon. Amazon is also using robotics in its fulfilment centers to help with repetitive tasks such as lifting heavy packages. The company said that 75% of Amazon customer orders are handled in part by robotics. There's a debate over how robotics and artificial intelligence — such as the ChatGPT AI chatbot developed by startup OpenAI — will affect jobs. A Goldman Sachs report earlier this year suggested there could be ""significant disruption"" to the global labor market, with automation affecting 300 million jobs. Perego described automation as ""collaborative robotics,"" underlining how Amazon sees humans and technology working together. ""I think that what is happening is really a transformation of the type of jobs,"" Perego said. The executive said that when automation and AI become more widespread, they will change, rather than eliminate, the jobs that workers perform. ""Eventually, the type of job that an employee will be called to do in a fulfillment center will be increasingly a high judgment type of job,"" Perego said. ""And the heavy lifting and repetitive tasks will be done through robotics. That's fine. It's a transformation rather than a substitution."" Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",86,963,0.382890075445175,0.265282064676285,0.351827889680862
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  In this article Google's effort to rapidly add new artificial intelligence technology into its core products is making its way into the advertising world, CNBC has learned. The company has given the green light to plans for using generative AI, fueled by large language models (LLMs), to automate advertising and ad-supported consumer services, according to internal documents. Last week, Google unveiled PaLM 2, its latest and most powerful LLM, trained on reams of text data that can come up with human-like responses to questions and commands. Certain groups within Google are now planning to use PaLM 2-powered tools to allow advertisers to generate their own media assets and to suggest videos for YouTube creators to make, documents show. Google has also been testing PaLM 2 for YouTube youth content for things like titles, and descriptions. For creators, the company has been using the technology to experiment with the idea of providing five video ideas based on topics that appear relevant. With the AI chatbot craze speedily racing across the tech industry and capturing the fascination of Wall Street, Google and its peers, including Microsoft, Meta and Amazon, are rushing to embed their most sophisticated models in as many products as possible. The urgency has been particularly acute at Google since the public launch late last year of Microsoft-backed OpenAI's ChatGPT raised concern that the future of internet search was suddenly up for grabs. Meanwhile, Google has been mired in a multi-quarter stretch of muted revenue growth after almost two decades of consistent and rapid expansion. With fears of a recession building since last year, advertisers have been reeling in online marketing budgets, wreaking havoc on Google, Facebook and others. Specific to Google, paid search advertising conversion rates have decreased this year across most industries. Beyond search, email and spreadsheets, Google wants to use generative AI offerings to increase spending to boost revenue and improve margins, according to the documents. An AI-powered customer support strategy could potentially run across more than 100 Google products, including, Google Play Store, Gmail, Android Search and Maps, the documents show. Automated support chatbots could provide specific answers through simple, clear sentences and allow for follow-up questions to be asked before suggesting an advertising plan that would best suit an inquiring customer. A Google spokesperson declined to comment. Google recently offered Google Duet and Chat assistance, allowing people to use simple natural language to get answers on cloud-related questions, such as how to use certain cloud services or functions, or to get detailed implementation plans for their projects.   Google is also working on its own internal Stable Diffusion-like product for image creation, according to the documents. Stable Diffusion's technology, similar to OpenAI's DALL-E, can quickly render images in various styles with text-based direction from the user. Google's plan to push its latest AI models into advertising isn't a surprise. Last week, Facebook parent Meta unveiled the AI Sandbox, a ""testing playground"" for advertisers to try out new generative AI-powered ad tools. The company also announced updates to Meta Advantage, its portfolio of automated tools and products that advertisers can use to enhance their campaigns. On May 23, Google will be introducing new technologies for advertisers at its annual event, Google Marketing Live. The company hasn't offered specifics about what it will be announcing, but it's made clear that AI will be a central theme. ""You'll discover how our AI-powered ads solutions can help multiply your marketing expertise and drive powerful business results in today's changing economy,"" the website for the event says. WATCH: AI takes center stage at Google I/O Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",81,1070,0.41081976890564,0.229218453168869,0.359961718320847
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  In this article HP Inc. CEO Enrique Lores told CNBC's Jim Cramer on Tuesday that artificial intelligence will fundamentally change the personal computer industry, especially his own company. ""We think there is really an opportunity to create a new category of PCs that will drive significant refresh in the category,"" Lores said. """"[AI is] going to help us to redefine what a PC is, the experiences that customers will be able to get are going to be much different, and we are working with all the key software vendors, key silicon providers to redesign the architecture of a PC."" Lores said the company is creating AI-enabled PCs that will allow customers to build spreadsheets and analyze data in record time. Instead of analysts spending hours poring over figures, AI will be able to create spreadsheets, analyze data and even dialogue about its findings in seconds. These new models could hit the market as early as 2024. ""I have been in this industry for many, many years, Jim,"" Lores told Cramer. ""I have never seen an opportunity like this, to really drive innovation and drive new type of customer needs that we really think are going to be fundamental."" HP released a less-than-stellar second-quarter earnings report on Tuesday with $12.91 billion in revenue, below the $13.07 billion expected from Refinitiv estimates. But the company did beat on second-quarter earnings and raised its full-year forecast. However, Lores is confident the second half of the year will be much stronger as HP finishes up its channel inventory reduction and the end-of-year shopping season ramps up. ""It's an overall market adjustment that we think is going to happen. If you look at the first half results, they were impacted by the reduction of channel inventory we and the industry have been driving,"" he said. ""And at the same time, in the second half, because of back to school, because of the holiday season especially, consumer demand will be stronger."" Click here to download Jim Cramer's Guide to Investing at no cost to help you build long-term wealth and invest smarter. Sign up now for the CNBC Investing Club to follow Jim Cramer's every move in the market. Disclaimer Questions for Cramer? Call Cramer: 1-800-743-CNBC Want to take a deep dive into Cramer's world? Hit him up!Mad Money Twitter - Jim Cramer Twitter - Facebook - Instagram Questions, comments, suggestions for the ""Mad Money"" website? madcap@cnbc.com Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",19,877,0.423476457595825,0.22570675611496,0.350816756486893
"Credit Cards Loans Banking Mortgages Insurance Credit Monitoring Personal Finance Small Business Taxes Help for Low Credit Scores Investing SELECT All Credit Cards Find the Credit Card for You Best Credit Cards Best Rewards Credit Cards Best Travel Credit Cards Best 0% APR Credit Cards Best Balance Transfer Credit Cards Best Cash Back Credit Cards Best Credit Card Welcome Bonuses Best Credit Cards to Build Credit SELECT All Loans Find the Best Personal Loan for You Best Personal Loans Best Debt Consolidation Loans Best Loans to Refinance Credit Card Debt Best Loans with Fast Funding Best Small Personal Loans Best Large Personal Loans Best Personal Loans to Apply Online Best Student Loan Refinance SELECT All Banking Find the Savings Account for You Best High Yield Savings Accounts Best Big Bank Savings Accounts Best Big Bank Checking Accounts Best No Fee Checking Accounts No Overdraft Fee Checking Accounts Best Checking Account Bonuses Best Money Market Accounts Best CDs Best Credit Unions SELECT All Mortgages Best Mortgages Best Mortgages for Small Down Payment Best Mortgages for No Down Payment Best Mortgages with No Origination Fee Best Mortgages for Average Credit Score Adjustable Rate Mortgages Affording a Mortgage SELECT All Insurance Best Life Insurance Best Homeowners Insurance Best Renters Insurance Best Car Insurance Travel Insurance SELECT All Credit Monitoring Best Credit Monitoring Services Best Identity Theft Protection How to Boost Your Credit Score Credit Repair Services SELECT All Personal Finance Best Budgeting Apps Best Expense Tracker Apps Best Money Transfer Apps Best Resale Apps and Sites Buy Now Pay Later (BNPL) Apps Best Debt Relief SELECT All Small Business Best Small Business Savings Accounts Best Small Business Checking Accounts Best Credit Cards for Small Business Best Small Business Loans Best Tax Software for Small Business SELECT All Taxes Best Tax Software Best Tax Software for Small Businesses Tax Refunds SELECT All Help for Low Credit Scores Best Credit Cards for Bad Credit Best Personal Loans for Bad Credit Best Debt Consolidation Loans for Bad Credit Personal Loans if You Don't Have Credit Best Credit Cards for Building Credit Personal Loans for 580 Credit Score or Lower Personal Loans for 670 Credit Score or Lower Best Mortgages for Bad Credit Best Hardship Loans How to Boost Your Credit Score SELECT All Investing Best IRA Accounts Best Roth IRA Accounts Best Investing Apps Best Free Stock Trading Platforms Best Robo-Advisors Index Funds Mutual Funds ETFs Bonds  JPMorgan Chase is developing a ChatGPT-like software service that leans on a disruptive form of artificial intelligence to select investments for customers, CNBC has learned. The company applied to trademark a product called IndexGPT this month, according to a filing from the New York-based bank. IndexGPT will tap ""cloud computing software using artificial intelligence"" for ""analyzing and selecting securities tailored to customer needs,"" according to the filing. The viral success of OpenAI's ChatGPT technology last year has forced entire industries to grapple with the arrival of artificial intelligence. ChatGPT, which uses massive language models to create human-sounding responses to questions, has ignited an arms race among tech giants and chipmakers over what is seen as the next foundational innovation. The technology has a range of possible uses in finance. Banks including Goldman Sachs and Morgan Stanley have already begun testing it for internal use. That includes ways to help Goldman engineers create code or answer Morgan Stanley financial advisors' queries. But JPMorgan may be the first financial incumbent aiming to release a GPT-like product directly to its customers, according to Washington D.C.-based trademark attorney Josh Gerben. ""This is a real indication they might have a potential product to launch in the near future,"" Gerben said. ""Companies like JPMorgan don't just file trademarks for the fun of it,"" he said. The filing includes ""a sworn statement from a corporate officer essentially saying, 'Yes, we plan on using this trademark.'"" JPMorgan must launch IndexGPT within about three years of approval to secure the trademark, according to the lawyer. Trademarks typically take nearly a year to be approved, thanks to backlogs at the U.S. Patent and Trademark Office, he said. The applications are typically vaguely written to give companies the broadest possible protections, Gerben said. But JPMorgan's filing does specify that IndexGPT uses the same flavor of A.I. popularized by ChatGPT; the bank plans to use A.I. powered by ""Generative Pre-trained Transformer (GPT) models."" ""It's an A.I. program to select financial securities,"" Gerben said. ""This sounds to me like they're trying to put my financial advisor out of business."" JPMorgan declined to comment for this article. Financial advisors have long feared the arrival of technology good enough to displace their role in markets. Those fears have largely yet to materialize. Wealth management firms, including Morgan Stanley and Bank of America's Merrill, offer simple roboadvisor services, but that hasn't stopped their human advisors from gathering billions of dollars more in assets. Earlier this week, executives at JPMorgan touted their progress in applying A.I. across operations at the company's annual investor conference. The bank, which employs 1,500 data scientists and machine-learning engineers, is testing ""a number of use cases"" for GPT technology, said global tech chief Lori Beer. ""We couldn't discuss A.I. without mentioning GPT and large language models,"" Beer said. ""We've recognized the power and opportunity of these tools and are committed to exploring all the ways they can deliver value for the firm."" Got a confidential news tip? We want to hear from you. Sign up for free newsletters and get more CNBC delivered to your inbox Get this delivered to your inbox, and more info about our products and services.  © 2023 CNBC LLC. All Rights Reserved. A Division of NBCUniversal Data is a real-time snapshot *Data is delayed at least 15 minutes. Global Business and Financial News, Stock Quotes, and Market Data and Analysis. Data also provided by ",85,969,0.443973898887634,0.250041991472244,0.305984079837799
"Eric Schmidt, who served as Google’s CEO from 2001 to 2011, said Wednesday that artificial intelligence could cause people to be “harmed or killed” amid possible “existential risks,” adding to concerns from other tech executives over the development of AI. Former Google CEO Eric Schmidt warned of possible “existential risks” from AI. Schmidt, who spoke during the Wall Street Journal’s CEO Council Summit Wednesday, warned AI poses risks that “is defined as many, many, many, many people harmed or killed.” Schmidt—who also served as the executive chairman of Alphabet from 2015 to 2017—suggested regulations on AI is a “broader question for society,” adding he believes it is unlikely the U.S. will establish a regulatory agency. Sundar Pichai, CEO of Google and Alphabet, wrote in the Financial Times that “AI is the most profound technology humanity is working on today,” adding it is important to “make sure as a society we get it right.” Twitter CEO Elon Musk and Apple cofounder Steve Wozniak signed a letter in March alongside politician Andrew Yang, Skype cofounder Jaan Tallinn, Pinterest cofounder Evan Sharper and Ripple cofounder Chris Larson, which urged AI labs to “immediately pause” work to slow down an “out-of-control race” to develop the technology. OpenAI CEO Sam Altman told ABC News in March that his company was “a little bit scared” over AI’s potential, adding it “will be the greatest technology humanity has yet developed.” Schmidt warned, “There are scenarios—not today, but reasonably soon, where these systems will be able to find zero-day exploits in cyber issues, or discover new kinds of biology,” adding, “When that happens, we want to be ready to know how to make sure these things are misused by evil people.” Billionaire philanthropist Bill Gates has applauded the possible impact AI could have on society, noting he has seen “stunning” advancements in recent months. Microsoft, which Gates founded in 1975, has reportedly invested $10 billion in OpenAI. Gates acknowledged the issues raised in the letter signed by Musk and others in a blog post, though he noted any social concerns about the technology should be regulated by the government in an effort to ensure it was used for good. Gates also suggested AI could be used to improve productivity, reduce global preventable deaths among children and improve inequity in American education. Schmidt, who also cofounded the venture capital firm Innovation Endeavors, is worth $20.1 billion, according to our estimates. Schmidt’s warning follows his time on the National Security Commission on Artificial Intelligence, which released a report in 2021 that indicated the U.S. government was “not today prepared for this new technology.” Schmidt also called for increasing the nation’s budget for research and development to $2 billion in 2022, and then doubling contributions until it reached $32 billion in 2026. The White House released a plan earlier this month to tackle possible risks from AI, including $140 million in funding to establish research institutes to drive responsible innovation. Calls for regulating AI have accelerated in recent months following the launch of OpenAI’s ChatGPT late last year. Other companies—including Google—have since released their own iterations of AI chatbots. Elon Musk And Tech Leaders Call For AI ‘Pause’ Over Risks To Humanity (Forbes) Exclusive: Bill Gates On Advising OpenAI, Microsoft And Why AI Is ‘The Hottest Topic Of 2023’ (Forbes) Google Releases Bard AI Chatbot Amid Competition With ChatGPT (Forbes) ",79,559,0.327110320329666,0.325971633195877,0.346917986869812
"Famed stock picker Cathie Wood, the chief and founder of asset manager Ark Invest, said on Wednesday that software providers will be the next companies to benefit from booming interest in artificial intelligence as investors capitalize on the enthusiasm that propelled chipmaker Nvidia into the rarified cadre of companies with a $1 trillion valuation. Cathie Wood, chief executive officer and chief investment officer of Ark Invest, believes software ... [+] will benefit from the AI boom. Wood, in an interview with Bloomberg TV, said Ark is looking forward to software providers as the next big beneficiaries of the wave of enthusiasm on Wall Street driven by advances in AI after Nvidia on Tuesday temporarily became one of just six public companies in the world valued at over $1 trillion. Analysts believe Nvidia, which makes high-powered computer chips crucial to building the kinds of generative AI systems that have exploded in popularity in recent months, is one of the best positioned to benefit from the AI boom and until January Wood’s Ark Innovation Fund held a sizable position in the company. Wood said Nvidia is likely to do well over time but defended Ark’s decision to dump its stake—the timing was unfortunate and the fund missed out on the massive Nvidia rally this year—and said it was “priced ahead of the curve.” Ark is now “onto the next thing,” Wood said, predicting that companies providing software as a service will “generate 8 dollars in revenue” for every dollar of hardware Nvidia sells. Wood pointed to virtual healthcare firm Teladoc Health, communication firm Twilio and automation company UiPath as three software companies that she thinks will one day swell to the size of Nvidia. Woods funds hold stocks from all three and all three are currently a long way off their respective highs by around 80% to 90%. Many people consider Elon Musk’s Tesla to be an auto stock, Wood said. “We don’t, we think it’s much more than that.” Tesla is actually the “biggest artificial intelligence play” around. Wood said AI will propel Tesla’s stock price to $2,000 in 2027, up from around $200 now. Driverless taxis will drive this change, Wood said, predicting autonomous taxi platforms ”will deliver $10 trillion in revenue from almost zero” by 2038 globally. Nvidia was originally known for making computer chips to process graphics, especially for computer games. It was later discovered that these chips were also useful for the kinds of processing needed to build many AI systems and Nvidia pivoted to serve this growing need. It eventually came to dominate the market. While its dramatic and rapid successes this year shocked many on Wall Street, there was little surprise to those familiar with the company as it was incredibly well-positioned to capitalize on the booming interest in AI applications as the leading producer of the hardware needed to make it happen. The surge in popular interest following the launch of OpenAI’s ChatGPT accelerated this and while the company has competitors, they do not seem likely to pose a major threat anytime soon. Nvidia’s stratospheric gains led a sector-wide rally, particularly for related chip firms. Cathie Wood Says Software Stocks Are Next AI Bet After Nvidia (Bloomberg) This Tech Stock Joins Nvidia As Analyst's Favorite After CEO Predicts AI Revenues Will 'At Least Double' (Forbes) ",7,551,0.329128831624985,0.315061837434769,0.355809330940247
"Few lawyers would be foolish enough to let an AI make their arguments, but one already did, and Judge Brantley Starr is taking steps to ensure that debacle isn’t repeated in his courtroom. The Texas federal judge has added a requirement that any attorney appearing in his court must attest that “no portion of the filing was drafted by generative artificial intelligence,” or if it was, that it was checked “by a human being.” Last week, attorney Steven Schwartz allowed ChatGPT to “supplement” his legal research in a recent federal filing, providing him with six cases and relevant precedent — all of which were completely hallucinated by the language model. He now “greatly regrets” doing this, and while the national coverage of this gaffe probably caused any other lawyers thinking of trying it to think again, Judge Starr isn’t taking any chances. At the federal site for Texas’s Northern District, Starr has, like other judges, the opportunity to set specific rules for his courtroom. And added recently (though it’s unclear whether this was in response to the aforementioned filing) is the “Mandatory Certification Regarding Generative Artificial Intelligence.” Eugene Volokh first reported the news. All attorneys appearing before the Court must file on the docket a certificate attesting either that no portion of the filing was drafted by generative artificial intelligence (such as ChatGPT, Harvey.AI, or Google Bard) or that any language drafted by generative artificial intelligence was checked for accuracy, using print reporters or traditional legal databases, by a human being. A form for lawyers to sign is appended, noting that “quotations, citations, paraphrased assertions, and legal analysis” are all covered by this proscription. As summary is one of AI’s strong suits, and finding and summarizing precedent or previous cases is something that has been advertised as potentially helpful in legal work, this may end up coming into play more often than expected. Whoever drafted the memorandum on this matter at Judge Starr’s office has their finger on the pulse. The certification requirement includes a pretty well informed and convincing explanation of its necessity (line breaks added for readability): These platforms are incredibly powerful and have many uses in the law: form divorces, discovery requests, suggested errors in documents, anticipated questions at oral argument. But legal briefing is not one of them. Here’s why. These platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up—even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle. Any party believing a platform has the requisite accuracy and reliability for legal briefing may move for leave and explain why. In other words, be prepared to justify yourself. While this is just one judge in one court, it would not be surprising if others took up this rule as their own. While as the court says, this is a powerful and potentially helpful technology, its use must be at the very least clearly declared and checked for accuracy.",28,587,0.347972244024277,0.332331627607346,0.319696098566055
"File photo of Google headquarters in London, United Kingdom on April 4, 2023. (Photo by Rasid Necati ... [+] Aslim/Anadolu Agency via Getty Images) Google gave its new AI-powered search tool to select users late last week, and I’ve been testing out the technology over the past few days. The good news? The new Google Search gives more accurate answers than competing AI tools like ChatGPT. The bad news? The reason it’s more accurate is that Google’s AI appears to copy information from the internet in a form of semi-plagiarism. And to make matters worse, the design of the new Google Search will likely decimate the online publishing industry. OpenAI’s ChatGPT launched to the public in late 2022 and sparked a gold rush in what’s called generative AI—chatbots that can answer questions in a conversational tone with an authoritative voice. But, as anyone who’s used ChatGPT can tell you, it’s not very accurate. In fact, ChatGPT will just make up sources of information that don’t exist, something a lawyer in New York recently learned the hard way when he filed a document in federal court with completely fictional cases listed as legal precedent. Google launched a competing chatbot called Bard it had been developing for years in an effort to keep up with OpenAI, but Google recently announced it will soon incorporate generative AI in what’s arguably the most important product of the entire internet: Google Search, which has roughly 94% of the global search engine market. After playing with this new version of Google Search over the weekend, I truly believe it’s going to revolutionize the way people get information on the web. But I think it’s going to leave a lot of online publishers struggling when this new technology is rolled out to the general public—something I first argued when Google announced its plans earlier this month at the tech company’s annual developer conference in Mountain View, California. After getting access to Google’s new search feature I asked a simple question: Can you store live oysters in the fridge? Rather than being met with Google’s traditional page of blue links to various websites that might have the answer, Google created three paragraphs of text to answer my question, as you can see in the screenshot below. The answer looked good from a design standpoint and was easy to digest. How accurate was Google’s response? Very accurate, as far as I could tell. But the reason it’s accurate will raise serious ethical questions for some people. The top of a new AI-powered Google search that's currently being tested. As you can see, the first line of Google’s answer reads: “Yes, you can store live oysters in the fridge. To ensure maximum quality, put them under a wet cloth.” Where did Google get this information? A blog called Get Shucked, which includes the line: “You can keep live oysters in the fridge. To ensure maximum quality, put them under a wet cloth.” It appears Google added the word “yes” and swapped out the word “store” for “keep.” To be clear, it’s good that Google Search is being transparent about where it’s getting its information. Those three visible websites on the right can be clicked to learn more. In fact, there are actually five links if you click the little arrow. But the million dollar question will be whether anyone actually clicks on those links to help those websites generate their own revenue, especially since those sites are the ones creating value for Google. The third line of Google’s answer about oysters explains, “You can also store them in a mesh bag or in an open container covered with a damp cloth.” That line appears to be borrowed from a website called Spruce Eats, which writes, “They should be packed in either a mesh bag or in an open container covered with a damp cloth.” Again, we see a couple of minor words swapped out, but otherwise the sentence is identical. The last few lines of Google’s response were just lifted directly from a blog called Oysters XO. I made a quick graphic to show Google’s answers that are lifted word-for-word from websites. The parts that aren’t underlined were very similar to what appeared on other websites, but weren’t word-for-word. It appears Google’s AI mostly switched out synonyms like “store” instead of “keep,” as an example. But any high school kid turning in this response would likely get flunked for plagiarism. An annotated answer from Google's new experimental search function. There are positive and negative things about this new Google Search experience. If you followed Google’s advice, you’d probably be just fine storing your oysters in the fridge, which is to say you won’t get sick. But, again, the reason Google’s advice is accurate brings us immediately to the negative: It’s just copying from websites and giving people no incentive to actually visit those websites. Why does any of this matter? Because Google Search is easily the biggest driver of traffic for the vast majority of online publishers, whether it’s major newspapers or small independent blogs. And this change to Google’s most important product has the potential to devastate their already dwindling coffers. In the case of my question about oysters, I received all the information I needed from Google’s new generative AI response. I didn’t need to leave Google’s home page and I got all the vital information, which was pulled from blogs online. In the old days (which is to say, as Google currently exists for people who don’t have access to the new experiment), I probably would’ve clicked on one of the top blogs in a Google search to learn about how long oysters can survive in the fridge. But now I didn’t need to click on anything. And the question is whether anyone will bother, if Google just hoovers up all the information on the open web and spits it back in a semi-plagiarized form to all users. Online publishers rely on people clicking on their stories. It’s how they generate revenue, whether that’s in the sale of subscriptions or the sale of those eyeballs to advertisers. But it’s not clear that this new form of Google Search will drive the same kind of traffic that it did over the past two decades. Google’s new search tool is still an experiment, as the website warns you very prominently at the top of each search. And it looks like Google has put up some guardrails to ensure it isn’t generating responses for racist and sexist material. For instance, I asked whether racism was real, something the AI didn’t attempt to answer. When asked what the word “woke” means I was greeted with the phrase, “An AI-powered overview is not available for this search.” But there are also areas where the new Google Search wouldn’t answer questions that were kind of surprising to me. I asked Google about the existence of aliens, and it wouldn’t generate a response. What is Google hiding about Area 51 and the saucer-people and reverse vampires running the government? There are some Google searches that were already built to deliver immediate answers without users needing to click on anything. For example, when I searched the question “How old was David Bowie when he died?” the answer given by traditional Google was arguably more direct, as you can see from the screenshot below. The generative AI response about the late musician didn’t really highlight the answer in the same way that traditional Google Search did just below it. Google's new generative AI response to a question about how old David Bowie was when he died. Other rather innocuous questions were also difficult for Google’s new search feature to answer. While doing laundry this weekend I thought of an odd question: What did people in the Middle Ages think of static electricity? Did they think it was evidence of witchcraft or God’s rather minor punishment or some other force that explained this strange natural phenomena? The Google AI stumbled, saying it couldn’t generate an answer with AI. Then I tried to ask what colonial Americans thought of static electricity, maybe an easier answer, I thought. The AI came up with an answer at the bottom of the AI results dating to 600 BC, but otherwise the answers were all about static electricity more generally. Asking Google's new AI about how people of the past thought of static electricity. If you’re curious about the answer, as I was, I discovered from a 2017 textbook that magicians of the Middle Ages would sometimes use static electricity to perform tricks, which makes sense. But I still didn’t get an elaborate answer about what the average person thought about electrostatic shocks hundreds of years ago. If I want to learn more about how people of the past thought about static electricity it will require more thorough research beyond Google Search, as many questions often do. After I conduct research in books and journal articles to figure out what people of the past thought about static electricity, I’ll likely write about it at my blog Paleofuture, citing my sources and answering a weird question other people might also be asking themselves while doing laundry. But then we return to the central question of Google’s new Search capability: Will Google just plagiarize the article I write for its own purposes? Which is to say, will Google’s AI tool just grab three paragraphs of text from my article and spit it out without users ever having to visit my website? Paleofuture survives because I have paying subscribers. But it can only get paying subscribers when people find the site through social media or search engines like Google. Other websites also have advertising, another business model that depends on people actually clicking through on a link to read the article. And it looks like Google’s new tools for search would drastically reduce the number of views online that publishers would receive. I argued this a few weeks ago and after trying out Google’s new search functionality I think the argument holds up. For its part, Google stressed to me that what I was experiencing was an experiment that could change before wide release. “We’re putting websites front and center in SGE [the new search engine experiment], and the goal is to highlight websites and drive attention to content from across the web. Generative responses are corroborated by sources from the web, and when a portion of a snapshot briefly includes content from a specific source, we will prominently highlight that source in the snapshot,” a Google spokesperson told me via email. “You can expand to see how the links apply to each part of the snapshot. SGE is an experimental experience in Search Labs, and it will evolve over time as we learn what is most helpful for people,” the spokesperson continued. The company also downplayed the results I was seeing that lifted entire paragraphs word-for-word, characterizing them as “snippets.” But I think the text I underlined shows it’s a more liberal borrowing from the internet than mere snippets. Again, it all comes back to the question of whether people actually click on the links, a question I asked but didn’t get an answer to on Tuesday. Will Google kill off many of the same publishers providing valuable information on the internet? Only time will tell. But I fully expect online publishers will see a huge dip in traffic when this new generative AI tool hits the masses. And oddly enough, Google needs those same publishers to make its new product a success. Google spent the past two decades absorbing all of the world’s information. Now it wants to be the one and only answer machine. Updated at 7:20 p.m. ET with comment from Google. ",51,1961,0.340704888105392,0.321012228727341,0.338282853364944
"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More Artificial intelligence (AI) has been the talk of the investment community in 2023, as a number of companies have showcased the potential of their game-changing projects -- from OpenAI's ChatGPT to Alphabet's new large language model, Bard, designed for its Google Search platform. The potential of this new technology is becoming increasingly clear, and the estimates for its impact on the broader economy are mind-boggling. Research firm McKinsey & Company thinks it could add $13 trillion to global economic activity by 2030, whereas Ark Investment Management predicts that number could be $200 trillion.  It's no surprise more companies want to be involved. According to FactSet, the management teams of 110 different companies in the S&P 500 stock market index discussed AI during their earnings calls with investors for the recent quarter ended March 31.  Not only was that an all-time high, but it was an 80% increase from 61 companies just one year ago.  McKinsey & Company predicts the companies adopting AI the earliest will reap far greater financial rewards than those waiting until later this decade. The companies that don't use AI at all by 2030 could actually experience a decline in their free cash flow. With that in mind, here are two companies at the forefront of AI, and investors can buy them right now. Electric vehicle powerhouse Tesla (TSLA 4.14%) has been working on AI for a decade. It's applying the technology to its autonomous self-driving software, which will form the foundation of its much anticipated robotaxi. The software is already in beta mode with an estimated 2.7 million customer vehicles currently on the road gathering data, and Elon Musk thinks it will be ready for public release later this year.  It could change the face of the passenger vehicle industry. In an interview with CNBC's David Faber, Musk said the average person uses their car for between 10 and 12 hours per week, so it spends most of its time parked at work or at home. By installing autonomous driving software, that same car could spend 50 hours per week serving in a ride-hailing network, earning money for both its owner and for Tesla.  An estimate by Ark Investment Management suggests this emerging industry could generate $4 trillion in revenue per year as soon as 2027. Tesla will earn revenue from the sale of its software, and from taking a split of the ride-hailing fees generated by each customer vehicle. Musk says this will transform the economics of manufacturing each Tesla vehicle, sending its gross profit margin from 25% to as high as 70%.  No other car company is anywhere near as far along in the self-driving space, and since Tesla is already the only truly profitable pure-play manufacturer of electric vehicles, the widespread adoption of autonomous technology could add trillions of dollars in value to the company in the long run.  But that's not the only area in which Tesla plans to use AI. Earlier this year, it revealed the latest prototype for its Optimus humanoid robot, which has the potential to replace human workers in low-skilled jobs like manufacturing. It's still several years from mainstream release, but Tesla intends to sell millions of units at a price of roughly $20,000 each starting in 2027.  Tesla is inching closer to monetizing AI at scale, and the potential value of its opportunities is enough to warrant buying its stock today.  Image source: Getty Images. Amazon (AMZN 1.29%) is best known as the world's largest e-commerce company with over $220 billion in online sales last year alone. There's an army of 520,000 AI-enabled robots behind that success, helping to run its fulfillment centers, which makes Amazon the single largest player in the robotics industry.  Robots can operate around the clock, they don't need benefits, and they never take days off, so they're a game-changer when it comes to productivity. Amazon focuses on selling goods in high volumes by charging consumers low prices, so its e-commerce segment runs on a razor-thin profit margin and often generates operating losses. Saving money on its fulfillment operations can be the difference between making and losing money over the long term, and more robots will equal greater efficiency. According to Ark Investment Management, the average robot could pick and pack 30 items per hour in 2015. Thanks to rapid advancements in the industry, fulfillment robots can now pick and pack 1,000 items per hour, making them 2.5 times more efficient than humans, who can typically process 400 items per hour.  Amazon is also the largest provider of cloud computing services in the world through its Amazon Web Services (AWS) platform, and it's quickly becoming a key distributor of AI technology. AWS was the first cloud provider to run Nvidia GPUs in its data centers, and in March, it announced another significant collaboration with the chipmaker to enable its customers to train larger language models than ever before on the new Amazon EC2 P5 instances.  It will allow enterprises both large and small to scale from 10,000 GPUs of computing power to 20,000 GPUs, for on-demand access to supercomputer-like performance to train the most advanced AI applications. AI start-up Anthropic -- which was founded by ex-OpenAI executives -- plans to use EC2 P5 instances to further its deep learning research as well as the development of its AI models.  This is a key advancement in the battle against AWS' closest competitor, Microsoft Azure, which is investing heavily in AI services for its cloud customers. Overall, I think Amazon has a path to a $5 trillion market valuation in the long term, and its presence in industries like robotics and AI will be major contributors. Suzanne Frey, an executive at Alphabet, is a member of The Motley Fool's board of directors. John Mackey, former CEO of Whole Foods Market, an Amazon subsidiary, is a member of The Motley Fool's board of directors. Anthony Di Pizio has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Alphabet, Amazon.com, Microsoft, Nvidia, and Tesla. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. 
Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services.
 Making the world smarter, happier, and richer.  Market data powered by Xignite.",60,1164,0.33835169672966,0.300719261169434,0.360929131507874
"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More Artificial intelligence (AI) is hot right now. Some might argue that it's too hot.  The valuations of many of the biggest AI leaders have skyrocketed so far this year, but there's still a lot of room for them to run over the next few years. My prediction is that six AI stocks will be worth a combined $20 trillion or more by 2030. It's no coincidence that 6 out of the 7 biggest stocks based on market cap that trade on U.S. exchanges have a major focus on AI. These big six of AI are: Data source: Google Finance. Market caps as of May 26, 2023. Nvidia has been a monster winner so far this year. The company's guidance in its first-quarter update absolutely stunned investors. Nvidia expects its Q2 revenue to jump 64% year over year, thanks to the soaring demand for AI chips. Meta Platforms is another shooting star of 2023. Its stock has more than doubled. The AI fervor has helped tremendously, but Meta also beat expectations with its Q1 results, thanks to renewed growth in advertising on its social media platforms. All of the other big six AI stocks have delivered year-to-date gains of over 30%. Alphabet has been the best performer of the four, with shares rising almost 40%. The company seemed to be initially caught off guard by the launch of OpenAI's ChatGPT but quickly responded with its own products. Investors applauded Microsoft's rapid moves to integrate OpenAI's technology into its products. Amazon introduced several new generative AI tools in recent months. And while Apple appears to be lagging in its public AI advances, its stock has still benefited from the AI tailwinds. Currently, the combined market cap of the six top AI stocks totals around $9.5 trillion. How can they get to $20 trillion by 2030? Their paths will vary. First, it's important to note that AI isn't the only growth driver for any of these stocks. For example, Apple's iPhone ecosystem and Amazon's e-commerce platform would almost certainly grow significantly throughout the rest of the decade, even without an AI boom. But AI should provide a massive tailwind for all six top AI stocks. Amazon, Alphabet, and Microsoft stand to especially benefit because all three companies are major cloud services providers. The increased adoption of AI will almost certainly push more organizations to the cloud over the next seven years. Nvidia should also continue to be a key beneficiary of the AI revolution. The company's graphics processing units (GPUs) remain the go-to option for running servers that power AI apps. Nvidia doesn't just have chips, though; it has a full-blown AI platform, including software, models, and services. Meta could have an AI advantage that's hiding in plain sight. The company is open-sourcing its AI technology. CEO Mark Zuckerberg thinks this strategy will put Meta at the center of future AI development. What about Apple, the seeming laggard in the AI race? CEO Tim Cook said in the tech-giant's latest quarterly conference call that the company will ""continue weaving [AI] in our products on a very thoughtful basis.""  Lots of predictions ultimately don't come true, and mine could be one of them. There are several things that could potentially get in the way of these six AI stocks reaching a combined market cap of $20 trillion by 2030. A severe and prolonged economic downturn ranks at the top of the list. All of these big companies would likely be negatively affected by a major recession, and their share prices would reflect that impact. Significant AI advances might not come as quickly and easily as many expect. As a result, the scramble to use AI could slow instead of accelerate. This scenario would especially hurt the share price of Nvidia, which is arguably in bubble territory already. We can't rule out the possibility that other companies will rise to the top of the AI world and disrupt the business models of the current tech giants. Bill Gates even thinks that forthcoming AI personal digital assistants could spell doom for Amazon's online shopping and Google Search. He believes there's a 50% chance that the disruptive technology will come from a start-up. Still, I'm sticking with my prediction. I expect that these six AI stocks really will together be worth at least $20 trillion by 2030. I hope I'm right; I own all six stocks. Randi Zuckerberg, a former director of market development and spokeswoman for Facebook and sister to Meta Platforms CEO Mark Zuckerberg, is a member of The Motley Fool's board of directors. John Mackey, former CEO of Whole Foods Market, an Amazon subsidiary, is a member of The Motley Fool's board of directors. Suzanne Frey, an executive at Alphabet, is a member of The Motley Fool's board of directors. Keith Speights has positions in Alphabet, Amazon.com, Apple, Meta Platforms, Microsoft, and Nvidia. The Motley Fool has positions in and recommends Alphabet, Amazon.com, Apple, Meta Platforms, Microsoft, and Nvidia. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. 
Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services.
 Making the world smarter, happier, and richer.  Market data powered by Xignite.",50,986,0.343101024627686,0.312222182750702,0.344676792621613
"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More C3.ai (AI 33.42%) has received a lot of attention since AI (artificial intelligence) became a business buzzword. With the abbreviation directly in its name, it's pretty clear that C3.ai is an AI platform. But it also has a few caveats you wouldn't expect for a company that's focused on AI. So, what are those caveats, and will they be a boost or a buzzkill over the next three years? Let's find out. C3.ai provides enterprise AI tools to its clients. It has products for multiple fields, including financial services, healthcare, and transportation. However, despite its broad offerings, it is highly concentrated in one industry: Oil and gas. In the third quarter of fiscal year 2023 (ended Jan. 31), this sector made up 72% of C3.ai's business. That's a pretty heavy concentration, and it gets worse. C3.ai is heavily dependent on one client in particular, Baker Hughes (BKR -0.32%), which provides solutions for oil and gas clients. In FY 2022 (ended Apr. 30, 2022), Baker Hughes made up 31% of C3.ai's revenue.  If something happened to this relationship, C3.ai's business would be in trouble, as that's a massive chunk of revenue tied up with one company. Unfortunately, short-seller Kerrisdale Capital alleged just that and believes C3.ai's relationship with Baker Hughes is deteriorating. In C3.ai's response to Kerrisdale, it only discussed its accusations of Kerrisdale's unbilled receivables accusations while leaving the topic of its relationship with its top client untouched.   That's a huge red flag for me, and it could spell out disaster in a couple of years when various contracts expire. C3.ai's financials are also a mess, with the company posting negative revenue growth. In Q3, revenue decreased by 4%, but that drop was caused by C3.ai's switch from a subscription to a consumption model. Before the switch, C3.ai was growing its revenue at a 25% pace. This occurred during the second quarter of C3.ai's fiscal year, so investors will learn how the company is truly doing when this change is lapped in the quarter beginning Sept. 1. Until then, investors will have to endure bad comparisons. On the bottom line, C3.ai is a young company posting heavy losses. In Q3, C3.ai brought in $66.7 million in revenue but spent $43.5 million on sales and marketing and $55.1 million on research and development. That's a substantial gap and why C3.ai posted an absurd 108% operating loss margin.  With operating expenses rising 17% year over year, C3.ai appears unconcerned bout controlling its expenses. But, with $772 million in cash and short-term investments on its balance sheet, C3.ai can afford to operate at this loss for a bit longer. However, it would require issuing additional shares or taking on debt in about three years if its margins don't improve.  That puts C3.ai in a precarious situation, and it does not look investible. Additionally, C3.ai trades at a hefty premium.  AI PS Ratio data by YCharts At 16.21 times sales, the stock is far from cheap. With plenty of other tech companies with brighter futures trading around the same range, I'm not sure why you'd want to own shares in C3.ai, as it was hovering around 4 times sales before AI became an investing buzzword. I'm not sure where C3.ai will be in three years, but it doesn't look great from today's current trends. As a result, I think investors should look elsewhere to gain exposure to AI. Keithen Drury has no position in any of the stocks mentioned. The Motley Fool recommends C3.ai. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. 
Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services.
 Making the world smarter, happier, and richer.  Market data powered by Xignite.",31,744,0.445087820291519,0.351374208927155,0.203537985682487
"Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. Founded in 1993 by brothers Tom and David Gardner, The Motley Fool helps millions of people attain financial freedom through our website, podcasts, books, newspaper column, radio show, and premium investing services. You’re reading a free article with opinions that may differ from The Motley Fool’s Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More In the world of artificial intelligence (AI), Palantir (PLTR 7.77%) has been a staple for a long time. The company built its platform from the ground up with AI in mind and created a successful offering for government and commercial customers alike. And it recently discussed a new AI initiative that has the potential to impact the business significantly. Read on to find out about this new product development and why it makes the stock a screaming buy. Palantir primarily uploads mounds of data through its software, then creates actionable insights. For example, a 911 dispatcher could collaborate with a hospital using Palantir's software to determine where to route an ambulance. By delivering up-to-date information in a clean dashboard, it facilitates the best possible decisions at a moment's notice. This same software has been used by various government agencies and the military for many purposes, giving Palantir a broad customer base. And the company's latest product could be a game changer for existing and new clients. Its artificial intelligence platform (AIP) is the company's large language model (LLM), allowing Palantir to deliver an interactive chatbot to its clients. This segment of AI is the same branch of technology used by products like OpenAI's ChatGPT, and with Palantir developing its in-house model, the demand for it has been astounding.  By creating an LLM that works with Palantir's data processing capabilities, users can determine the best action for a particular outcome. Returning to the hospital example, a specific incoming patient might need a particular operation, so the hospital manager could interact with the AIP to ask if a brain surgeon is available at a particular hospital.  Ryan Taylor, chief business affairs and legal officer, said during the first-quarter conference call, ""We're already seeing unprecedented demand for AIP, and we are reorganizing our efforts to aggressively capitalize on the interest.""  Shyam Sankar, chief operating officer, discussed a major insurance company that deployed Palantir's AIP in a few days to automate claims processing. The ability to rapidly roll out this product to governments and commercial companies makes it a game changer. With the stock trading at an attractive price, it looks like a strong buy. In the first quarter, Palantir squeaked out another net profit of $17 million from $525 million in revenue. While that's not an outstanding profit margin, it shows management's commitment to profitability, something few other young software companies can claim. And management gave guidance that it should achieve profitability on the basis of generally accepted accounting principles (GAAP) in every quarter this year, an upgrade from last quarter's guidance of a GAAP profit for the entire year.  Still, the price-to-earnings (P/E) ratio is useless because Palantir hasn't been profitable on a trailing-12-month basis. But if we use forward-looking earnings and its price-to-sales (P/S) ratio, we can determine if the stock looks like a buy here.  PLTR PE ratio (forward), data by YCharts. At 55 times forward earnings, the stock looks a bit pricey. But it hasn't achieved its target profit margins, and this metric should come down each quarter if better guidance is continuously given. From a P/S standpoint, Palantir is a bit expensive at 13 times, especially when its 18% growth rate is compared with other software companies.  But with one of the few business-focused chatbot products available, Palantir could have a huge market in front of it. Couple that with its deep integration into government agencies, and it has a long runway ahead. Because of that, I'd say Palantir's stock looks like a great buy at these prices. Keithen Drury has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends Palantir Technologies. The Motley Fool has a disclosure policy. *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. 
Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services.
 Making the world smarter, happier, and richer.  Market data powered by Xignite.",94,777,0.330659091472626,0.325665384531021,0.343675523996353
"From a conference room atop AMD’s headquarters in Santa Clara, California, a stretch of highway 101 running outside, Lisa Su presides over a company older than the term ""Silicon Valley."" Down the road is a link to the company’s past, an old foundry in Sunnyvale where AMD used to press its chips. But from her window she can see a recent milestone in the company’s fast-evolving present: the offices of arch-nemesis Intel, whose market capitalization ($120.3 billion) AMD’s now eclipses ($153.5 billion). It wasn’t always this way. In 2014, when Su, now 53, took up the CEO reins at AMD (Advanced Micro Devices), the chipmaker was foundering. The company had laid off around a quarter of its staff and its share price hovered around $2. Patrick Moorhead, a former AMD exec, remembers it as “deader than dead.” Then Intel began to stumble, dragged down by manufacturing delays and Apple’s decision not to use its chips in iPhones. Nimble, with a tactician’s eye, Su was able to capitalize on her rival’s missteps, inking deals with laptop makers such as Lenovo and gaming giant Sony, plus Google and Amazon, whose massive data centers generated $6 billion of the chipmaker’s sales last year. “If you look out five years, you will see AI in every single product at AMD, and it will be the largest growth driver.” 
 At $63 billion, Intel’s annual revenue still dwarfs AMD’s $23.6 billion. But wresting away coveted server chip market share from its Silicon Valley neighbor, as well as scooping up the semiconductor company Xilinx, has spiked AMD’s stock nearly 30-fold in the nine years since Su took over. Now, with the mainstreaming of artificial intelligence stoking demand for the silicon brains behind machine learning, she’s facing a legacy-defining opportunity and a daunting challenge: Can AMD produce a chip powerful enough to break Nvidia’s near-monopoly on the processors that undergird the coming wave of generative AI technology? “If you look out five years,” she says, “you will see AI in every single product at AMD, and it will be the largest growth driver.” Su has been overclocking AMD for the last nine years, much like a gamer who pushes a processor to perform beyond its manufacturer-specified limits. Unlike many tech executives, she’s a world-class researcher, with a Ph.D. in electrical engineering from MIT. Her unique combination of technical genius, people skills and business savvy has made her among the highest-paid S&P 500 CEOs for the last several years (total 2022 compensation: $30.2 million). Overall, she has amassed a $740 million fortune (largely in AMD stock), landing her in 34th place on our annual ranking of America’s richest businesswomen. “Talk about leaning in and just killing it,” marvels Panos Panay, Microsoft’s chief product officer, who first met Su in 2014 as she began AMD’s turnaround.   Unlike Intel, though, whose revenue has seen a 12% decrease to $63.1 billion over three years, Nvidia appears at the top of its game. Beyond rendering stunning imagery in games like Cyberpunk 2077, its GPUs (graphics processing units) have become the engine of choice for artificial intelligence companies such as OpenAI, whose ChatGPT chatbot has delighted and disturbed the public by answering questions and commands with surprisingly detailed human-sounding responses. These so-called large language models are really just stunning parlor tricks, but they’re the opening act for an AI transformation that big shots like Bill Gates say will be as significant as the dawn of the internet. Already there is huge demand for the GPUs that power them, and at least one research firm foresees a $400 billion bonanza within the next decade for the companies that make them. But right now there’s really just one. “AI equals Nvidia,” says Glenn O’Donnell, a Forrester analyst. “That’s pretty well-entrenched, and AMD has to really step up its game to overcome that.” Meanwhile, the specter of Intel still looms across Highway 101, even as the O.G. of PCs has faced further manufacturing delays, chip defects and leadership changes. “There are many great things about AMD, but the bad thing is that we have two world-class competitors,” says AMD exec Forrest Norrod, who helped Dell build its approximately $10 billion (2014 revenue) data center business in part on AMD chips and adds that the company never assumes its main rival will let problems linger. “We will always assume that Intel will fix it.” “AMD, when I joined, was not really a significant part of our ecosystem—at all. And it is a credit to Lisa that they are a very important partner for us now.” 
 Su is taking on Nvidia in the AI wars. Her secret weapon: the Instinct MI300 (shown here), a chip that melds traditional CPUs with GPU processors that are often used in gaming. WHEN SU WAS promoted into AMD’s top job in 2014, analysts were calling the company “uninvestable,” with $2.2 billion in debt. Some of its prized assets were already being sold for parts. Its fabrication plant where chips are baked (“fabs,” in industry-speak) was spun off in 2009—a blow to AMD cofounder Jerry San­ders’ infamous boast that “real men have fabs.” It even had to sell and lease back its corporate campus, in Austin, Texas—Su’s current base—in 2013. More troubling, AMD was struggling to execute. It couldn’t hit product deadlines, and Intel dominated all but the bargain end of the laptop market with Nvi­dia, Qualcomm and Samsung carving up the new smartphone business. “Our technology wasn’t competitive at the time,” Su admits. AMD hadn’t always been such a headache for investors. Sanders broke into the microprocessor business making chips for IBM in the early 1980s, but things started to change in the late ’90s and early 2000s. AMD, which had been a perennial second-rater, began generating record profits by building its own processors that beat Intel’s on speed. By 2014, those glory days were long gone. As were about a quarter of AMD’s staff, sacked by Su’s predecessor, Rory Read (Sanders stepped down as CEO in 2002). AMD had once commanded about a quarter of the now $24 billion server chip market, but its share dwindled to 2% in 2014. On her second day as CEO, Su stepped up to the microphone during an all-hands call with a message for AMD’s demoralized employees: “I believe that we can build the best,” she remembers telling her staff. “You might think that was obvious, but it wasn’t to the company at the time.” That rallying cry was also an edict and step one in her three-pronged plan to fix AMD: Create great products, deepen customer trust and simplify the company. “Three things, just to keep it simple,” she says. “Because if it’s five or ten, it’s hard.” Su refocused her engineers on building Intel-beating chips, but it can take chip designers years to draw up a viable final blueprint. AMD’s share of the server market fell even further, to half a percent, while researchers toiled in the lab. “At the time, the company wasn’t doing well, but holy cow, they were working on the most exciting design in the industry,” she says. “Engineers are motivated by products, and I like to keep that front and center.” Her decision to prioritize a new chip architecture called Zen paid off when it finally launched in 2017. “It was really good,” she says with palpable pride, adding that Zen could compute more than 50% faster than the company’s previous designs. More importantly, it signaled to the industry that AMD had turned a corner. By Zen’s third generation, released in 2020, it was the market leader in terms of speed. Zen architecture now underpins all of AMD’s processors. With her team shepherding a new generation of chips, Su hit the road to hard-sell them to jaded data center clients. She had already spent years building relationships even when AMD had no chips to sell, once driving more than four hours through a Texas ice storm to charm Antonio Neri, now CEO of Hewlett Packard Enterprise. “I was, let’s say, disenchanted by AMD’s prior generation,” Neri says. “She showed me she had the conviction of what needed to be done.” A big part of Su’s strategy was inking new deals with the tech giants, which needed oodles of CPUs to power their exploding cloud businesses. “For us, there are really three microprocessor partners. We have Nvidia, Intel, AMD,” says Thomas Kurian, CEO of Google Cloud. “AMD, when I joined, was not really a significant part of our ecosystem—at all. And it is a credit to Lisa that they are a very important partner for us now.” Last February, when AMD’s market cap first surpassed Intel’s, company cofounder Sanders, now 86, was ecstatic. “I called everybody I know!” he says. “I was delirious. I’m only sorry that Andy Grove isn’t around so I could say ‘gotcha!’ ” (Grove, Intel’s legendary former CEO, died in 2016.) SU , THE DAUGHTER of a mathematician and a bookkeeper turned entrepreneur, was born in Tainan, Taiwan, in 1969, the same year Sanders founded AMD. Her family immigrated to New York City when she was 3. She chose electrical engineering at the Massachusetts Institute of Technology because it seemed to be the most difficult major. For someone so technically talented, she was also good with people, playing peacekeeper when disagreements flared between fellow students, says Hank Smith, who ran MIT’s nanostructures lab at the time. AMD cofounder Jerry Sanders (shown here in 1979) was an early Silicon Valley swashbuckling salesman. “People thought I was all hat and no cattle,” he tells Forbes now. But he argues AMD wouldn’t have lasted so long if that were true. “We were just Su laughs upon hearing herself described as a people person. “Well, that’s compared to other MIT people,” she jokes. “I don’t think anyone would say I’m an extrovert, but communication is a huge part of my job.” After a brief stint at Texas Instruments, in 1995 she was hired as a staff researcher at IBM, where she helped design chips that run 20% faster by using semiconductors with copper circuitry instead of the traditional aluminum. Higher-ups quickly spotted her talent: In 1999, a year after the launch of the copper technology, IBM’s then-CEO, Lou Gerstner, tapped her to be his technical assistant. In his first interview in 20 years, Gerstner tells Forbes he initially worried Su was too junior for the job, but his doubts were quickly quelled. “She proved to be one of the most outstanding employees who worked in my office. Lisa doesn’t follow normal patterns—she’s been blowing them up her entire career.” That appointment gave Su a front-row seat for a corporate turnaround that is now a classic business school case study—a rejuvenation ignited, in part, by leaning on the company’s scale and creating a culture devoted to customers. Gerstner grew the stalled IBM’s market cap nearly sixfold in his almost nine-year run. Su also got a taste for dealmaking, helping IBM sign a joint deal with Sony and Toshiba in 2001 to put its chips in Sony’s PlayStation 3. Early on, she sometimes worried she wasn’t qualified to sit at tables packed with business heavyweights, but Su soon realized that her hard-learned technical acumen gave her an edge over executive-track types. “I saw that MIT Ph.D.’s worked for Harvard MBAs, and the truth is that made absolutely no sense to me,” she said in a 2017 graduation speech at her alma mater, whose new nanotech laboratory now carries her name. In late 2011, Nick Donofrio, at the time an AMD board member whom Su had met at IBM, called up his old protégée, who by then was a senior vice president of Free-scale, the Austin-based chipmaker that is now part of NXP Semiconductors. The two met for dinner, and over a bottle of Brunello, he made his pitch: an opportunity not just to chase incremental improvements, but to reinvent and innovate—with the ground cover to actually do it. A few days later, Su accepted a role as senior vice president of AMD’s global business units. Two years after she started, she was running the entire company—making her the first female CEO of a major semiconductor company. “I would walk into rooms where there were, like, 25 people, and I might have been the only woman,” she recalls of her early engineering days. “Where I have a lot of passion is young women engineers—keeping them in engineering.” “AMD, when I joined, was not really a significant part of our ecosystem—at all. And it is a credit to Lisa that they are a very important partner for us now.” 
 When Su first took over, she flew to Beverly Hills and personally asked AMD’s Sanders to speak to her team. Sanders says he was touched by Su’s offer but declined. “It’s not my team now. It’s your team,” he recalls telling her. Ever the salesman, though, he also made a counteroffer: He’d make the visit once the company had hit two years of profitability. In 2019, coinciding with the company’s 50th anniversary, Sanders made good on that promise. Semiconductor hotshot Mark Papermaster, who led the iPhone and iPod engineering teams at Apple and who had joined AMD around the same time as Su, has been a keen observer of the company’s remarkable comeback under her leadership. At Apple, Papermaster had worked for another skilled turnaround artist: cofounder Steve Jobs, who saved the company from catastrophe and set it on the path that would make it the world’s most valuable company. “What Lisa had, in a number of ways, was an even more difficult task,” Papermaster says. “When you’re not a founder, you have to establish your own credibility and your own vision, and bring the entire company, your customers and your investors with you.” Su’s success at AMD has made her an inspiration for young engineers and a hero to investors. It has also made her into a meme: A few years ago, 8-bit animations in which Su uses AMD’s Ryzen chips to morph into a superhero or shoot lasers from her eyes went viral on Twitter. A figurine of her clad in orange-and-red armor, helmet at her side, features prominently on her office shelf, a gift from a fan at the E3 gaming conference. “That’s probably one of the funnier moments in my career,” says Su, who, while a keen Twitter and Reddit user, isn’t “big on memes. It’s not my thing.” NOW THAT SU has renewed and energized AMD, she’s focused on ensuring its future in a highly competitive market. While she diligently rebuilt its business, Nvidia cofounder and CEO Jensen Huang was hard at work making his company the go-to vendor for artificial intelligence computing power. Huang, who is a distant relative of Su’s (“his mother is sisters with my grandfather,” she says), sees a gold mine in selling the chips to buttress AI tools like Chat­GPT. Demand has already catapulted Nvidia’s share price to near all-time highs with a forward P/E of around 64x—nearly double AMD’s. “It’s why investors are looking at AMD: because they want the poor man’s Nvidia,” says Stacy Rasgon, an analyst at Bernstein. “Maybe the market is so big they don’t need to be competitive.” Su is in a good position to take a run at the AI chip market. But she knows well how quickly turnarounds can become downfalls.
 But Su intends it to be. And she hopes to take on Nvidia’s AI-centric H100 GPUs by betting on annual chip upgrades meant to burnish AMD’s position. Under her leadership, R&D spending has risen nearly fourfold, to $5 billion—almost as much as AMD’s entire revenue when she took over. A new supercomputer at Tennessee’s Oak Ridge National Laboratory—the fastest in the world when completed in 2022—is Su’s passion project. The groundbreaking machine was built to have the processing power of at least a quintillion calculations per second and is a showcase for AMD’s AI chips. She’s throwing a curveball as well: The MI300 chip, which fuses CPUs with GPUs in a bid to counter Nvidia’s new superchip, will ship later this year. She has also been maneuvering against Nvidia with acquisitions, such as her $48.8 billion takeover in 2022 of Xilinx, a company that makes programmable processors that help speed up tasks like video compression. As part of the deal, Victor Peng, Xilinx’s former CEO, became AMD’s president and leader of AI strategy. Beyond Nvidia lurk other emerging threats: Some of AMD’s customers have begun doing chip development of their own—a move designed to mitigate their dependence on the semiconductor giants. Amazon, for example, designed a server chip in 2018 for its AWS business. Google has spent nearly a decade developing its own AI chips, dubbed Tensor Processing Units, to help “read” the names of the signs captured by its roving Street View cameras and provide the horsepower behind the company’s Bard chatbot. Even Meta has plans to build its own AI hardware. Su shrugs off concerns that her customers could someday be competitors. “It’s natural,” she says, for companies to want to build their own components as they look for efficiencies in their operations. But she thinks they can do only so much without the technical expertise AMD has built over the decades. “I think it’s unlikely that any of our customers are going to replicate that entire ecosystem.” Su is in a good position to take a run at the AI chip market. But she knows well how quickly turnarounds can become downfalls. There’s more work to be done to ensure AMD endures: “I think there’s another phase for AMD. We had to prove that we were a good company. I think we’ve done that. Proving, again, that you’re great, and that you have a lasting legacy of what you’re contributing to the world, those are interesting problems for me.”  ",33,2967,0.338638335466385,0.326591491699219,0.334770143032074
"From the technology upsetting jobs and causing intellectual property issues to models making up fake answers to questions, here’s why we’re concerned about generative AI. Tulika Bose: Last week, Google announced the new products and features coming from the company. And it was AI all the way down. Sophie Bushwick: AI features are coming to Google’s software for email, word processing, data analysis—and of course searching the web. Bose: This follows Microsoft’s previous announcements that it also plans to incorporate generative AI into its own Bing search engine and Office Suite of products.  Bushwick: The sheer volume of AI being introduced, and the speed with which these features are rolling out, could have some, uh, unsettling consequences. This is Tech, Quickly, a tech-flavored version of Scientific American’s Science, Quickly podcast. I’m Sophie Bushwick. Bose: And I’m Tulika Bose.  [MUSIC] Bose: Sophie, hasn’t Google had AI in the works for a long time? What’s the big problem?  Bushwick: That’s true. Actually, some of the basic principles that were later used in programs like OpenAI’s GPT-4, those were actually developed in-house at Google. But they didn’t want to share their work, so they kept their own proprietary large language models and other generative AI programs under wraps. Bose: Until OpenAI came along. Bushwick: Exactly. So ChatGPT becomes available to the public, and then the use of this AI-powered chatbot explodes. (wow) AI is on everyone’s mind. Microsoft is using a version of this ChatGPT software in its search engine, Bing. And so Google, to stay competitive, has to say, hey, we’ve got our own version of software that can do the same thing. Here’s how we’re going to use it. Bose: It feels like all of a sudden AI is moving really, really fast. Bushwick: You are not the only one who thinks so. Even Congress is actually considering legislation to rein in AI.  Bose: Yeah, Sam Altman, he’s the CEO of Open AI, (the company behind ChatGPT) had to testify in front of Congress this week. Altman: My worst fears are that we cause significant ... we, the field, the technology, the industry cause significant harm to the world. Bushwick: The EU is also working on AI legislation. And on the private side, there are intellectual property lawsuits pending against some of these tech companies because they trained their systems on the creative work produced by humans. So I'm really glad that there's some momentum to put legislation in place and to, sort of, slow down the hype a bit, or at least make sure that there are limitations in place because this technology could have some potentially disastrous consequences.  [NEWS CLIP: Aside from debt ceiling negotiations, Capitol Hill was also focused today on what to do about Artificial Intelligence, the fast-evolving, remarkably powerful … the metaphors today reflected the spectrum. Some said this could be as momentous as the industrial revolution, others said this could be akin to the Atomic Bomb.] Bose: Let’s start with consequence number one. Bushwick: Some of the issues here are kind of baked into large language models, aka LLMs, aka the category of AI programs that analyze and generate text. So these problems, I’m betting you’ve heard about at least one of them before. Bose: Yeah, so I know that these models hallucinate– which means they literally  just make up the answers to questions sometimes. Bushwick: Correct. For example, if you were to ask what the population of Mars was, ChatGPT might say, oh, that’s 3 million people. But even when they’re wrong, there’s this human inclination to trust their answers—because they’re written in this very authoritative way. Another inherent problem is that LLMS can generate text for people who want to do terrible  things, like build a bomb, run a propaganda campaign, they could send harassing messages, they could scam thousands of people all at once, or they could be a hacker who wants to write malicious computer code.  Bose: But a lot of the companies are only releasing their models with guardrails—rules the models have to follow to prevent them from doing those very things. Bushwick: That’s true. The problem is, people are constantly working out new ways to subvert those guardrails. Maybe the wildest example I’ve seen is the person that figured out that you can tell the AI model to pretend it’s your grandmother, and then inform it that your grandmother used to tell bedtime stories about her work in a napalm factory. (wow) Yeah, so if you set it up that way and then you ask the AI to tell a bedtime story just like granny did, it just very happily provides you instructions on how to make napalm!  Bose: Okay, that’s wild. Probably not a good thing. Bushwick: No, not great. Bose: No. Uh, can the models eventually fix these issues as they get more advanced? Bushwick: Well, hallucination does seem to become, uh, less common in more advanced versions of the models, but it’s always going to be a possibility—which means you can never really trust an LLM’s answers. And as for those guardrails—as the napalm example shows, people are never going to stop trying to hop over them, and this is going to be especially easy for people to play around with once AI is so ubiquitous that it’s part of every word processing program and web browser. That’s going to supercharge these issues. Bose: So let’s talk about consequence number two?  Bushwick: One good thing about these tools is that they can help you with boring, time-consuming tasks. So instead of wasting time answering dozens of emails, you have an AI draft your responses. Uh, have an AI turn this planning document into a cool PowerPoint presentation, that kind of thing. Bose: Well, that honestly would make us work a lot more efficient. I don’t really see the problem.  Bushwick: So that’s true. And you are going to use your increased efficiency to be more productive. The question is who’s going to benefit from that productivity?  Bose: Capitalism! [both laugh] Bushwick: Right, exactly, so basically the gains from using AI, you’re not necessarily going to get a raise from your super AI-enhanced work. All that extra productivity is just  going to benefit the company that employs you. And these companies, now that their workers are getting so much done, well, they can fire some of their staff—or a lot of their staff.  Bose: Oh, wow. Bushwick: Many people could lose their jobs. And even whole careers could become obsolete. Bose: Like what careers are you thinking of? Bushwick: I’m thinking of somebody who writes code or is maybe an entry-level programmer, and they’re writing pretty simple code. I can see that being automated through an AI. Uh, certain types of writing. Um, I don’t think that AI is necessarily going to be writing a feature article for Scientific American or to be capable of doing that, but AI is already being used to do things like simple news articles based on sports games or financial journalism that’s about changes in the market. Some of these changes happen pretty regularly, ​​and so you could have sort of a rote form that an AI fills out.  Bose: That makes sense. Wow, that's actually really scary. Bushwick: I definitely think so. In our career, I definitely find that scary. Like I said, AI can't do everything. It can't write as well as a professional journalist, but it might be that a company says, well, we're gonna have AI write the first draft of- of all of these pieces, and then we're gonna hire a human writer to edit that work, but we're gonna pay them a much lower salary than we would if they just wrote it in the first place. And that's an issue because it takes a lot of work to edit some of the stuff that comes out of these models because like I said,  they're not necessarily writing, the way that a professional human journalist or writer would. Bose: That sounds like a lot of what’s going on with the Hollywood writer’s strike. Bushwick: Yes, one of the union’s demands is that studios not replace human writers with AI. Bose: I mean, ChatGPT is good. But it can’t write, like, I don’t know, Spotlight, all on its own—not yet anyway. Bushwick: I totally agree! And I think that the issue isn't that AI is going to replace me in my job. I think it's that for some companies, that quality level doesn't necessarily matter. If what you care about is cutting costs, then a mediocre but super cheap imitation of human creativity might just be good enough.  Bose: Okay. So I guess we could probably figure out who we're gonna benefit, right? Bushwick: Right, the ones on top are going to be reaping the benefits, the financial benefits of this. And that also is true with this AI rush and tech companies. So it takes a lot of resources to train these very large models that are then used as the basis for other programs built on top of them. And the ability to do that is concentrated in already powerful tech giants like Google. And right now a lot of companies that work on these models have been making them pretty accessible to researchers and developers. Uh, they make them open access. For instance, Meta has made its large language model called LLaMa really easy for researchers to explore and to study. And this is great because it helps them understand how these models work. It could potentially help people catch flaws and biases in the programs. But because of this newly competitive landscape, because of this rush to get AI out there, a lot of these companies are starting to say, well, maybe we shouldn't be so open. And if they do decide to double down on their competition and limit open access, that would further concentrate their power and their control over this newly lucrative field. Bose: What’s consequence number three? I'm kind of starting to get scared here. Bushwick: So this is a consequence that is really important for you and me, and it has to do with the change in search engines, the idea that when you type in a query, instead of giving you a list of links, it's going to generate text to answer your question. A lot of the traffic to Scientific American’s website comes because someone searches for something like artificial intelligence on Google, and then they see a link to our coverage and click it.  Bose: Mm-hmm. Bushwick: Now Google has demonstrated a  version of their search engine that uses generative text, so it still has a list of links beneath the AI generated answer, and the answer itself cites some of its sources and links out to them. But a lot of people are just gonna see the AI-written answer, read it, and move on.  Why would they go all the way to Scientific American when it’s so easy to just read a regurgitated summary of our coverage? Bose: I mean, if people stop clicking through to media websites, that could seriously cut down on website traffic, which would reduce advertising revenue, which a lot of publications rely on. And it also sounds like, basically, this is aggregation. Bushwick: In—in a way it is. It's relying on the work that humans did and taking it and remixing it into the AI-written answer. Bose: What could happen to original reporting if this happens? Bushwick: You could picture a future of the Internet where most of the surviving publications are producing a lot of AI-written content ‘cause it's cheaper and it doesn't really matter in this scenario that it's lower quality, that maybe it doesn't have as much original reporting and high quality sources as the current best journalistic practices would call for. But then you could say, well, what are Google's answers now gonna be drawn from? What is its AI program gonna pull from in order to put together its answer to your search engine query? And maybe it's gonna be an AI pulling from AI, and it's gonna just be lower quality information (mm-hmm) And it, it's gonna suck. It's gonna be terrible. [laughs] Bose: Yeah.... Bushwick: This is the worst case scenario, right? So not for sure that would play out, but I could see that as a possibility. Sort of a—internet as a wasteland, AI tumbleweeds blowing around and getting tangled up in the Google search engine. Bose: That sounds terrible. Bushwick: Don’t—don’t really love that. Bose: We've talked about some truly terrible things so far, but there's a consequence number four, isn't there? Bushwick: Yes. This is the science fiction doomsday scenario. AI becomes so powerful, it destroys humanity.  Bose: Okay, you mean like Hal, 2001 Space Odyssey?  Bushwick: Sure, or Skynet from the Terminator movies or uh, whatever the evil AI is called in the Matrix. I mean, the argument here isn't that you're gonna have like, uh, you know, an Arnold-shaped evil robot coming for us, it's a little bit more real world than that.  But the basic idea is that AI is already surpassing our expectations about what it can do. These large language models are capable of things like passing the bar exam, um, they’re able to do math, which is not something they were trained to do, and in order to do these things called emergent abilities, researchers are surmising that they might possibly be doing something like developing an internal model of the physical world (wow) in order to solve some of these problems. So some researchers like most famously Geoffrey Hinton— Bose: Also known as the godfather of AI— Bushwick: Yeah. He's been in the news a lot recently because he just recently resigned his position at Google. Um, (okay) so Hinton actually helped develop the machine learning technique that has been used to train all of these super powerful models. And he's now sounding the alarm on AI. And so one of the reasons he stepped down from Google was so he could speak for himself without being a representative of the company when he's talking about the potential negative consequences of AI. Geoffrey Hinton: I think it’s quite conceivable that humanity is just a passing phase in the evolution of intelligence. You couldn’t directly evolve digital intelligence; it requires too much energy and too much careful fabrication. You need biological intelligence to evolve, so that it can create digital intelligence. Digital intelligence can then absorb everything people ever wrote, in a fairly slow way which is what ChatGPT’s been doing, but then it can start getting direct experience of the world and learn much faster. They may keep us for awhile to keep the power stations running, but after that maybe not. Bose: So AI surpassing us could be bad. How likely is it really?  Bushwick: I don’t want to just dismiss this idea as catastrophizing. Hinton is an expert in this field and I think the idea that AI could become powerful and then could be given sort of enough initiative to do something negative—it doesn't have to be, you know, a sentient robot, right in order to, to come to some pretty nasty conclusions. Like, if you create an AI and tell it, your goal is to maximize the amount of money that this bank makes, you could see the AI maybe deciding, well, the best way to do this is to destroy all the other banks (right) because then people will be forced to use my bank. Bose: Okay. Bushwick: Right? So if- if you give it enough initiative, you could see an AI following this logical chain of reasoning to doing horrible things. (Oh my gosh) Right, without guardrails or other limitations in place. But I do think this catastrophic scenario, it’s—for me, it’s less immediate than the prospect of an AI-powered propaganda or scam campaign or, um, the disruption that this is gonna cause to something that was formerly a stable career or to, you know, the destruction of the internet as we know it, etcetera. (Wow) Yeah, so for me, I worry less about what AI will do to people on its own (mm-hmm) and more about what some people will do to other people using AI as a tool. Bose: Wow, okay. Um [laughs] When you put it that way, the killer AI doesn’t sound quite so bad. Bushwick: I mean, halting the killer AI scenario, it would take some of the same measures as halting some of these other scenarios. Don't let the rush to implement AI overtake the caution necessary to consider the problems it could cause and to try to prevent them  before you put it out there. Make sure that there are some limitations on the use of this technology and that there's some human oversight  over it. And I think that is what legislators are hoping to do. That's the reason that Sam Altman is testifying before Congress this week, and I just would hope that  they actually take steps on it because there's a lot of other tech issues, like, for example, data privacy that Congress has raised an alarm about, but not actually passed legislation on.  Bose: Right. I mean, this sounds like it's a big deal. Bushwick: This is absolutely a big deal. Bushwick: Science, Quickly is produced by Jeff DelViscio, Tulika Bose and Kelso Harper. Our theme music was composed by Dominic Smith. Bose: Don’t forget to subscribe to Science, Quickly wherever you get your podcasts. For more in-depth science news and features, go to ScientificAmerican.com. And if you like the show, give us a rating or review! Bushwick: For Scientific American’s Science, Quickly, I’m Sophie Bushwick.  Bose: I’m Tulika Bose. See you next time!   Sophie Bushwick is an associate editor covering technology at Scientific American. Follow her on Twitter @sophiebushwick Credit: Nick Higgins Tulika Bose is senior multimedia editor at Scientific American. Follow Tulika Bose on Twitter Discover world-changing science. Explore our digital archive back to 1845, including articles by more than 150 Nobel Prize winners. Follow us Scientific american arabic © 2023 Scientific American, a Division of Springer Nature America, Inc. All Rights Reserved. Support science journalism. Thanks for reading Scientific American. Knowledge awaits. Already a subscriber? Sign in. Thanks for reading Scientific American. Create your free account or Sign in to continue. See Subscription Options Continue reading with a Scientific American subscription. You may cancel at any time.",91,3085,0.378174751996994,0.286003053188324,0.33582216501236
"Generative tools like ChatGPT and Stable Diffusion have got everyone talking about artificial intelligence (AI) – but where is it headed next? The Future Of Generative AI Beyond ChatGPT  It’s already clear that this exciting technology will have a big impact on the way we live and work. UK energy provider Octopus Energy has said that 44% of its customer service emails are now being answered by AI. And the CEO of software firm Freshworks has said that tasks that previously took eight to 10 weeks are now being completed in days as a consequence of adopting AI tools into its workflows. But we’re still only at the beginning. In the coming weeks, months, and years we will see an acceleration in the pace of development of new forms of generative AI. These will be capable of carrying out an ever-growing number of tasks and augmenting our skills in all manner of ways. Some of them may seem as unbelievable to us today as the rise of ChatGPT and similar tools would have done just a few months back. So, let’s take a look at some of the ways we can expect generative AI to evolve in the near future and some of the tasks it will be lending a hand with before too long:  Beyond ChatGPT Text-based generative AI is already pretty impressive, particularly for research, creating first drafts, and planning. You might have had fun getting it to write stories or poems, too, but probably realized it isn’t quite Stephen King or Shakespeare yet, particularly when it comes to coming up with original ideas. Next-generation language models – beyond GPT-4 – will understand factors like psychology and the human creative process in more depth, enabling them to create written copy that’s deeper and more engaging. We will also see models iterating on the progress made by tools such as AutoGPT, which enable text-based generative AI applications to create their own prompts, allowing them to carry out more complex tasks.  Generative Visual AI As well as text, current generative AI technology is quite good at creating images based on natural language prompts, and there are even some tools that use it to generate video. However, they have some limitations due to the intensive nature of the required data processing. As this domain of generative AI becomes more advanced, it’s likely that it will become easy to create images and videos of just about anything, to the extent that it becomes difficult to distinguish generative AI content from reality. This could lead to issues such as deepfakes becoming problematic, resulting in the spread of fake news and disinformation.  Generative AI in the Metaverse There are many predictions about how the way we interact with information and each other in the digital domain will involve. Many of these focus on immersive, 3D environments and experiences that can be explored through virtual and augmented reality (VR/AR). Generative AI will speed up the design and development of these environments, which is a time and resource-intensive process, and Meta (formerly Facebook) has indicated that this could play a part in the future of its 3D worlds platforms. Additionally, generative AI can be used to create more lifelike avatars that help to bring these environments to life, capable of more dynamic actions and interactions with other users.  Generative Audio, Music, and Voice AI AI models are already impressively capable when it comes to generating music and mimicking human voices. In music, generative AI is likely to increasingly become an invaluable tool for songwriters and composers, creating novel compositions that can serve as inspiration or encourage musicians to approach their creative process in new ways. We are also likely to see it being used to create real-time, adaptive soundtracks – for example, in video games or even to accompany live footage of real-world events such as sports. AI voice synthesis will also improve, bringing computer-generated voices closer to the levels of expression, inflection, and emotion conveyed by a human voice. This will open new possibilities for real-time translation, audio dubbing, and automated, real-time voiceovers and narrations.   Generative Design AI can be used by designers to assist in prototyping and creating new products of many shapes and sizes. Generative design is the term given for processes that use AI tools to do this. Tools are emerging that will allow designers to simply enter the details of the materials that will be used and the properties that the finished product must have, and the algorithms will create step-by-step instructions for engineering the finished item. Airbus engineers used tools like this to design interior partitions for the A320 passenger jet, resulting in a weight reduction of 45% over human-designed versions. In the future, we can expect many more designers to adopt these processes and AI to play a part in the creation of increasingly complex objects and systems.  Generative AI in Video Games Generative AI has the potential to significantly impact the way video games are designed, built, and played. Designers can use it to help conceptualize and build the immersive environments that games use to challenge players. AI algorithms can be trained to generate landscapes, terrain, and architecture, freeing up time for designers to work on engaging stories, puzzles, and gameplay mechanics. It can also create dynamic content – such as non-player characters (NPCs) that behave in realistic ways and can communicate with players as if they are humans (or orcs or aliens) themselves, rather than being restricted to following scripts. Once game designers get to grips with implementing generative AI into their workflows, we can expect to see games and simulations that react to players’ interactions on the fly, with less need for scripted scenarios and challenges. This could potentially lead to games that are far more immersive and realistic than even the most advanced games available today.  To stay on top of the latest on new and emerging business and tech trends, make sure to subscribe to my newsletter, follow me on Twitter, LinkedIn, and YouTube, and check out my books Future Skills: The 20 Skills and Competencies Everyone Needs to Succeed in a Digital World and The Future Internet: How the Metaverse, Web 3.0, and Blockchain Will Transform Business and Society. ",24,1045,0.325243681669235,0.316784918308258,0.357971370220184
"HP Inc. (HPQ) CEO Enrique Lores says consumers and businesses are poised to see big changes to their computers within 24 months due to new advances in artificial intelligence. ""What we are working on is to build AI capabilities into the PC,"" Lores told analysts on a late Tuesday earnings call. ""So, consumers or professionals will be able to run AI applications at the edge and will not have to run them on the cloud."" ""The benefit this will bring is that if you're a small company and you want to use some of your private data in an AI application, you will not have to upload it, you will be able to run it locally. And also there will be advantages in cost and advantages in latency,"" he said. Lores added HP is working closely with chipmakers to integrate to new AI designs into its computers. The efforts will be evident soon, Lores said. ""There's going to be a significant change. Customers will start seeing some of these solutions available in 2024 about 12 months, 20 months from now, and it's going to be a huge opportunity to really bring energy to the category,"" Lores explained. A heavy dose of energy is badly needed in the computing market. HP saw unit sales under pressure in its PC and printing segments in the most recent quarter as businesses and consumers continued to closely manage their finances after the pandemic. Fiscal second quarter net sales plunged 21.7% from the prior year to $12.9 billion, just shy of estimates for $13.03 billion. Sales in the personal systems (PCs) segment tanked 29% from the year-ago period, missing analyst estimates by about $200 million. Printing sales fell 5% year over year. With HP's various cost-cutting efforts over the past year, margins in both businesses managed to stay intact. HP shares dipped by about 5% in pre-market trading on Wednesday as investors locked in on the unit declines in PCs amid a softening economy. But going somewhat overlooked is the potential for the first half of 2023 to mark the bottom for PC makers like HP. Amid the rise in AI, consumers and businesses will likely need to refresh their pandemic-era computers to stay competitive. US PC shipments in the fourth quarter of 2023 are expected to grow 6% year on year, according to new data from industry researcher Canalys. Full-year shipments in 2024 are forecasted to be 13% higher than in 2023. Wall Street sees this refresh cycle as an inevitable earnings tailwind for HP. ""We view management’s commentary on the earnings call around much lower inventory levels to work down as well as a seasonal rebound in PC volumes led by consumer demand as largely in line with our own checks, and expect the seasonal upside to follow as long as there is no further deterioration in the macro- economy,"" said JP Morgan analyst Samik Chatterjee in a research note. Chatterjee added: ""Further driving our confidence in the volume projections are the higher installed base of PC volumes and robust replacement cycle with priority given to high-end features on PCs."" Brian Sozzi is Yahoo Finance's Executive Editor. Follow Sozzi on Twitter @BrianSozzi and on LinkedIn. Tips on the banking crisis? Email brian.sozzi@yahoofinance.com Click here for the latest stock market news and in-depth analysis, including events that move stocks Read the latest financial and business news from Yahoo Finance Related Quotes (Bloomberg) -- Things are going from bad to worse for Chinese equities, with a key index tumbling into a bear market as disappointing manufacturing data added to the bleak outlook.Most Read from BloombergChina Is Drilling a 10,000-Meter-Deep Hole Into Earth’s CrustWinklevoss Twins Attempt Pivot After Gemini Loses Money and EmployeesTwitter Is Now Worth Just 33% of Elon Musk’s Purchase Price, Fidelity SaysJPMorgan Builds Unit for World’s Richest Families in Wealth BetMcCarthy Confident on Debt Vo US stocks opened lower Wednesday morning, as investors kept a watchful eye on the prospects for the debt-limit deal in an expected House floor vote later. Meanwhile, China’s economic woes pressured global markets. (Bloomberg) -- Jamie Dimon said pursuing a political career has crossed his mind amid speculation about what might come next for the chief executive officer of JPMorgan Chase & Co.Most Read from BloombergChina Is Drilling a 10,000-Meter-Deep Hole Into Earth’s CrustWinklevoss Twins Attempt Pivot After Gemini Loses Money and EmployeesTwitter Is Now Worth Just 33% of Elon Musk’s Purchase Price, Fidelity SaysJPMorgan Builds Unit for World’s Richest Families in Wealth BetMcCarthy Confident on Debt Vo Legislation brokered by President Joe Biden and House Speaker Kevin McCarthy to lift the $31.4 trillion U.S. debt ceiling and achieve new federal spending cuts passed an important hurdle late on Tuesday, advancing to the full House of Representatives for debate and an expected vote on passage on Wednesday. HP Inc. managed to beat on earnings amid a focus on managing its costs. IBM CEO Arvind Krishna announced a hiring pause earlier this month, but that’s not all. He also stated the company plans to replace nearly 8,000 jobs with AI. Krishna noted that back-office functions, specifically in the human resources (HR) sector, will be the first to face these changes. See Next: Gamers Making Thousands Selling Gaming Skins And Assets: Gameflip's Bold Vision For The Future Of Gaming Commerce The transition will happen gradually over the next few years, with machines potential Mester's comments come after some Fed policymakers hinted they may support a pause in raising rates in June to assess the impact of the U.S. central bank's policy tightening so far. The current deal wouldn't end the debt-ceiling standoff until the beginning of 2025. The next thing Congress should do is repeal the debt ceiling altogether. He sticks to these four indicators regardless of his strategy. His versatile approach has allowed him to remain profitable in different markets. The company laid out a future in which it plays a key role in the emerging artificial intelligence market. (Bloomberg) -- Whether they favor debt-limit brinkmanship or not, most Americans are alarmed by the level of US government borrowing. Treasury Secretary Janet Yellen isn’t one of them.Most Read from BloombergChina Is Drilling a 10,000-Meter-Deep Hole Into Earth’s CrustWinklevoss Twins Attempt Pivot After Gemini Loses Money and EmployeesTwitter Is Now Worth Just 33% of Elon Musk’s Purchase Price, Fidelity SaysJPMorgan Builds Unit for World’s Richest Families in Wealth BetMcCarthy Confident on Deb As beer drinkers flock away from Bud Light, investors are losing their taste for InBev stock. Pop quiz: True or False – Social Security retirement benefits are subject to income tax just like withdrawals from a traditional IRA account. If you answered ""False,"" you're smarter than more than 60% of your fellow Americans nearing retirement. In … Continue reading → The post Can You Pass This Social Security Quiz? Nearly 7 in 10 People Struggle With These Basic Questions appeared first on SmartAsset Blog. The new, shortened week has begun with at least one big overhang removed, as the debt ceiling battle looks to be behind us. This past weekend, House leader McCarthy announced that he had reached a compromise with the Biden Administration acceptable to his Republican caucus, and a bill could come to a Congressional vote this week. In other news, core inflation is still elevated, the labor market is strong, and the housing data may be turning upwards – all pointing toward the Fed keeping interest The S&P's comeback this year gets only two cheers. A few big stocks are overperforming, while the rest of the market reflects the reversal of a historic earnings blowout. Dow Jones futures fell Wednesday ahead of this evening's House vote on the debt-ceiling deal. Hot AI stock C3.ai dropped ahead of earnings. The S&P 500 dipped by 19% in 2022, but stocks still don’t seem cheap to Charlie Munger, Warren Buffett’s billionaire partner at Berkshire Hathaway. Don't Miss: Why Jason Calacanis and Other Silicon Valley Elites Are Betting On This Startups Vision For Re-Uniting American Families “In my whole adult life, I have never hoarded cash, waiting for better conditions,” Munger said in an interview in late 2022. “I’ve just invested in the best thing I could find.” Yet he acknowledged that Berkshire Hatha Such is the paradox that now confronts investors after  Nvidia  recent earnings report inadvertently created a large-scale ethics experiment on Wall Street.  On Tuesday, days after Nvidia (ticker: NVDA) stunned investors with a financial forecast that some have said augurs a new Industrial Revolution, AI experts released an ominous warning.  Shares of Nvidia, which makes semiconductor chips that are used to power AI, have continued to surge ever higher. Many investors will be tuned into C3.ai's release, as artificial intelligence has become the hot item on Wall Street. Can the momentum behind AI sustain itself? Warren Buffett bought 4.66 million OXY shares for about $275 million over May 25-30. Berkshire Hathaway now owns 24.9% Occidental Petroleum.",1,1485,0.305249929428101,0.300152003765106,0.394598007202148
"Is AI coming for your job? If so, when? Unfortunately, there are no clear-cut answers to these questions. Technology develops in unpredictable ways. But a paper published last month by three scholars — Princeton’s Edward W. Felten, Manav Raj of the University of Pennsylvania and Robert Seamans of New York University — offered some helpful insight, at least in terms of artificial intelligence as we now know it. The team looked into two types of AI: One capable of generating and analyzing speech (think ChatGPT) and the other with the same capacity for images (think Midjourney). It examined dozens of skills that humans use to perform their jobs, from writing to reasoning to lifting heavy things, for the potential for artificial intelligence to enhance those skills or to supplant humans entirely. The chart below provides an example of their analysis of professions related to art, design, entertainment, sports and media: Researchers developed the AI Exposure score by connecting 10 AI applications to 52 human skills across approximately 800 professions. * Jobs with $0 median salary are due to occupation code change or lack of data. Each dot in this chart represents a profession. Orange dots show the potential impact of AI language models such as ChatGPT. Public relations specialists will probably be significantly affected by language models, dancers likely won’t. Green dots show the potential impact of AI image generators. Interior designers probably will be greatly impacted by image models. Dancers, again, are likely to be least affected. Language models are more likely to impact jobs that rely on communication and language skills. While image generators are more likely to alter professions that depend more on visual or spatial abilities. Many occupations may be affected by both. Higher exposure doesn’t necessarily mean these workers will be replaced by machines, however. In some areas, artificial intelligence could give workers a productivity boost. In others, it could lead to automation and job loss. For example, while people in telemarketing are clearly vulnerable to chat bots, lawyers might use artificial intelligence to help them speed their work and get more done. Now that you understand how this works, let’s see how artificial intelligence could affect your own profession. Tap on each dot to reveal the job Bottom line: Highly skilled, highly educated white-collar occupations, ranging from architects to astronomers to judges, are most likely to see change as a result of the development of artificial intelligence. A recent study by researchers from OpenAI, the company behind ChatGPT, estimated that 80 percent of the U.S. workforce would have its work-related tasks at least 10 percent affected by language models. One in 5 would see at least half of daily tasks affected by artificial intelligence, the researchers found. In some cases, workers will need to learn new software that uses artificial intelligence to expedite their tasks and increase their productivity. Other jobs might go extinct entirely. The charts below give some hint of the potential ramifications for different industries. Workers in jobs that require manual tasks such as food prep, fishing and construction are less likely to be severely affected: While jobs already done with the help of a computer might see more change. Highly skilled jobs, such as in engineering, software development, astronomy and law, are likely to experience more change than average: Most researchers hesitate to predict the speed in which artificial intelligence will be implemented. NYU’s Seamans, who is an associate professor of management and organizations, believes that it will still take a lot of time for the effect to grow large. “Whereas in 2019 [Elon] Musk was touting driverless fleets of Tesla by 2020 (still doesn’t exist), Chris Urmson, who knows AVs (autonomous vehicles) very well, was saying 30-50 years,” Seamans wrote in an email to us. Although it is clear that most jobs will be affected by artificial intelligence in one way or another, these changes won’t affect all workers at the same rate or at the same time. This gives us a bit of leeway to address some important questions: If technology allows one worker to do the work of five, who reaps the financial windfall from that? What role should the government play in fairly distributing the wealth that is created? Should we be taking universal basic income more seriously now, ahead of the disruptions that we know are coming? What can we do to help people adapt, learn new skills and find new jobs? Who will pay for that? This will not be the first time that new technology changes how we work, of course. From lamplighters to switchboard operators to video store clerks, professions have come and gone. We’ve adjusted. But artificial intelligence sure looks like a monster of a disrupter — even some of its lead creators are warning of its danger. There is still time to tame AI. We should slow it down to give people a chance to adapt and ensure that it will be a technology that benefits everyone, not just those already at the top of the food chain. Occupational exposure data by Edward W. Felten, Princeton University; Manav Raj, University of Pennsylvania; and Robert Seamans, New York University, “Occupational Heterogeneity in Exposure to Generative AI.” Salary data is from the “2022 Occupational Employment and Wage Statistics” by the U.S. Bureau of Labor Statistics.",95,883,0.333755224943161,0.334288895130157,0.331955939531326
"Join top executives in San Francisco on July 11-12, to hear how leaders are integrating and optimizing AI investments for success. Learn More  Execs at AmEx Digital Labs, the company’s innovation arm, say they likely won’t launch their own LLM and will instead rely on integrating existing offerings The wave of new business products and services launching with generative artificial intelligence ( generative AI) features isn’t slowing down, but many established companies are taking a measured approach when evaluating how to use and deploy the technology.  Among those exploring AI for business is American Express (AmEx), the financial services, credit card and remote concierge giant. Founded in the United States in 1850 originally as a physical goods delivery company, AmEx has transformed itself into a global fintech leader, steadily embracing new technologies throughout its nearly two centuries of existence. Now, confronted with the recent rollout and drumbeat of hype around OpenAI’s ChatGPT, Google’s Bard, Anthropic’s Claude and similar generative AI products built atop large language models, AmEx sees opportunity to use these technologies to better improve its customer experience across its credit cards and bank offerings for businesses and individuals. Transform 2023 Join us in San Francisco on July 11-12, where top executives will share how they have integrated and optimized AI investments for success and avoided common pitfalls.   “It may not be a standalone product this year, but we’ll be looking at direct-to-consumer or small-business services as part of our experimentation, and see what works and what doesn’t,” said Luke Gebb, executive vice president of American Express Digital Labs. AmEx Digital Labs is a division of AmEx formed in 2017 with the ethos of a startup, rapidly evaluating and testing new technologies and using them to incubate new products that make their way to the rest of the company. In fact, AmEx Digital Labs has already been at the forefront of using AI for financial services. This team of 100 technology experts scattered around the globe was responsible for integrating Mezi, an AI digital assistant that makes customer travel booking recommendations, into an AmEx mobile app pilot program. AmEx later acquired the company behind Mezi and wove its technology throughout some of its services.  “We launch 20 to 25 pilots a year,” said Laura Grant, VP of product development for emerging platforms and AI at AmEx Digital Labs. “We look at technology from the perspective of, ‘How is this going to impact our customers and make their lives better?’” The way AmEx Digital Labs works is that it first builds and prototypes products within its own small unit, before rolling it out to other parts of the company, ultimately turning over control to whatever team within AmEx is best equipped to lead that particular digital offering. For example, AmEx Digital Labs created the company’s Rewards Checking account, a high-yield (1% annual percentage yield) consumer banking offering wherein customers earn rewards points for every dollar spent with their debit cards, then turn those points back into direct deposits.  In addition, AmEx labs stood up the company’s “Pay with Bank Transfer” feature, which allows online merchants to include an AmEx-powered button among the payment options they accept. If a customer presses the “Bank Transfer” button, they’re prompted to securely and privately link their checking account at one of many supported banks, and pay directly for the product from it, avoiding credit card transaction fees. AmEx Digital Labs intends to explore the potential of generative AI in enhancing predictive analytics, helping them to better understand customer patterns and behaviors. In relation to AI applications, AmEx aims to focus on “predicting how our customers are going to perform over time, enabling better financial planning and decision-making,” said Gebb. Another major use case: customer sentiment analysis and customer interactions. “The way I think about AI is as we think about the customer experience,” said Grant. “What is it going to do to make people’s lives easier and customer experience better?” AmEx Digital Labs is currently exploring ways LLMs could be used “behind the scenes” to analyze all of the feedback and inquiries customers provide through AmEx’s existing customer service portals, as well as unofficially on social media, and to understand how to deliver appropriate and helpful responses.  Grant said that when approaching all new technology but especially AI, AmEx Digital Labs seeks to understand how it can help with its “3 Ps,” making a product more personalized to an individual customer, more proactive and more predictive. As for whether AmEx would seek to develop its own homemade LLM using its financial services data, following in the footsteps of the financial information and media giant Bloomberg launching its BloombergGPT with 50 million parameters a month ago, Gebb was dubious.  “Our hypothesis at the moment is that we would be better suited using LLMs through partnerships,” Gebb said, but added he did not have any specific ones to announce. “I don’t see us spinning up our own LLM from scratch.”  While AmEx Digital Labs sees a customer-facing generative AI based on an LLM as potentially useful, for now it is focused more on backend implementations. “It feels smarter initially that a human is looking at and reviewing the output” of an LLM, Grant said. Furthermore, because of the critical importance of its product offerings — affecting customers’ bank accounts and credit lines — and the associated hefty amount of government regulations in financial services, AmEx Digital Labs also makes sure to approach all of its experimentation with AI very cautiously and securely. Asked how AmEx might avoid the same fate of the three Samsung employees who reportedly shared sensitive, proprietary information with LLMs, Gebb said: “We are ring-fencing,” referencing the broader security approach that limits software applications and the data they can access.   “Not every employee can access LLMs through their employee laptop clear — only those who have been cleared and trained up on what they are doing.”  AmEx’s experiments with generative AI are clearly in their earliest phases, but that is in line with many of its peers.  A recent survey by KPMG found that 65% of 225 executives surveyed believe that generative AI will have a high or extremely high impact on their organization in three to five years, but 60% say they are still one to two years away from implementing their first solutions, and anticipate spending the next six to 12 months increasing their understanding of how generative AI works, evaluating internal capabilities and investing in new tools. The survey also showed that executive prioritization of generative AI varies significantly by sector. Most of the executives in technology, media, telecommunications, and healthcare and life sciences felt they have appropriately prioritized generative AI, while only 30% in consumer and retail said it was a priority. Respondents in technology, media, telecommunications and financial services said that researching generative AI applications is a high or extremely high priority in the next three to six months. VentureBeat's mission is to be a digital town square for technical decision-makers to gain knowledge about transformative enterprise technology and transact. Discover our Briefings. Want must read news straight to your inbox?
  © 2023 VentureBeat. All rights reserved.",46,1192,0.300766140222549,0.295572102069855,0.403661817312241
"Keeping up with an industry as fast-moving as AI is a tall order. So until an AI can do it for you, here’s a handy roundup of the last week’s stories in the world of machine learning, along with notable research and experiments we didn’t cover on their own. This week, movers and shakers in the AI industry, including OpenAI CEO Sam Altman, embarked on a goodwill tour with policymakers — making the case for their respective visions of AI regulation. Speaking to reporters in London, Altman warned that the EU’s proposed AI Act, due to be finalized next year, could lead OpenAI ultimately to pull its services from the bloc. “We will try to comply, but if we can’t comply we will cease operating,” he said. Google CEO Sundar Pichai, also in London, emphasized the need for “appropriate” AI guardrails that don’t stifle innovation. And Microsoft’s Brad Smith, meeting with lawmakers in Washington, proposed a five-point blueprint for the public governance of AI. To the extent that there’s a common thread, tech titans expressed a willingness to be regulated — so long as it doesn’t interfere with their commercial ambitions. Smith, for instance, declined to address the unresolved legal question of whether training AI on copyrighted data (which Microsoft does) is permissible under the fair use doctrine in the U.S. Strict licensing requirements around AI training data, were they to be imposed at the federal level, could prove costly for Microsoft and its rivals doing the same. Altman, for his part, appeared to take issue with provisions in the AI Act that require companies to publish summaries of the copyrighted data they used to train their AI models, and make them partially responsible for how the systems are deployed downstream. Requirements to reduce the energy consumption and resource use of AI training — a notoriously compute-intensive process — were also questioned. The regulatory path overseas remains uncertain. But in the U.S., the OpenAIs of the world may get their way in the end. Last week, Altman wooed members of the Senate Judiciary Committee with carefully-crafted statements about the dangers of AI, and his recommendations for regulating it. Sen. John Kennedy (R-LA) was particularly deferential: “This is your chance, folks, to tell us how to get this right … Talk in plain English and tell us what rules to implement,” he said. In comments to The Daily Beast, Suresh Venkatasubramanian, Brown University’s director of the Center for Tech Responsibility, perhaps summed it up it best: “We don’t ask arsonists to be in charge of the fire department.” And yet that’s what’s in danger of happening here, with AI. It’ll be incumbent on legislators to resist they honeyed words of tech execs and clamp down where it’s needed. Only time will tell if they do. Here are the other AI headlines of note from the past few days: Bill Gates may not be an expert on AI, but he is very rich, and he’s been right on things before. Turns out he is bullish on personal AI agents, as he told Fortune: “Whoever wins the personal agent, that’s the big thing, because you will never go to a search site again, you will never go to a productivity site, you’ll never go to Amazon again.” How exactly this would play out is not stated, but his instinct that people would rather not borrow trouble by using a compromised search or productivity engine is probably not far off base. Evaluating risk in AI models is an evolving science, which is to say we know next to nothing about it. Google DeepMind (the newly formed super entity comprising Google Brain and DeepMind) and collaborators across the globe are trying to move the ball forward, and have produced a model evaluation framework for “extreme risks” such as “strong skills in manipulation, deception, cyber-offense, or other dangerous capabilities.” Well, it’s a start.  Image Credits: SLAC Particle physicists are finding interesting ways to apply machine learning to their work: “We’ve shown that we can infer very complicated high-dimensional beam shapes from astonishingly small amounts of data,” says SLAC’s Auralee Edelen. They created a model that helps them predict the shape of the particle beam in the accelerator, something that normally takes thousands of data points and lots of compute time. This is much more efficient and could help make accelerators everywhere easier to use. Next up: “demonstrate the algorithm experimentally on reconstructing full 6D phase space distributions.” OK! Adobe Research and MIT collaborated on an interesting computer vision problem: telling which pixels in an image represent the same material. Since an object can be multiple materials as well as colors and other visual aspects, this is a pretty subtle distinction but also an intuitive one. They had to build a new synthetic dataset to do it, but at first it didn’t work. So they ended up fine-tuning an existing CV model on that data, and it got right to it. Why is this useful? Hard to say, but it’s cool.  Frame 1: material selection; 2: source video; 3: segmentation; 4: mask Image Credits: Adobe/MIT Large language models are generally primarily trained in English for many reasons, but obviously the sooner they work as well in Spanish, Japanese and Hindi the better. BLOOMChat is a new model built on top of BLOOM that works with 46 languages at present, and is competitive with GPT-4 and others. This is still pretty experimental so don’t go to production with it but it could be great for testing out an AI-adjacent product in multiple languages. NASA just announced a new crop of SBIR II fundings, and there are a couple interesting AI bits and pieces in there: Geolabe is detecting and predicting groundwater variation using AI trained on satellite data, and hopes to apply the model to a new NASA satellite constellation going up later this year. Zeus AI is working on algorithmically producing “3D atmospheric profiles” based on satellite imagery, essentially a thick version of the 2D maps we already have of temperature, humidity, and so on.  Image Credits: Zeus AI Up in space your computing power is very limited, and while we can run some inference up there, training is right out. But IEEE researchers want to make a SWaP-efficient neuromorphic processor for training AI models in situ. Robots operating autonomously in high-stakes situations generally need a human minder, and Picknick is looking at making such bots communicate their intentions visually, like how they would reach to open a door, so that the minder doesn’t have to intervene as much. Probably a good idea.",65,1097,0.34347477555275,0.326377719640732,0.330147504806519
"LULEA, Sweden, May 31 (Reuters) - The United States and European Union should push the artificial intelligence (AI) industry to adopt a voluntary code of conduct within months to provide safeguards while new laws are developed, EU tech chief Margrethe Vestager said on Wednesday. The European Union's AI Act, with rules on facial recognition and biometric surveillance, could be the world's first comprehensive legislation governing the technology, but is still going through the legislative process. ""In the best of cases it will take effect in two and a half to three years time. That is obviously way too late,"" Vestager told reporters before a meeting of the joint EU-U.S Trade and Technology Council in Sweden. ""We need to act now."" EU industry chief Thierry Breton said last week that Alphabet (GOOGL.O) and the European Commission aimed to develop an AI pact as concerns mount about the impact on society particularly from generative AI, like ChatGPT, that create content. Leaders of the G7 nations called earlier this month for the development of technical standards to keep AI ""trustworthy"", urging international discussions on topics such as governance, copyrights, transparency and the threat of disinformation Vestager said there needed to be agreement on specifics, not just general statements, suggesting the European Union and the United States could help drive the process. ""If the two of us take the lead with close friends, I think we can push something that will make us all much more comfortable with the fact that generative AI is now in the world and is developing at amazing speeds,"" she said. Vestager, a European Commission vice president, said a code of conduct come emerge quickly while governments and legislators from the EU to Canada to India establish rules. ""That is the kind of speed you need, to discuss in the coming weeks, a few months, and of course also involve industry ... in order for society to trust what is ongoing,"" she said. (This story has been refiled to fix a typographical error in paragraph 4) Our Standards: The Thomson Reuters Trust Principles. Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",25,537,0.337003111839294,0.315797060728073,0.347199827432632
"Markets  Fear & Greed Index  
            Latest Market News  
      Booming demand for its AI chips has landed Nvidia
            
                (NVDA) a prime spot in one of the most exclusive clubs on Wall Street: The chipmaker hit a market cap of $1 trillion on Tuesday. 
   
      It’s just the ninth company globally to have achieved such a feat, and only five other companies worldwide currently have the distinction: Apple
            
                (AAPL), Microsoft
            
                (MSFT), Alphabet
            
                (GOOG), Amazon
            
                (AMZN) and Saudi Aramco. 
   
      Nvidia took Wall Street by surprise last week when it reported gangbusters earnings and an exceedingly strong revenue forecast for the year ahead. Fueled largely by the recent boom in AI, the report pushed share price higher and secured its position as the one of the largest publicly traded companies in the world.
   
      Nvidia makes chips that power generative AI, a type of artificial intelligence that can create new content, such as text and images, in response to user prompts. That’s the kind of AI underlying ChatGPT, Google’s Bard, Dall-E and many of the other new AI technologies.
   
      Last quarter, the company’s profit surged 26% to $2 billion, and sales rose 19% to $7.2 billion, each easily surpassing Wall Street analysts’ forecasts. Nvidia’s outlook for the current quarter was also significantly — about 50% — higher than analysts’ predictions.
   
      Its stock is up about 180% so far this year.
   
      “Nvidia gave jaw-dropping robust guidance that will be heard around the world and shows the historical demand for AI happening now in the enterprise and consumer landscape,” said Dan Ives, an analyst at Wedbush Securities. “For any investor calling this an AI bubble (crypto, metaverse, now AI bubble thesis) we would point them to this Nvidia quarter and especially guidance which cements our bullish thesis around AI and speaks to the 4th Industrial Revolution now on the doorstep with AI.”
   
      Nvidia shares need to remain above $404.86 to retain a trillion dollar market cap. Shares of the stock are currently trading at $408.50.
   Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2023 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network.",6,722,0.376875191926956,0.298071980476379,0.325052857398987
"Markets  Fear & Greed Index  
            Latest Market News  
      The government needs to work faster to regulate AI, which has more potential for the good of humanity than any other invention preceding it, Brad Smith, Microsoft
            
                (MSFT) president and vice chair, said on CBS’ “Face the Nation” Sunday.
   
      Its uses are almost “ubiquitous” Smith said, “in medicine and drug discovery and diagnosing diseases, in scrambling the resources of, say, the Red Cross or others in a disaster to find those who are most vulnerable where buildings have collapsed,” the executive added.
   
      Smith also said AI isn’t as “mysterious” as many think, adding it is getting more powerful.
   
      
      Lawyer apologizes for fake court citations from ChatGPT
     
      “If you have a Roomba at home, it finds its way around your kitchen using artificial intelligence to learn what to bump into and how to get around it,” Smith said.
   
      Regarding concerns about AI’s power, Smith said any technology that exists today looked dangerous to people who lived before it.
   
      Smith said that there should be a safety break in place.
   
      Job disruptions due to AI will unfold over years, not months, Smith said.
   
      “For most of us, the way we work will change,” Smith said. “This will be a new skill set we’ll need to, frankly, develop and acquire.”
   
      To prevent instances like the fake photo of the explosion near the Pentagon, Smith said there needs to be a watermark system, or “use the power of AI to detect when that happens.”
   
      “You embed what we call metadata, it’s part of the file, if it’s removed, we’re able to detect it. If there’s an altered version, we in effect, create a hash. Think of it like the fingerprint of something, and then we can look for that fingerprint across the internet,” Smith said, adding a new path should be found to find a balance between regulating deepfakes and misleading ads and free expression.
   
      With a US presidential election year approaching and the ongoing threat of foreign cyber influence operations, Smith said the tech sector needs to come together with governments in an international initiative.
   
      Smith supports a new government agency to regulate AI systems.
   
      
      Welcome to the ""generative AI"" era. Resistance is futile 
     
      “Something that would ensure not only that these models are developed safely, but they’re deployed in say, large data centers, where they can be protected from cybersecurity, physical security and national security threats,” Smith said.
   
      Smith did not believe a six-month pause on AI systems that are more powerful than GPT4 is “the answer,” as Elon Musk and Apple co-founder Steve Wozniak have said.
   
      “Rather than slow down the pace of technology, which I think is extraordinarily difficult, I don’t think China’s going to jump on that bandwagon,” Smith said. “Let’s use six months to go faster.”
   
      Smith suggested an executive order where the government itself says it will only buy AI services from companies that are implementing AI safety protocols.
   
      “The world is moving forward,” Smith said. “Let’s make sure that the United States at least keeps pace with the rest of the world.”
   Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2023 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network.",41,904,0.384926438331604,0.324235588312149,0.290837943553925
"Markets  Fear & Greed Index  
            Latest Market News  
  Editor’s Note: A version of this article first appeared in the “Reliable Sources” newsletter. Sign up for the daily digest chronicling the evolving media landscape here.
 
      Human extinction.
   
      Think about that for a second. Really think about it. The erasure of the human race from planet Earth.
   
      That is what top industry leaders are frantically sounding the alarm about. These technologists and academics keep smashing the red panic button, doing everything they can to warn about the potential dangers artificial intelligence poses to the very existence of civilization.
   
      On Tuesday, hundreds of top AI scientists, researchers, and others — including OpenAI chief executive Sam Altman and Google DeepMind chief executive Demis Hassabis — again voiced deep concern for the future of humanity, signing a one-sentence open letter to the public that aimed to put the risks the rapidly advancing technology carries with it in unmistakable terms.
   
      “Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war,” said the letter, signed by many of the industry’s most respected figures.
   
      It doesn’t get more straightforward and urgent than that. These industry leaders are quite literally warning that the impending AI revolution should be taken as seriously as the threat of nuclear war. They are pleading for policymakers to erect some guardrails and establish baseline regulations to defang the primitive technology before it is too late.
   
      
      AI industry and researchers sign statement warning of 'extinction' risk
     
      Dan Hendrycks, the executive director of the Center for AI Safety, called the situation “reminiscent of atomic scientists issuing warnings about the very technologies they’ve created. As Robert Oppenheimer noted, ‘We knew the world would not be the same.’”
   
      “There are many ‘important and urgent risks from AI,’ not just the risk of extinction; for example, systemic bias, misinformation, malicious use, cyberattacks, and weaponization,” Hendrycks continued. “These are all important risks that need to be addressed.”
   
      And yet, it seems that the dire message these experts are desperately trying to send the public isn’t cutting through the noise of everyday life. AI experts might be sounding the alarm, but the level of trepidation — and in some cases sheer terror — they harbor about the technology is not being echoed with similar urgency by the news media to the masses.
   
      Instead, broadly speaking, news organizations treated Tuesday’s letter — like all of the other warnings we have seen in recent months — as just another headline, mixed in with a garden variety of stories. Some major news organizations didn’t even feature an article about the chilling warning on their website’s homepages.
   
      To some extent, it feels eerily reminiscent of the early days of the pandemic, before the widespread panic and the shutdowns and the overloaded emergency rooms. Newsrooms kept an eye on the rising threat that the virus posed, publishing stories about it slowly spreading across the world. But by the time the serious nature of the virus was fully recognized and fused into the very essence in which it was covered, it had already effectively upended the world.
   
      History risks repeating itself with AI, with even higher stakes. Yes, news organizations are covering the developing technology. But there has been a considerable lack of urgency surrounding the issue given the open possibility of planetary peril.
   
      
      Microsoft leaps into the AI regulation debate, calling for a new US agency and executive order
     
      Perhaps that is because it can be difficult to come to terms with the notion that a Hollywood-style science fiction apocalypse can become reality, that advancing computer technology might reach escape velocity and decimate humans from existence. It is, however, precisely what the world’s most leading experts are warning could happen.
   
      It is much easier to avoid uncomfortable realities, pushing them from the forefront into the background and hoping that issues simply resolve themselves with time. But often they don’t — and it seems unlikely that the growing concerns pertaining to AI will resolve themselves. In fact, it’s far more likely that with the breakneck pace in which the technology is developing, the concerns will actually become more apparent with time.
   
      As Cynthia Rudin, a computer science professor and AI researcher at Duke University, told CNN on Tuesday: “Do we really need more evidence that AI’s negative impact could be as big as nuclear war?”
   Most stock quote data provided by BATS. US market indices are shown in real time, except for the S&P 500 which is refreshed every two minutes. All times are ET. Factset: FactSet Research Systems Inc. All rights reserved. Chicago Mercantile: Certain market data is the property of Chicago Mercantile Exchange Inc. and its licensors. All rights reserved. Dow Jones: The Dow Jones branded indices are proprietary to and are calculated, distributed and marketed by DJI Opco, a subsidiary of S&P Dow Jones Indices LLC and have been licensed for use to S&P Opco, LLC and CNN. Standard & Poor’s and S&P are registered trademarks of Standard & Poor’s Financial Services LLC and Dow Jones is a registered trademark of Dow Jones Trademark Holdings LLC. All content of the Dow Jones branded indices Copyright S&P Dow Jones Indices LLC and/or its affiliates. Fair value provided by IndexArb.com. Market holidays and trading hours provided by Copp Clark Limited. © 2023 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network.",2,1078,0.400866121053696,0.3147913813591,0.284342497587204
"MAS partners with U.S. giant in exploring emerging tech after forays into blockchain SINGAPORE -- Singapore's central bank is partnering with Google's cloud unit on artificial intelligence projects for internal use and to equip staffers with ""deep AI skillsets."" The Monetary Authority of Singapore, the city-state's central bank and financial regulator, announced the collaboration with the U.S. technology giant on Wednesday, adding to its growing list of initiatives exploring emerging tech to bolster the development of cutting-edge innovations in the Asian financial hub. Discover the all new Nikkei Asia app 
        Nikkei Asian Review, now known as Nikkei Asia, will be the voice of the Asian Century.
     
            
                        By continuing to browse this website, you accept cookies which are used for several reasons such as personalizing content/ads and analyzing how this website is used.                        Please review our
                Cookie Policy
                to learn how you can update your cookie settings.
        ",48,254,0.210225656628609,0.255473583936691,0.534300684928894
"May 17 (Reuters) - The swift growth of artificial intelligence technology could put the future of humanity at risk, according to most Americans surveyed in a Reuters/Ipsos poll published on Wednesday. More than two-thirds of Americans are concerned about the negative effects of AI and 61% believe it could threaten civilization. Since OpenAI's ChatGPT chatbot became the fastest growing application of all time, the widespread integration of AI into everyday life has catapulted AI to the forefront of public discourse. ChatGPT has kicked off an AI arms race, with tech heavyweights like Microsoft (MSFT.O) and Google (GOOGL.O) vying to outdo each other's AI accomplishments. Lawmakers and AI companies are also concerned: OpenAI CEO Sam Altman on Tuesday testified before U.S. Congress, voicing concerns about potential misuse of the technology and asking for regulation. ""There's no way to put this genie in the bottle. Globally, this is exploding,"" said Senator Cory Booker, one of many lawmakers with questions about how best to regulate AI during a Senate panel on the uses of AI Tuesday. The Reuters/Ipsos poll found that the number of Americans who foresee adverse outcomes from AI is triple the number of those who don't. According to the data, 61% of respondents believe that AI poses risks to humanity, while only 22% disagreed, and 17% remained unsure. Those who voted for Donald Trump in 2020 expressed higher levels of concern; 70% of Trump voters compared to 60% of Joe Biden voters agreed that AI could threaten humankind. When it came to religious beliefs, Evangelical Christians were more likely to ""strongly agree"" that AI presents risks to humanity, standing at 32% compared to 24% of non-Evangelical Christians. ""It's telling such a broad swath of Americans worry about the negative effects of AI,"" said Landon Klein, director of U.S. policy of the Future of Life Institute, the organization behind an open letter, co-signed by Tesla (TSLA.O) CEO Elon Musk, demanding a six-month pause in AI research. ""We view the current moment similar to the beginning of the nuclear era, and we have the benefit of public perception that is consistent with the need to take action."" While Americans are concerned about AI, crime and the economy rank higher in the list of kitchen table issues: 77% support increasing police funding to fight crime and 82% are worried about the risk of a recession. Those in the industry said the public should understand AI's benefits more. “The concerns are very legitimate, but I think what’s missing in the dialogue in general is why are we doing this in the first place?” said Sebastian Thrun, a computer science professor at Stanford who founded Google X. “AI will raise peoples’ quality of life, and help people be more competent and more efficient.” The positive applications of AI, such as revolutionizing drug discovery, are not as visible as ChatGPT, said Ion Stoica, a UC Berkeley professor who also co-founded AI company Anyscale. ""Americans may not realize how pervasive AI already is in their daily lives, both at home and at work,"" he said. The online poll of 4,415 U.S. adults was conducted between May 9 and May 15. It has a credibility interval, a measure of accuracy, of plus or minus 2 percentage points. (This story has been refiled to correct a typographical error in paragraph 10) Our Standards: The Thomson Reuters Trust Principles. Thomson Reuters Anna Tong is a correspondent for Reuters based in San Francisco, where she reports on the technology industry. She joined Reuters in 2023 after working at the San Francisco Standard as a data editor. Tong previously worked at technology startups as a product manager and at Google where she worked in user insights and helped run a call center. Tong graduated from Harvard University.
Contact:4152373211 Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",88,818,0.337425738573074,0.328721463680267,0.333852887153625
"May 31 (Reuters) - Specialized cloud computing provider CoreWeave has raised $200 million in funding from its existing investor Magnetar Capital, highlighting investor interest in backing infrastructure powering the generative AI boom. The funding, which valued the company at more than $2 billion, comes weeks after CoreWeave raised $221 million from investors including Magnetar Capital and Nvidia (NVDA.O). CoreWeave specializes in providing cloud computing services based on graphics processing units (GPUs), the category of chip pioneered by Nvidia that has become central to artificial intelligence (AI) services like OpenAI's ChatGPT. The capital will be used to fund the company's expansion to build out six data centers across the U.S. and for potential acquisition opportunities, Brannin McBee, CoreWeave co-founder and chief strategy officer said in an interview. CoreWeave is in talks with lenders to raise debt for GPU purchases, he added, without giving details. CoreWeave previously focused on serving the cryptocurrency mining market but has seen sharp growth since moving toward AI, where several startups have raised more than $12 billion in a gold rush kicked off by ChatGPT in the first quarter of 2023. CoreWeave sells computing power to those AI companies, competing with cloud computing service providers such as Microsoft Azure and Amazon's AWS. (MSFT.O), (AMZN.O) CoreWeave aims to stand out by building its data centers differently for AI work, using a networking technology called InfiniBand to link computers together instead of Ethernet cables that are the current standard in most data centers, McBee said. InfiniBand, popular in supercomputers, is known to generate high performance with low latency. Mcbee also pointed out that his firm had a close partnership with Nvidia. Having access to chips supply is key in a market where Nvidia has strained to meet demand for its AI chips. CoreWeave has put Nvidia's latest H100 chips into use before many of its larger rivals. ""We're the only provider of that infrastructure at scale and an InfiniBand environment,"" McBee said. ""Nvidia wants their products to be as good as possible and to be accessible. We hit both those check boxes for them the way that no one else does."" Our Standards: The Thomson Reuters Trust Principles. Thomson Reuters Krystal reports on venture capital and startups for Reuters. She covers Silicon Valley and beyond through the lens of money and characters, with a focus on growth-stage startups, tech investments and AI. She has previously covered M&A for Reuters, breaking stories on Trump's SPAC and Elon Musk's Twitter financing. Previously, she reported on Amazon for Yahoo Finance, and her investigation of the company's retail practice was cited by lawmakers in Congress. Krystal started a career in journalism by writing about tech and politics in China. She has a master's degree from New York University, and enjoys a scoop of Matcha ice cream as much as getting a scoop at work.  Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",20,665,0.333614140748978,0.298774987459183,0.36761087179184
"Microsoft has been relentless with its efforts in the artificial intelligence (AI) space, especially with the growing buzz around generative AI and all of the incredible potential this technology entails. The company’s latest venture in this arena is Jugalbandi, a generative AI-driven platform and chatbot. The platform was developed to help Indians have easier access to information about government initiatives and public programs. This is especially important in India, where there are nearly 22 official languages with significant variations locally and regionally— often creating significant challenges in the dissemination of information. Of note, the name “Jugalbandi” is an ode to Indian classical music; the term typically refers to a duet of two musicians who exchange musical compositions in friendly competition. In this case, jugalbandi denotes the productive back-and-forth between the user and the AI system. The platform’s mission is simple: “Jugalbandi is a free and open platform that combines the power of ChatGPT and Indian language translation models to power conversational AI solutions in any domain.” Specifically, its goal is to help users overcome barriers in literacy and language and empower citizens in accessing necessary information across a variety of sectors, ranging from justice, to education and health. Notably, the developers have done an incredible job of innovating in a way that makes the most sense for the Indian population. For example, the chatbot has been directly integrated with WhatsApp, which is the most popular form of mobile communication in India and connects more than 480 million people across the country. In fact, WhatsApp is so well integrated into Indian culture that it is also often used to make payments, conduct business, and is a primary source of social and news media. Furthermore, Jugalbandi is powered by language and learning models from AI4Bharat, a government-backed initiative which supports “an inter-disciplinary team of researchers, language experts, and software developers,” and is focused on building “open-source language AI [systems] for Indian languages, including datasets, models, and applications.” The program is mainly housed at the Indian Institute of Technology (IIT) in Madras, one of the world’s most premier engineering and technical universities. In an article for Microsoft, Chen May Yee explains how the chatbot operates: a user “sends a text or audio message to a WhatsApp number, which initiates the Jugalbandi bot. That is transcribed to text using the AI4Bharat speech recognition model. That in turn is translated to English by the Bhashini translation model trained by AI4Bharat. Based on the prompt, Azure OpenAI Service’s model retrieves information on the relevant government [program]. The answer is translated to Hindi. That is then synthesized with the AI4Bharat text-to-speech model and sent back to WhatsApp – and the villager’s ear.” Abhigyan Raman, a Project Officer for AI4Bharat, explains further: “We saw this Jugalbandi as a kind of ‘chatbot plus plus’ because it’s like a personalized agent […] It understands your exact problem in your language and then tries to deliver the right information reliably and cheaply, even if that exists in some other language in a database somewhere.” The article describes the example of Vandna, an 18 year old college freshman who used the application to find scholarships. When she prompted the system with the query “What kind of scholarships are available for me?” and inputted her courses of study (Political Science, History and Hindi), the system provided her with a list of central and state government programs she was eligible for, along with specific eligibility criteria and the required documents. Microsoft CEO Satya Nadella speaks during a keynote address announcing ChatGPT integration for Bing ... [+] at Microsoft in Redmond, Washington, on February 7, 2023. - Microsoft's long-struggling Bing search engine will integrate the powerful capabilities of language-based artificial intelligence, CEO Satya Nadella said, declaring what he called a new era for online search. (Photo by Jason Redmond / AFP) (Photo by JASON REDMOND/AFP via Getty Images) Overall, this initiative has the power and potential to be an incredibly impactful and positive force in India. For one, the country is home to more than 1.4 billion people, many of whom dwell outside major city centers. Fortunately, India has invested billions of dollars to develop the technology and infrastructure needed to bring high-speed internet access to rural areas. This means that Jugalbandi can be used to serve and help even the most rural communities. Moreover, the potential applications for the platform itself are endless. Though the use-case is currently for government programs, the application may also one day be used to help Indians connect on a variety of other topic areas and sectors, including law, healthcare, banking, public health, and other pertinent social initiatives. Indeed, applications like this may potentially be the key to harness and disseminate information to the masses in a safe and scalable manner—all while helping millions of people feel connected and stay informed. ",18,802,0.332999020814896,0.301391512155533,0.365609467029572
"Most Widely Read Newspaper Kindly share this story: In an era of rapid technological advancement, Artificial Intelligence is emerging as a game-changer for the digital product landscape, a technology and design expert, Abubakar Najeeb, has said that business digital products can be elevated through the use of Artificial technology.
 Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems.
 Najeeb, on Tuesday, in Abuja, added that AI is poised to reshape the landscape of user experience in the digital realm.
 He noted that the possibilities are immense and organisations that embrace AI integrations are well-positioned to unlock new dimensions of user satisfaction, customer acquisition and loyalty in this fast-evolving digital age. “With AI, we have the opportunity to create more intuitive, personalised, and efficient experiences for users. By harnessing the power of AI technology, businesses can truly elevate their digital products and foster stronger connections with their customers. Having a seamless user experience increases customer acquisition and I believe AI integrations will help to achieve that,” he said.  He added that by leveraging AI’s capabilities, opportunities to enhance user experiences and drive customer satisfaction which will lead to product growth and customer acquisition in the ecosystem can be unlocked by startups and businesses as a way.
 On how AI technology is set to revolutionise user experience for digital products, the digital product expert disclosed “AI brings a whole new level of intelligence and personalisation to digital products. By leveraging machine learning algorithms, natural language processing, and data analytics, AI-powered systems can understand user preferences, predict their needs, and offer tailored experiences which will increase the number of active users for digital platforms because of the seamless ease it brings.” While acknowledging the importance of responsible AI implementation especially for new startups, Najeeb urged organisations to be transparent on how AI is being used while noting that user data privacy needs to be protected.
 “As we embrace AI’s potential, it’s crucial to ensure ethical use and transparency. User trust is paramount, and organisations must be transparent about how AI is being used, protect user data privacy, and address any biases or ethical concerns that may arise.”
 “The reason why I am pointing this out is that when we imagine the level of advancement of AI in the next couple of years, then I am sure it will be difficult to differentiate the things done by AI and a real person,” he said.
                                             Kindly share this story:  All rights reserved. This material, and other digital content on this website, may not be reproduced, published, broadcast, rewritten or redistributed in whole or in part without prior express written permission from PUNCH. Contact:  [email protected]   
                            punchng.com © 1971- 2023 Punch Nigeria Limited",56,526,0.353537023067474,0.316068142652512,0.330394893884659
"OAKLAND, California, May 31 (Reuters) - Lightmatter, a Boston-based startup using light for AI computing, said on Wednesday it raised $154 million and tripled its valuation as customers in need of faster, more energy efficient computing for AI work are buying its systems. ""We use light to link computer chips together and we also use light to do calculations for deep learning,"" Lightmatter co-founder and CEO Nick Harris said. ""The reason that we're getting these customers and data center scale deployments with our interconnect is that the generative AI boom is driving high-end chips like crazy."" Light has been used for decades in the telecommunications industry to move data across the world with fiber optic cables. Now, large tech companies are looking to use light-based technology in their data centers to reduce power costs. The challenge has been to shrink the size of the devices used to create or control light. As chips become more advanced, they consume more power. ""It can be up to half of the entire operating cost of a data center just getting the power to the chips,"" Harris said, adding that light transmits information far more energy efficiently than electrical signals traveling over wires. Harris said its chip called Passage that connects other chips by translating electrons to photons and back will be mass produced and used in data centers next year. A large part of the funding will help roll out this product. Its more futuristic product Envise is a system that uses light, instead of electrons, to do computing. Harris said Envise will be able to run large AI models but will not train them. Customers will test the system this year, he said. GV (Google Ventures), the venture capital investment arm of Alphabet Inc (GOOGL.O), and Fidelity Management & Research Company were among the investors taking part in this latest round, the company said. Our Standards: The Thomson Reuters Trust Principles. Thomson Reuters Reports on global trends in computing from covering semiconductors and tools to manufacture them to quantum computing. Has 27 years of experience reporting from South Korea, China, and the U.S. and previously worked at the Asian Wall Street Journal, Dow Jones Newswires and Reuters TV. In her free time, she studies math and physics with the goal of grasping quantum physics.  Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",30,576,0.346184819936752,0.31377112865448,0.340044051408768
"One of the leading thinkers on artificial intelligence discusses responsibility, ‘moral outsourcing’ and bridging the gap between people and technology Rumman Chowdhury often has trouble sleeping, but, to her, this is not a problem that requires solving. She has what she calls “2am brain”, a different sort of brain from her day-to-day brain, and the one she relies on for especially urgent or difficult problems. Ideas, even small-scale ones, require care and attention, she says, along with a kind of alchemic intuition. “It’s just like baking,” she says. “You can’t force it, you can’t turn the temperature up, you can’t make it go faster. It will take however long it takes. And when it’s done baking, it will present itself.” It was Chowdhury’s 2am brain that first coined the phrase “moral outsourcing” for a concept that now, as one of the leading thinkers on artificial intelligence, has become a key point in how she considers accountability and governance when it comes to the potentially revolutionary impact of AI. Moral outsourcing, she says, applies the logic of sentience and choice to AI, allowing technologists to effectively reallocate responsibility for the products they build onto the products themselves – technical advancement becomes predestined growth, and bias becomes intractable. “You would never say ‘my racist toaster’ or ‘my sexist laptop’,” she said in a Ted Talk from 2018. “And yet we use these modifiers in our language about artificial intelligence. And in doing so we’re not taking responsibility for the products that we build.” Writing ourselves out of the equation produces systematic ambivalence on par with what the philosopher Hannah Arendt called the “banality of evil” – the wilful and cooperative ignorance that enabled the Holocaust. “It wasn’t just about electing someone into power that had the intent of killing so many people,” she says. “But it’s that entire nations of people also took jobs and positions and did these horrible things.” Chowdhury does not really have one title, she has dozens, among them Responsible AI fellow at Harvard, AI global policy consultant and former head of Twitter’s Meta team (Machine Learning Ethics, Transparency and Accountability). AI has been giving her 2am brain for some time. Back in 2018 Forbes named her one of the five people “building our AI future”. A data scientist by trade, she has always worked in a slightly undefinable, messy realm, traversing the realms of social science, law, philosophy and technology, as she consults with companies and lawmakers in shaping policy and best practices. Around AI, her approach to regulation is unique in its staunch middle-ness – both welcoming of progress and firm in the assertion that “mechanisms of accountability” should exist. Effervescent, patient and soft-spoken, Chowdhury listens with disarming care. She has always found people much more interesting than what they build or do. Before skepticism around tech became reflexive, Chowdhury had fears too – not of the technology itself, but of the corporations that developed and sold it. As the global lead at the responsible AI firm Accenture, she led the team that designed a fairness evaluation tool that pre-empted and corrected algorithmic bias. She went on to start Parity, an ethical AI consulting platform that seeks to bridge “different communities of expertise”. At Twitter – before it became one of the first teams disbanded under Elon Musk – she hosted the company’s first-ever algorithmic bias bounty, inviting outside programmers and data scientists to evaluate the site’s code for potential biases. The exercise revealed a number of problems, including that the site’s photo-cropping software seemed to overwhelmingly prefer faces that were young, feminine and white. This is a strategy known as red-teaming, in which programmers and hackers from outside an organization are encouraged to try and curtail certain safeguards to push a technology to “do bad things to identify what bad things it’s capable of”, says Chowdhury. These kinds of external checks and balances are rarely implemented in the world of tech because of technologists’ fear of “people touching their baby”. She is currently working on another red-teaming event for Def Con – a convention hosted by the hacker organization AI Village. This time, hundreds of hackers are gathering to test ChatGPT, with the collaboration of its founder OpenAI, along with Microsoft, Google and the Biden administration. The “hackathon” is scheduled to run for over 20 hours, providing them with a dataset that is “totally unprecedented”, says Chowdhury, who is organizing the event with Sven Cattell, founder of AI Village and Austin Carson, president of the responsible AI non-profit SeedAI. In Chowdhury’s view, it’s only through this kind of collectivism that proper regulation – and regulation enforcement – can occur. In addition to third-party auditing, she also serves on multiple boards across Europe and the US helping to shape AI policy. She is wary, she tells me, of the instinct to over-regulate, which could lead models to overcorrect and not address ingrained issues. When asked about gay marriage, for example, ChatGPT and other generative AI tools “totally clam up”, trying to make up for the amount of people who have pushed the models to say negative things. But it’s not easy, she adds, to define what is toxic and what is hateful. “It’s a journey that will never end,” she tells me, smiling. “But I’m fine with that.” Early on, when she first started working in tech, she realized that “technologists don’t always understand people, and people don’t always understand technology”, and sought to bridge that gap. In its broadest interpretation, she tells me, her work deals with understanding humans through data. “At the core of technology is this idea that, like, humanity is flawed and that technology can save us,” she says, noting language like “body hacks” that implies a kind of optimization unique to this particular age of technology. There is an aspect of it that kind of wishes we were “divorced from humanity”. Chowdhury has always been drawn to humans, their messiness and cloudiness and unpredictability. As an undergrad at MIT, she studied political science, and, later, after a disillusioning few months in non-profits in which she “knew we could use models and data more effectively, but nobody was”, she went to Columbia for a master’s degree in quantitative methods. Sign up to Business Today Get set for the working day – we'll point you to all the business news and analysis you need every morning after newsletter promotion In the last month, she has spent a week in Spain helping to carry out the launch of the Digital Services Act, another in San Francisco for a cybersecurity conference, another in Boston for her fellowship, and a few days in New York for another round of Def Con press. After a brief while in Houston, where she’s based, she has upcoming talks in Vienna and Pittsburgh on AI nuclear misinformation and Duolingo, respectively. At its core, what she prescribes is a relatively simple dictum: listen, communicate, collaborate. And yet, even as Sam Altman, the founder and CEO of OpenAI, testifies before Congress that he’s committed to preventing AI harms, she still sees familiar tactics at play. When an industry experiences heightened scrutiny, barring off prohibitive regulation often means taking control of a narrative – ie calling for regulation, while simultaneously spending millions in lobbying to prevent the passing of regulatory laws. The problem, she says, is a lack of accountability. Internal risk analysis is often distorted within a company because risk management doesn’t often employ morals. “There is simply risk and then your willingness to take that risk,” she tells me. When the risk of failure or reputational harm becomes too great, it moves to an arena where the rules are bent in a particular direction. In other words: “Let’s play a game where I can win because I have all of the money.” But people, unlike machines, have indefinite priorities and motivations. “There are very few fundamentally good or bad actors in the world,” she says. “People just operate on incentive structures.” Which in turn means that the only way to drive change is to make use of those structures, ebbing them away from any one power source. Certain issues can only be tackled at scale, with cooperation and compromise from many different vectors of power, and AI is one of them. Though, she readily attests that there are limits. Points where compromise is not an option. The rise of surveillance capitalism, she says, is hugely concerning to her. It is a use of technology that, at its core, is unequivocally racist and therefore should not be entertained. “We cannot put lipstick on a pig,” she said at a recent talk on the future of AI at New York University’s School of Social Sciences. “I do not think ethical surveillance can exist.” Chowdhury recently wrote an op-ed for Wired in which she detailed her vision for a global governance board. Whether it be surveillance capitalism or job disruption or nuclear misinformation, only an external board of people can be trusted to govern the technology – one made up of people like her, not tied to any one institution, and one that is globally representative. On Twitter, a few users called her framework idealistic, referring to it as “blue sky thinking” or “not viable”. It’s funny, she tells me, given that these people are “literally trying to build sentient machines”. She’s familiar with the dissonance. “It makes sense,” she says. We’re drawn to hero narratives, the assumption that one person is and should be in charge at any given time. Even as she organizes the Def Con event, she tells me, people find it difficult to understand that there is a team of people working together every step of the way. “We’re getting all this media attention,” she says, “and everybody is kind of like, ‘Who’s in charge?’ And then we all kind of look at each other and we’re like, ‘Um. Everyone?’”",57,1650,0.394732415676117,0.29050812125206,0.314759433269501
"One of the so-called ""godfathers"" of Artificial Intelligence (AI) has said he would have prioritised safety over usefulness had he realised the pace at which it would evolve. Prof Yoshua Bengio told the BBC he felt ""lost"" over his life's work. The computer scientist's comments come after experts in AI said it could lead to the extinction of humanity. Prof Bengio, who has joined calls for AI regulation, said he did not think militaries should be granted AI powers. He is the second of the so-called three ""godfathers"" of AI, known for their pioneering work in the field, to voice concerns about the direction and the speed at which it is developing.  AI describes the ability of computers to perform tasks so complex, they have previously required human intelligence to complete. A recent example has been the development of AI-powered chatbots, like ChatGPT, which appear to give human-like responses to questions. But some fear that advanced computational ability could be used for harmful purposes, such as the development of deadly new chemical weapons. Prof Bengio told the BBC he was concerned about ""bad actors"" getting hold of AI, especially as it became more sophisticated and powerful. ""It might be military, it might be terrorists, it might be somebody very angry, psychotic. And so if it's easy to program these AI systems to ask them to do something very bad, this could be very dangerous. ""If they're smarter than us, then it's hard for us to stop these systems or to prevent damage,"" he added. Prof Bengio admitted those concerns were taking a personal toll on him, as his life's work, which had given him direction and a sense of identity, was no longer clear to him. ""It is challenging, emotionally speaking, for people who are inside [the AI sector],"" he said. ""You could say I feel lost. But you have to keep going and you have to engage, discuss, encourage others to think with you.""  The Canadian has signed two recent statements urging caution about the future risks of AI. Some academics and industry experts have warned that the pace of development could result in malicious AI being deployed by ""bad actors"" to actively cause harm - or choosing to inflict harm by itself. Fellow ""godfather"" Dr Geoffrey Hinton has also signed the same warnings as Prof Bengio, and retired from Google recently saying he regretted his work.  The third ""godfather"", Prof Yann LeCun, who along with Prof Bengio and Dr Hinton won a prestigious Turing Award for their pioneering work, has said apocalyptic warnings are overblown. Twitter and Tesla owner Elon Musk has also voiced his concerns. ""I don't think AI will try to destroy humanity, but it might put us under strict controls,"" he said recently at an event hosted by the Wall Street Journal.  ""There's a small likelihood of it annihilating humanity. Close to zero but not impossible."" Prof Bengio told the BBC all companies building powerful AI products needed to be registered. ""Governments need to track what they're doing, they need to be able to audit them, and that's just the minimum thing we do for any other sector like building aeroplanes or cars or pharmaceuticals,"" he said. ""We also need the people who are close to these systems to have a kind of certification... we need ethical training here. Computer scientists don't usually get that, by the way.""  But not everybody in the field believes AI will be the downfall of humans - others argue that there are more imminent problems which need addressing. Dr Sasha Luccioni, research scientist at the AI firm Huggingface, said society should focus on issues like AI bias, predictive policing, and the spread of misinformation by chatbots which she said were ""very concrete harms"".  ""We should focus on that rather than the hypothetical risk that AI will destroy humanity,"" she added. There are already many examples of AI bringing benefits to society. Last week an AI tool discovered a new antibiotic, and a paralysed man was able to walk again just by thinking about it, thanks to a microchip developed using AI. But this is juxtaposed with fears about the far-reaching impact of AI on countries' economies. Firms are already replacing human staff with AI tools, and it is a factor in the current strike under way by scriptwriters in Hollywood. ""It's never too late to improve,"" says Prof Bengio of AI's current state. ""It's exactly like climate change.  ""We've put a lot of carbon in the atmosphere. And it would be better if we hadn't, but let's see what we can do now."" Follow Zoe Kleinman on Twitter @zsk What are your questions about artificial intelligence? Is artificial intelligence a good idea? AI could lead to extinction, experts warn AI chatbots 'may soon be more intelligent than us' Advanced AI risk to humanity - technology leaders AI could affect 300 million jobs - report Ukraine mothers go behind enemy lines to save stolen children Why South Korea woke up to sirens... in 73 seconds. VideoWhy South Korea woke up to sirens... in 73 seconds AI 'godfather' feels 'lost' over life's work Travelling into enemy land to save my stolen child A top soldier's defamation case rocks Australia New Yorkers gather for Manhattanhenge The Myanmar soldiers refusing to fight Could a US debt default unleash global chaos? Going to the city of my mother's dreams After a synagogue shooting, can a community heal? The 'exploding' demand for giant heat pumps Holmes has gone to jail. Will she pay victims too? Why it's 'imperative' to start using AI Jellyfish blooms: Why not just eat them? A 5,000-year-old craft under threat © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.",9,967,0.374799787998199,0.328964918851852,0.296235263347626
"People are already tired of machine-generated text, and they’re not afraid to say it. If you want to really hurt someone’s feelings in the year 2023, just call them an AI.  An all-star cast of celebrities and public figures have recently been the victim of such jokes: the NBA player Jordan Poole (“AI Steph Curry”), Raquel Leviss from the reality-TV show Vanderpump Rules (“what would happen if you asked chat GBT [sic] to create an American girl”), Transportation Secretary Pete Buttigieg (“our first A.I. cabinet member?”). That these slights span the three pillars of American life—sports, politics, Bravo—suggests that no one, or rather nothing, is safe. Such digs have popped up all over social media; on Twitter alone, insults like these have been levied against TV shows, songs, sports uniforms, commencement speeches, White House press releases, proposed legislation, and lots of news articles.  That AI has become an attack is a result of the huge moment for AI we’re in. Anyone can ask ChatGPT to write a silly poem or a college paper or a wedding toast—and that the chatbot can actually mimic human language with impressive precision is exactly why bots have taken off. But compared with human-generated work, much of what a chatbot spits out is dull or uninspired, riddled with clich​​és and recycled ideas. At a time when AI is capable of more than ever, Did a chatbot write this? is not a compliment. It’s a diss.  “Are we sure that ChatGPT didn’t write season three of Ted Lasso?” Bloomberg’s Ashlee Vance tweeted. The Guardian, reviewing the action film Ghosted, called it “so carelessly and lifelessly cobbled together that we’re inclined to believe it’s the first film created entirely by AI.” My colleague Spencer Kornhaber, writing about Ed Sheeran’s new album, surmised that its dull lyrics could’ve been composed by “a neural network trained on Sheeran’s past work.”  I called up Frank Lantz, the director of New York University’s Game Center and a writer who has focused on making sense of AI, to discuss this style of joke. “Well, first of all, it is a funny insult,” he said. Lantz positioned the dig as part of the larger arc of AI in pop culture. Even before ChatGPT arrived, robots were usually portrayed as both smart and dumb—sentient calculators that struggle to understand human emotion. He made beeping noises and put on a robot voice: “It does not compute.”  Today we’re teasing ChatGPT and other bots for a different reason: because they sound like they’re posting on LinkedIn all the time, which is a different kind of soullessness. What has changed is that bots no longer primarily live on servers in some distant researcher’s lab; they’re right in front of us, just one browser window away. “For a long time, people may have had concerns about [AI technology], but there wasn’t a popular narrative that it was janky or broken or a thin version of the original,” Meredith Whittaker, the president of the messaging app Signal and the chief adviser to the think tank AI Now Institute, told me. “And I think that’s in large part because there wasn’t a direct interface that most people in the public could play with and actually experience.”  Part of why AI is an insult is also that we’re in the middle of an AI hype cycle, where every company is trying to stuff anything and everything into a chatbot. These jokes serve to ground us in the technology’s present-day abilities: A bot can write a college paper, but can it write a good college paper? Janelle Shane, who runs the blog AI Weirdness, which tracks machine-learning quirks and gaffes, sees humor as a leveling force. “In some ways, this being a meme is a hopeful sign,” she told me. Shane thinks it is somewhat useful to “bring that discourse back to the level of ‘No, no. These things are incompetent.’”  Read: Welcome to the golden age of clichés  But there’s a darker spin. Humor, of course, is a coping mechanism; jokes about AI are on some level an expression of the anxiety around these tools. Bots are already replacing some jobs, and surely will replace more: Just last week, the National Eating Disorders Association announced that it was firing the humans who run its hotline and using a chatbot. The labor issues surrounding AI are also a big tension in the ongoing Hollywood writers’ strike, even as the public jokes about lackluster scripts being the work of AI. One picketer held up a poster that said A.I. THIS SIGN WROTE. Even a bad version of AI can take jobs, Whittaker argued, “not because it’s competent, but because it will allow companies to justify degrading their position, paying them less, offering fewer benefits, turning them into contractors—all of this.”  Over time, perhaps AI will get the last laugh. AI Steph Curry will cease to be a punch line if AI work becomes indistinguishable from the work of real humans. And the insult is becoming familiar enough that it verges on botlike itself. Last week, Fletcher Peters, an entertainment reporter for The Daily Beast, tweeted that writers need to find a better criticism—because “your review sounds like it was written by chatGPT.”",12,878,0.384438186883926,0.310650408267975,0.304911494255066
"Plus: Catching bad content in the age of AI. This story originally appeared in The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first, sign up here. Everyone is talking about AI, it seems. But if you feel overwhelmed or uncertain about what the hell people are talking about, don’t worry. I’ve got you. I asked some of the best AI journalists in the business to share their top tips on how to talk about AI with confidence. My colleagues and I spend our days obsessing over the tech, listening to AI folks and then translating what they say into clear, relatable language with important context. I’d say we know a thing or two about what we’re talking about. Here are seven things to pay attention to when talking about AI.  1. Don’t worry about sounding dumb “The tech industry is not great at explaining itself clearly, despite insisting that large language models will change the world. If you’re struggling, you aren’t alone,” says Nitasha Tiku, the Washington Post’s tech culture reporter. It doesn’t help that conversations about AI are littered with jargon, she adds. “Hallucination” is a fancy way of saying an AI system makes things up. And “prompt engineers” are just people who know how to talk to the AI to get what they want. Tiku recommends watching YouTube explainers on concepts and AI models. “Skip the AI influencers for the more subdued hosts, like Computerphile,” she says. “IBM Technology is great if you’re looking for something short and simple. There’s no channel aimed at casual observers, but it can help demystify the process.”  And however you talk about AI, some people will grumble. “It sometimes feels like the world of AI has splintered into fandoms with everyone talking past each other, clinging to pet definitions and beliefs,” says Will Douglas Heaven, MIT Technology Review’s senior editor for AI. “Figure out what AI means to you, and stick to it.” 2. Be specific about what kind of AI you’re talking about “‘AI’” is often treated as one thing in public discourse, but AI is really a collection of a hundred different things,” says Karen Hao, the Wall Street Journal’s China tech and society reporter (and the creator of The Algorithm!). Hao says that it’s helpful to distinguish which function of AI you are talking about so you can have a more nuanced conversation: are you talking about natural-language processing and language models, or computer vision? Or different applications, such as chatbots or cancer detection? If you aren’t sure, here are some good definitions of various practical applications of artificial intelligence.  Talking about ""AI"" as a singular thing obscures the reality of the tech, says Billy Perrigo, a staff reporter at Time.  “There are different models that can do different things, that will respond differently to the same prompts, and that each have their own biases, too,” he says.  3. Keep it real “The two most important questions for new AI products and tools are simply: What does it do and how does it do it?” says James Vincent, senior editor at The Verge.  There is a trend in the AI community right now to talk about the long-term risks and potential of AI. It’s easy to be distracted by hypothetical scenarios and imagine what the technology could possibly do in the future, but discussions about AI are usually better served by being pragmatic and focusing on the actual, not the what-ifs, Vincent adds.  The tech sector also has a tendency to overstate the capabilities of their products. “Be skeptical; be cynical,” says Douglas Heaven. This is especially important when talking about AGI, or artificial general intelligence, which is typically used to mean software that is as smart as a person. (Whatever that means in itself.) “If something sounds like bad science fiction, maybe it is,” he adds.  4. Adjust your expectations Language models that power AI chatbots such as ChatGPT often “hallucinate,” or make things up. This can be annoying and surprising to people, but it’s an inherent part of how they work, says Madhumita Murgia, artificial-intelligence editor at the Financial Times.  It’s important to remember that language models aren’t search engines that are built to find and give the “right” answers, and they don’t have infinite knowledge. They are predictive systems that are generating the most likely words, given your question and everything they’ve been trained on, Murgia adds.  “This doesn’t mean that they can’t write anything original … but we should always expect them to be inaccurate and fabricate facts. If we do that, then the errors matter less because our usage and their applications can be adjusted accordingly,” she says.  5. Don’t anthropomorphize AI chatbots have captured the public’s imagination because they generate text that looks like something a human could have written, and they give users the illusion they are interacting with something other than a computer program. But programs are in fact all they are. It’s very important not to anthropomorphize the technology, or attribute human characteristics to it, says Chloe Xiang, a reporter at Motherboard. “Don’t give it a [gendered] pronoun, [or] say that it can feel, think, believe, et cetera.” Doing this helps feed into the misconception that AI systems are more capable and sentient than they are.  I’ve found it’s really easy to slip up with this, because our language has not caught up with ways to describe what AI systems are doing. When in doubt, I replace “AI” with “computer program.” Suddenly you feel really silly saying a computer program told someone to divorce his wife!  6. It’s all about power While hype and nightmare scenarios may dominate news headlines, when you talk about AI it is crucial to think about the role of power, says Khari Johnson, a senior staff writer at Wired. “Power is key to raw ingredients for making AI, like compute and data; key to questioning ethical use of AI; and key to understanding who can afford to get an advanced degree in computer science and who is in the room during the AI model design process,” Johnson says.  Hao agrees. She says it’s also helpful to keep in mind that AI development is very political and involves massive amounts of money and many factions of researchers with competing interests: “Sometimes the conversation around AI is less about the technology and more about the people.” 7. Please, for the love of God, no robots Don’t picture or describe AI as a scary robot or an all-knowing machine. “Remember that AI is basically computer programming by humans—combining big data sets with lots of compute power and intelligent algorithms,” says Sharon Goldman, a senior writer at VentureBeat. Catching bad content in the age of AI In the last 10 years, Big Tech has become really good at some things: language, prediction, personalization, archiving, text parsing, and data crunching. But it’s still surprisingly bad at catching, labeling, and removing harmful content. One simply needs to recall the spread of conspiracy theories about elections and vaccines in the United States over the past two years to understand the real-world damage this causes. The ease of using generative AI could turbocharge the creation of more harmful online content. People are already using AI language models to create fake news websites.  But could AI help with content moderation? The newest large language models are much better at interpreting text than previous AI systems. In theory, they could be used to boost automated content moderation. Read more from Tate Ryan-Mosley in her weekly newsletter, The Technocrat. Scientists used AI to find a drug that could fight drug-resistant infectionsResearchers at MIT and McMaster University developed an AI algorithm that allowed them to find a new antibiotic to kill a type of bacteria responsible for many drug-resistant infections that are common in hospitals. This is an exciting development that shows how AI can accelerate and support scientific discovery. (MIT News)  Sam Altman warns that OpenAI could quit Europe over AI rulesAt an event in London last week, the CEO said OpenAI could “cease operating” in the EU if it cannot comply with the upcoming AI Act. Altman said his company found much to criticize in how the AI Act was worded, and that there were “technical limits to what’s possible.” This is likely an empty threat. I’ve heard Big Tech say this many times before about one rule or another. Most of the time, the risk of losing out on revenue in the world’s second-largest trading bloc is too big, and they figure something out. The obvious caveat here is that many companies have chosen not to operate, or to have a restrained presence, in China. But that’s also a very different situation. (Time) Predators are already exploiting AI tools to generate child sexual abuse materialThe National Center for Missing and Exploited Children has warned that predators are using generative AI systems to create and share fake child sexual abuse material. With powerful generative models being rolled out with safeguards that are inadequate and easy to hack, it was only a matter of time before we saw cases like this. (Bloomberg) Tech layoffs have ravaged AI ethics teams This is a nice overview of the drastic cuts Meta, Amazon, Alphabet, and Twitter have all made to their teams focused on internet trust and safety as well as AI ethics. Meta, for example, ended a fact-checking project that had taken half a year to build. While companies are racing to roll out powerful AI models in their products, executives like to boast that their tech development is safe and ethical. But it’s clear that Big Tech views teams dedicated to these issues as expensive and expendable. (CNBC)  “I have suddenly switched my views on whether these things are going to be more intelligent than us.” Hinton will be speaking at EmTech Digital on Wednesday. Watch Hinton speak with Will Douglas Heaven, MIT Technology Review’s senior editor for AI, at EmTech Digital. Greater access to the code behind generative models is fueling innovation. But if top companies get spooked, they could close up shop. Discover special offers, top stories,
            upcoming events, and more. Thank you for submitting your email! It looks like something went wrong. 
                We’re having trouble saving your preferences.
                Try refreshing this page and updating them one
                more time. If you continue to get this message,
                reach out to us at
                customer-service@technologyreview.com with a list of newsletters you’d like to receive.  © 2023 MIT Technology Review",43,1816,0.361543923616409,0.327112197875977,0.311343878507614
"SAN FRANCISCO — Recently, researchers asked two versions of OpenAI’s ChatGPT artificial intelligence chatbot where Massachusetts Institute of Technology professor Tomás Lozano-Pérez was born. One bot said Spain and the other said Cuba. Once the system told the bots to debate the answers, the one that said Spain quickly apologized and agreed with the one with the correct answer, Cuba. The finding, in a paper released by a team of MIT researchers last week, is the latest potential breakthrough in helping chatbots to arrive at the correct answer. The researchers proposed using different chatbots to produce multiple answers to the same question and then letting them debate each other until one answer won out. The researchers found using this “society of minds” method made them more factual. “Language models are trained to predict the next word,” said Yilun Du, a researcher at MIT who was previously a research fellow at OpenAI, and one of the paper’s authors. “They are not trained to tell people they don’t know what they’re doing.” The result is bots that act like precocious people-pleasers, making up answers instead of admitting they simply don’t know. The researchers’ creative approach is just the latest attempt to solve for one of the most pressing concerns in the exploding field of AI. Despite the incredible leaps in capabilities that “generative” chatbots like OpenAI’s ChatGPT, Microsoft’s Bing and Google’s Bard have demonstrated in the last six months, they still have a major fatal flaw: they make stuff up all the time. Figuring out how to prevent or fix what the field is calling “hallucinations” has become an obsession among many tech workers, researchers and AI skeptics alike. The issue is mentioned in dozens of academic papers posted to the online database Arxiv and Big Tech CEOs like Google’s Sundar Pichai have addressed it repeatedly. As the tech gets pushed out to millions of people and integrated into critical fields including medicine and law, understanding hallucinations and finding ways to mitigate them has become even more crucial. Most researchers agree the problem is inherent to the “large language models” that power the bots because of the way they’re designed. They predict what the most apt thing to say is based on the huge amounts of data they’ve digested from the internet, but don’t have a way to understand what is factual or not. Still, researchers and companies are throwing themselves at the problem. Some firms are using human trainers to rewrite the bots’ answers and feed them back into the machine with the goal of making them smarter. Google and Microsoft have started using their bots to give answers directly in their search engines, but still double check the bots with regular search results. And academics around the world have suggested myriad clever ways to decrease the rates of false answers, like MIT’s proposal to get multiple bots to debate each other. The drive to improve the hallucinations problem is urgent for a reason. Already, when Microsoft launched its Bing chatbot, it quickly started making false accusations against some of its users, like telling a German college student that he was a threat to its safety. The bot adopted an alter-ego and started calling itself “Sydney.” It was essentially riffing off the student’s questions, drawing on all the science fiction it had digested from the internet about out-of-control robots. Microsoft eventually had to limit the number of back-and-forths a bot could engage in with a human to prevent it from happening more. In Australia, a government official threatened to sue OpenAI after ChatGPT said he had been convicted of bribery, when in reality he was a whistleblower in a bribery case. And last week a lawyer admitted to using ChatGPT to generate a legal brief after he was caught because the cases cited so confidently by the bot simply did not exist, according to the New York Times. Even Google and Microsoft, which have pinned their futures on AI and are in a race to integrate the tech into their wide range of products, have missed hallucinations their bots made during key announcements and demos. None of that is stopping the companies from rushing headlong into the space. Billions of dollars in investment is going into developing smarter and faster chatbots and companies are beginning to pitch them as replacements or aids for human workers. Earlier this month OpenAI CEO Sam Altman testified at Congress saying AI could “cause significant harm to the world” by spreading disinformation and emotionally manipulating humans. Some companies are already saying they want to replace workers with AI, and the tech also presents serious cybersecurity challenges. On Tuesday, Altman joined hundreds of other AI researchers and executives, including some senior leaders from Google and Microsoft, in signing a statement saying AI poses an existential risk to humanity on par with pandemics and nuclear war. Hallucinations have also been documented in AI-powered transcription services, adding words to recordings that weren’t spoken in real life. Microsoft and Google using the bots to answer search queries directly instead of sending traffic to blogs and news stories could erode the business model of online publishers and content creators who work to produce trustworthy information for the internet. “No one in the field has yet solved the hallucination problems. All models do have this as an issue,” Pichai said in an April interview with CBS. Whether it’s even possible to solve it is a “matter of intense debate,” he said. Depending on how you look at hallucinations, they are both a feature and a bug of large language models. Hallucinations are part of what allows the bots to be creative and generate never-before-seen stories. At the same time they reveal the stark limitations of the tech, undercutting the argument that chatbots are intelligent in a way similar to humans by suggesting that they do not have an internalized understanding of the world around them. “There is nothing in there that tells the model that whatever it’s saying should be actually correct in the world,” said Ece Kamar, a senior researcher at Microsoft. The model itself also trains on a set amount of data, so anything that happens after the training is done doesn’t factor into its knowledge of the world, Kamar said. Hallucinations are not new. They’ve been an inherent problem of large language models since their inception several years ago, but other problems such as the AIs producing nonsensical or repetitive answers were seen as bigger issues. Once those were largely solved, though, hallucinations have now become a key focus for the AI community. I asked ChatGPT to write a short bio of @billmurphyjr (aka, me)Apparently I:* went to @BrownUniversity* & @MITSloan** worked in investment banking* & management consulting** wrote a book about medical breakthroughs*(*not true)When challenged it insisted it was correct. pic.twitter.com/vCFzD9fKhh Potsawee Manakul was playing around with ChatGPT when he asked it for some simple facts about tennis legend Roger Federer. It’s a straightforward request, easy for a human to look up on Google or Wikipedia in seconds, but the bot kept giving contradictory answers. “Sometimes it says he won Wimbledon five times; sometimes it says he won Wimbledon eight times,” Manakul, an AI researcher at the University of Cambridge and ardent tennis fan, said in an interview. (The correct answer is eight.) Manakul and a group of other Cambridge researchers released a paper in March suggesting a system they called “SelfCheckGPT” that would ask the same bot a question multiple times, then tell it to compare the different answers. If the answers were consistent, it was likely the facts were correct, but if they were different, they could be flagged as probably containing made-up information. When humans are asked to write a poem, they know it’s not necessarily important to be factually correct. But when asking them for biographical details about a real person, they automatically know their answer should be rooted in reality. Because chatbots are simply predicting what word or idea comes next in a string of text, they don’t yet have that contextual understanding of the question. “It doesn’t have the concept of whether it should be more creative or if it should be less creative,” Manakul said. Using their method, the researchers showed that they could eliminate factually incorrect answers and even rank answers based on how factual they were. It’s likely a whole new method of AI learning that hasn’t been invented yet will be necessary, Manakul said. Only by building systems on top of the language model can the problem really be mitigated. “Because it blends information from lots of things it will generate something that looks plausible,” he said. “But whether it’s factual or not, that’s the issue.” That’s essentially what the leading companies are already doing. When Google generates search results using its chatbot technology, it also runs a regular search in parallel, then compares whether the bot’s answer and the traditional search results match. If they don’t, the AI answer won’t even show up. The company has tweaked its bot to be less creative, meaning it’s not very good at writing poems or having interesting conversations, but is less likely to lie. By limiting its search-bot to corroborating existing search results, the company has been able to cut down on hallucinations and inaccuracies, said Google spokeswoman Jennifer Rodstrom. A spokesperson for OpenAI pointed to a paper the company had produced where it showed how its latest model, GPT4, produced fewer hallucinations than previous versions. Companies are also spending time and money improving their models by testing them with real people. A technique called reinforcement learning with human feedback, where human testers manually improve a bot’s answers and then feed them back into the system to improve it, is widely credited with making ChatGPT so much better than chatbots that came before it. A popular approach is to connect chatbots up to databases of factual or more trustworthy information, such as Wikipedia, Google search or bespoke collections of academic articles or business documents. Some leading AI researchers say hallucinations should be embraced. After all, humans have bad memories as well and have been shown to fill-in the gaps in their own recollections without realizing it. “We’ll improve on it but we’ll never get rid of it,” Geoffrey Hinton, whose decades of research helped lay the foundation for the current crop of AI chatbots, said of the hallucinations problem. He worked at Google until recently, when he quit to speak more publicly about his concerns that the technology may get out of human control. “We’ll always be like that and they’ll always be like that.”",55,1765,0.330163061618805,0.328826308250427,0.341010689735413
"Scientists have used artificial intelligence (AI) to discover a new antibiotic that can kill a deadly species of superbug. The AI helped narrow down thousands of potential chemicals to a handful that could be tested in the laboratory. The result was a potent, experimental antibiotic called abaucin, which will need further tests before being used. The researchers in Canada and the US say AI has the power to massively accelerate the discovery of new drugs. It is the latest example of how the tools of artificial intelligence can be a revolutionary force in science and medicine.  Antibiotics kill bacteria. However, there has been a lack of new drugs for decades and bacteria are becoming harder to treat, as they evolve resistance to the ones we have.  More than a million people a year are estimated to die from infections that resist treatment with antibiotics.  The researchers focused on one of the most problematic species of bacteria - Acinetobacter baumannii, which can infect wounds and cause pneumonia. You may not have heard of it, but it is one of the three superbugs the World Health Organization has identified as a ""critical"" threat.  It is often able to shrug off multiple antibiotics and is a problem in hospitals and care homes, where it can survive on surfaces and medical equipment. Dr Jonathan Stokes, from McMaster University, describes the bug as ""public enemy number one"" as it's ""really common"" to find cases where it is ""resistant to nearly every antibiotic"". To find a new antibiotic, the researchers first had to train the AI. They took thousands of drugs where the precise chemical structure was known, and manually tested them on Acinetobacter baumannii to see which could slow it down or kill it.  This information was fed into the AI so it could learn the chemical features of drugs that could attack the problematic bacterium.  The AI was then unleashed on a list of 6,680 compounds whose effectiveness was unknown. The results - published in Nature Chemical Biology - showed it took the AI an hour and a half to produce a shortlist.  The researchers tested 240 in the laboratory, and found nine potential antibiotics. One of them was the incredibly potent antibiotic abaucin. Laboratory experiments showed it could treat infected wounds in mice and was able to kill A. baumannii samples from patients.  However, Dr Stokes told me: ""This is when the work starts.""  The next step is to perfect the drug in the laboratory and then perform clinical trials. He expects the first AI antibiotics could take until 2030 until they are available to be prescribed.  Curiously, this experimental antibiotic had no effect on other species of bacteria, and works only on A. baumannii.  Many antibiotics kill bacteria indiscriminately. The researchers believe the precision of abaucin will make it harder for drug-resistance to emerge, and could lead to fewer side-effects.  In principle, the AI could screen tens of millions of potential compounds - something that would be impractical to do manually. ""AI enhances the rate, and in a perfect world decreases the cost, with which we can discover these new classes of antibiotic that we desperately need,"" Dr Stokes told me. The researchers tested the principles of AI-aided antibiotic discovery in E. coli in 2020, but have now used that knowledge to focus on the big nasties. They plan to look at Staphylococcus aureus and Pseudomonas aeruginosa next. ""This finding further supports the premise that AI can significantly accelerate and expand our search for novel antibiotics,"" said Prof James Collins, from the Massachusetts Institute of Technology.  He added: ""I'm excited that this work shows that we can use AI to help combat problematic pathogens such as A. baumannii."" Prof Dame Sally Davies, the former chief medical officer for England and government envoy on anti-microbial resistance, told Radio 4's The World Tonight: ""We're onto a winner."" She said the idea of using AI was ""a big game-changer, I'm thrilled to see the work he (Dr Stokes) is doing, it will save lives"". Follow James on Twitter.  Ukraine mothers go behind enemy lines to save stolen children Why South Korea woke up to sirens... in 73 seconds. VideoWhy South Korea woke up to sirens... in 73 seconds AI 'godfather' feels 'lost' over life's work Travelling into enemy land to save my stolen child A top soldier's defamation case rocks Australia New Yorkers gather for Manhattanhenge The Myanmar soldiers refusing to fight Could a US debt default unleash global chaos? Going to the city of my mother's dreams After a synagogue shooting, can a community heal? The 'exploding' demand for giant heat pumps Holmes has gone to jail. Will she pay victims too? Why it's 'imperative' to start using AI Jellyfish blooms: Why not just eat them? A 5,000-year-old craft under threat © 2023 BBC. The BBC is not responsible for the content of external sites. Read about our approach to external linking.",68,832,0.323329448699951,0.328242987394333,0.348427534103394
"SHANGHAI, May 31 (Reuters) - Chinese search giant Baidu Inc (9988.HK) will set up a venture capital fund of 1 billion yuan ($145 million) to back start-ups focused on content generated by artificial intelligence applications, it said on Wednesday. The company will also launch a competition for developers to build applications off its ERNIE large language model (LLM) or integrate the model into their existing products, it added. Chinese tech companies have raced to release their own LLMs following the dramatic success of ChatGPT, the AI-powered chatbot released by Microsft-backed (MSFT.O) OpenAI. Almost 80 organisations in China have launched their own LLMs since 2020, with releases this year slightly exceeding those of the United States, a report showed this week. In March, Baidu unveiled Ernie Bot, its own AI-powered LLM. E-commerce giant Alibaba Group Holding Ltd (9988.HK) was among the other Chinese companies that followed quickly. In response to the surge of LLMs, China published draft regulations in April on the use of generative AI. ($1=6.9121 Chinese yuan renminbi) Our Standards: The Thomson Reuters Trust Principles. Hewlett Packard Enterprise Co missed Wall Street estimates for second-quarter revenue on Tuesday, as clients scale down spending on tech, including cloud services, amid an economic slowdown. Reuters, the news and media division of Thomson Reuters, is the world’s largest multimedia news provider, reaching billions of people worldwide every day. Reuters provides business, financial, national and international news to professionals via desktop terminals, the world's media organizations, industry events and directly to consumers. Build the strongest argument relying on authoritative content, attorney-editor expertise, and industry defining technology. The most comprehensive solution to manage all your complex and ever-expanding tax and compliance needs. The industry leader for online information for tax, accounting and finance professionals.  Access unmatched financial data, news and content in a highly-customised workflow experience on desktop, web and mobile.  Browse an unrivalled portfolio of real-time and historical market data and insights from worldwide sources and experts. Screen for heightened risk individual and entities globally to help uncover hidden risks in business relationships and human networks. All quotes delayed a minimum of 15 minutes. See here for a complete list of exchanges and delays. © 2023 Reuters. All rights reserved",32,369,0.339478045701981,0.31646990776062,0.344052016735077
"Shares of NVIDIA Corp. NVDA rose 44% in May, with the company joining the elite group of five U.S. tech companies with a market cap higher than $1 trillion. The chipmaker giant launched a new class of large-memory AI supercomputer DGX GH200, designed to facilitate the building of massive, next-generation models for generative AI language applications, recommender systems and data analytics workloads. The NVIDIA rise was also spurred by a fantastic quarterly earnings report, which led to widespread gains on other semiconductor and software manufacturers, as well as any industry tied to artificial intelligence space as investors race for the AI-catchup play.  Here are 5 AI-related stocks which have outperformed the chipmaker's spectacular performance in May:  Palantir Technologies Inc. PLTR Upstart Holdings, Inc. UPST Super Micro Computer, Inc. SMCI C3.ai, Inc. AI Nano-X Imaging Ltd NNOX Chart: Top Performing AI-Related Stocks In May 2023    Read Next: Nvidia Stock Made This Insider A Billionaire — Here's How Much He Just Sold © 2023 Benzinga.com. Benzinga does not provide investment advice. All rights reserved.",59,167,0.30731588602066,0.29770103096962,0.394983053207397
"Subscribe Now! Get features like With the advent of AI tools for image generation, people have been able to bring their imaginative ideas to life and even share them on social media. Among the numerous shares online, one, in particular, has captured the attention of netizens. It portrays pictures of two characters from the series Game of Thrones, Daenerys Targaryen and Jon Snow, having a wedding in Kerala. Artist Gokul Pillai shared AI-generated pictures on Instagram with the caption, “They got married in Kerala and, trust me I was invited.” The images depict Jon Snow and Daenerys Targaryen dressed in stunning Indian wedding attire. Daenerys Targaryen adorns exquisite jewellery and wears gorgeous Indian outfits, while Jon Snow sports a traditional kurta and pyjama ensemble. A post shared by Gokul Pillai (@withgokul) Since being shared five days ago, the post has accumulated over 12,500 hearts, and the numbers are still increasing. Many even left their thoughts in the comments section. “Game of Thrissur,” wrote an individual. Another added, “King in the south.” “Haha lovely,” expressed a third. A fourth commented, “The mother of tuskers.” “Amazing,” posted a fifth. A sixth shared, “They look stunning in a traditional Kerala look.” 
Arfa Javaid is a journalist working with the Hindustan Times' Delhi team. She covers trending topics, human interest stories, and viral content online.
...view detail
  ",8,223,0.2647365629673,0.275997877120972,0.459265530109406
"Suggestions or feedback? 
    Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a 
    Creative Commons Attribution Non-Commercial No Derivatives license.
    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided 
    below, credit the images to ""MIT."" 
   
          Previous image
          Next image
         When machine-learning models are deployed in real-world situations, perhaps to flag potential disease in X-rays for a radiologist to review, human users need to know when to trust the model’s predictions.  But machine-learning models are so large and complex that even the scientists who design them don’t understand exactly how the models make predictions. So, they create techniques known as saliency methods that seek to explain model behavior.  With new methods being released all the time, researchers from MIT and IBM Research created a tool to help users choose the best saliency method for their particular task. They developed saliency cards, which provide standardized documentation of how a method operates, including its strengths and weaknesses and explanations to help users interpret it correctly.  They hope that, armed with this information, users can deliberately select an appropriate saliency method for both the type of machine-learning model they are using and the task that model is performing, explains co-lead author Angie Boggust, a graduate student in electrical engineering and computer science at MIT and member of the Visualization Group of the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL).  Interviews with AI researchers and experts from other fields revealed that the cards help people quickly conduct a side-by-side comparison of different methods and pick a task-appropriate technique. Choosing the right method gives users a more accurate picture of how their model is behaving, so they are better equipped to correctly interpret its predictions.  “Saliency cards are designed to give a quick, glanceable summary of a saliency method and also break it down into the most critical, human-centric attributes. They are really designed for everyone, from machine-learning researchers to lay users who are trying to understand which method to use and choose one for the first time,” says Boggust.  Joining Boggust on the paper are co-lead author Harini Suresh, an MIT postdoc; Hendrik Strobelt, a senior research scientist at IBM Research; John Guttag, the Dugald C. Jackson Professor of Computer Science and Electrical Engineering at MIT; and senior author Arvind Satyanarayan, associate professor of computer science at MIT who leads the Visualization Group in CSAIL. The research will be presented at the ACM Conference on Fairness, Accountability, and Transparency.  Picking the right method  The researchers have previously evaluated saliency methods using the notion of faithfulness. In this context, faithfulness captures how accurately a method reflects a model’s decision-making process.  But faithfulness is not black-and-white, Boggust explains. A method might perform well under one test of faithfulness, but fail another. With so many saliency methods, and so many possible evaluations, users often settle on a method because it is popular or a colleague has used it.  However, picking the “wrong” method can have serious consequences. For instance, one saliency method, known as integrated gradients, compares the importance of features in an image to a meaningless baseline. The features with the largest importance over the baseline are most meaningful to the model’s prediction. This method typically uses all 0s as the baseline, but if applied to images, all 0s equates to the color black.  “It will tell you that any black pixels in your image aren’t important, even if they are, because they are identical to that meaningless baseline. This could be a big deal if you are looking at X-rays since black could be meaningful to clinicians,” says Boggust.   Saliency cards can help users avoid these types of problems by summarizing how a saliency method works in terms of 10 user-focused attributes. The attributes capture the way saliency is calculated, the relationship between the saliency method and the model, and how a user perceives its outputs.  For example, one attribute is hyperparameter dependence, which measures how sensitive that saliency method is to user-specified parameters. A saliency card for integrated gradients would describe its parameters and how they affect its performance. With the card, a user could quickly see that the default parameters — a baseline of all 0s — might generate misleading results when evaluating X-rays.  The cards could also be useful for scientists by exposing gaps in the research space. For instance, the MIT researchers were unable to identify a saliency method that was computationally efficient, but could also be applied to any machine-learning model.  “Can we fill that gap? Is there a saliency method that can do both things? Or maybe these two ideas are theoretically in conflict with one another,” Boggust says.  Showing their cards  Once they had created several cards, the team conducted a user study with eight domain experts, from computer scientists to a radiologist who was unfamiliar with machine learning. During interviews, all participants said the concise descriptions helped them prioritize attributes and compare methods. And even though he was unfamiliar with machine learning, the radiologist was able to understand the cards and use them to take part in the process of choosing a saliency method, Boggust says.  The interviews also revealed a few surprises. Researchers often expect that clinicians want a method that is sharp, meaning it focuses on a particular object in a medical image. But the clinician in this study actually preferred some noise in medical images to help them attenuate uncertainty.  “As we broke it down into these different attributes and asked people, not a single person had the same priorities as anyone else in the study, even when they were in the same role,” she says.  Moving forward, the researchers want to explore some of the more under-evaluated attributes and perhaps design task-specific saliency methods. They also want to develop a better understanding of how people perceive saliency method outputs, which could lead to better visualizations. In addition, they are hosting their work on a public repository so others can provide feedback that will drive future work, Boggust says.  “We are really hopeful that these will be living documents that grow as new saliency methods and evaluations are developed. In the end, this is really just the start of a larger conversation around what the attributes of a saliency method are and how those play into different tasks,” she says.  The research was supported, in part, by the MIT-IBM Watson AI Lab, the U.S. Air Force Research Laboratory, and the U.S. Air Force Artificial Intelligence Accelerator. 
    Previous item
    Next item
   
              Read full story →
             
              Read full story →
             
              Read full story →
             
              Read full story →
             
              Read full story →
             
              Read full story →
             This website is managed by the MIT News Office, part of the Institute Office of Communications. Massachusetts Institute of Technology77 Massachusetts Avenue, Cambridge, MA, USA",22,1388,0.316327273845673,0.327575266361237,0.356097400188446
"Team behind ChatGPT say equivalent of atomic watchdog is needed to guard against risks of ‘superintelligent’ AIs The leaders of the ChatGPT developer OpenAI have called for the regulation of “superintelligent” AIs, arguing that an equivalent to the International Atomic Energy Agency is needed to protect humanity from the risk of accidentally creating something with the power to destroy it. In a short note published to the company’s website, co-founders Greg Brockman and Ilya Sutskever and the chief executive, Sam Altman, call for an international regulator to begin working on how to “inspect systems, require audits, test for compliance with safety standards, [and] place restrictions on degrees of deployment and levels of security” in order to reduce the “existential risk” such systems could pose. “It’s conceivable that within the next 10 years, AI systems will exceed expert skill level in most domains, and carry out as much productive activity as one of today’s largest corporations,” they write. “In terms of both potential upsides and downsides, superintelligence will be more powerful than other technologies humanity has had to contend with in the past. We can have a dramatically more prosperous future; but we have to manage risk to get there. Given the possibility of existential risk, we can’t just be reactive.” In the shorter term, the trio call for “some degree of coordination” amongcompanies working on the cutting-edge of AI research, in order to ensure the development of ever-more powerful models integrates smoothly with society while prioritising safety. That coordination could come through a government-led project, for instance, or through a collective agreement to limit growth in AI capability. Researchers have been warning of the potential risks of superintelligence for decades, but as AI development has picked up pace those risks have become more concrete. The US-based Center for AI Safety (CAIS), which works to “reduce societal-scale risks from artificial intelligence”, describes eight categories of “catastrophic” and “existential” risk that AI development could pose. While some worry about a powerful AI completely destroying humanity, accidentally or on purpose, CAIS describes other more pernicious harms. A world where AI systems are voluntarily handed ever more labour could lead to humanity “losing the ability to self-govern and becoming completely dependent on machines”, described as “enfeeblement”; and a small group of people controlling powerful systems could “make AI a centralising force”, leading to “value lock-in”, an eternal caste system between ruled and rulers. OpenAI’s leaders say those risks mean “people around the world should democratically decide on the bounds and defaults for AI systems”, but admit that “we don’t yet know how to design such a mechanism”. However, they say continued development of powerful systems is worth the risk. Sign up to TechScape Alex Hern's weekly dive in to how technology is shaping our lives after newsletter promotion “We believe it’s going to lead to a much better world than what we can imagine today (we are already seeing early examples of this in areas like education, creative work, and personal productivity),” they write. They warn it could also be dangerous to pause development. “Because the upsides are so tremendous, the cost to build it decreases each year, the number of actors building it is rapidly increasing, and it’s inherently part of the technological path we are on. Stopping it would require something like a global surveillance regime, and even that isn’t guaranteed to work. So we have to get it right.”",74,568,0.414546519517899,0.281406402587891,0.304047048091888
"The .gov means it’s official.Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you're on a federal government site. The site is secure. The https:// ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely. Artificial Intelligence (AI) and Machine Learning (ML) can be described as a branch of computer science, statistics, and engineering that uses algorithms or models to perform tasks and exhibit behaviors such as learning, making decisions, and making predictions. ML is considered a subset of AI that allows models to be developed by training algorithms through analysis of data, without models being explicitly programmed. FDA recognizes the increased use of AI/ML throughout the drug development life cycle and across a range of therapeutic areas. In fact, FDA has seen a significant increase in the number of drug and biologic application submissions using AI/ML components over the past few years, with more than 100 submissions reported in 2021. These submissions traverse the landscape of drug development — from drug discovery and clinical research to postmarket safety surveillance and advanced pharmaceutical manufacturing.   Additionally, AI/ML is increasingly integrated in areas where FDA is actively engaged, including Digital Health Technologies (DHTs), and Real-World Data (RWD) analytics.  FDA is committed to ensuring that drugs are safe and effective while facilitating innovations in their development. As with any innovation, AI/ML creates opportunities and new and unique challenges. To meet these challenges, FDA has accelerated its efforts to create an agile regulatory ecosystem that can facilitate innovation while safeguarding public health. As part of this effort, FDA’s Center for Drug Evaluation and Research (CDER), in collaboration with the Center for Biologics Evaluation and Research (CBER) and the Center for Devices and Radiological Health (CDRH), issued an initial discussion paper to communicate with a range of stakeholders and to explore relevant considerations for the use of AI/ML in the development of drugs and biological products. The agency will continue to solicit feedback as it advances regulatory science in this area. AI/ML will undoubtedly play a critical role in drug development, and FDA plans to develop and adopt a flexible risk-based regulatory framework that promotes innovation and protects patient safety. 05/16/2023",67,373,0.31937164068222,0.243053808808327,0.437574476003647
"The California Issue In Silicon Valley’s hacker houses, the latest crop of young entrepreneurs is partying, innovating — and hoping not to get crushed by the big guys. Emily Liu (center) and Dave Fontenot, two co-founders of the start-up accelerator HF0, with Marylin Ma, a member of its latest batch of fellows, in San Francisco.Credit...Laura Morton for The New York Times Supported by Send any friend a story As a subscriber, you have 10 gift articles to give each month. Anyone can read what you share. By Yiren Lu The archbishop’s mansion in San Francisco, built in 1904, is now a stately hotel at the northeast corner of Alamo Square Park. Since February, it has been rented out entirely to HF0, or Hacker Fellowship Zero, a start-up accelerator that provides 12-week residencies for batches of fellows from 10 different start-ups. Their experience, which culminates in a demonstration day, is supposed to be the most productive three months of the fellows’ lives. Dave Fontenot, one of HF0’s founders, was inspired by the two years he spent living in monasteries in his 20s: While monastery life was materially ascetic, he found that it was luxurious in the freedom it gave residents to focus on the things that really mattered. And at the Archbishop’s Mansion this year, almost everyone has been monastically focused on what has become San Francisco’s newest religion: artificial intelligence. For more audio journalism and storytelling, download New York Times Audio, a new iOS app available for news subscribers. The A.I. gospel had not yet spread in 2021, when Fontenot and his two co-founders, Emily Liu and Evan Stites-Clayton, started the accelerator. Even a year ago, when HF0 hosted a batch of fellows at a hotel in Miami, six out of the eight companies represented were cryptocurrency start-ups. But at the mansion in San Francisco, eight of the 10 companies in HF0’s first batch this year were working on A.I.-based apps, and the lone crypto start-up — focused on what happens to your Bitcoin when you die — was worried, they told me, about whether the investors who showed up at this spring’s demo day would actually want to invest in them. That generative A.I. has largely supplanted crypto in the eyes of founders and venture capitalists alike is not exactly surprising. When OpenAI released ChatGPT late last year, it sparked a new craze at a time when the collapsing crypto and tech markets had left many investors and would-be entrepreneurs adrift, unsure of where to put their capital and time. Suddenly users everywhere were realizing that A.I. could now respond to verbal queries with a startling degree of humanlike fluency. “Large language models have been around for a long time, but their uses were limited,” says Robert Nishihara, a co-founder of Anyscale, a start-up for machine-learning infrastructure. “But there’s a threshold where they become dramatically more useful, and I think now it’s crossed that.” One appeal of generative A.I. is that it offers something for every would-be entrepreneur. For the technically minded, there is research to be done. For the business types, it’s easy to create applications on top of the OpenAI platforms. For the philosophically inclined, A.I. offers interesting avenues through which to explore what it means to be conscious and human. And unlike crypto, especially now, A.I. is a more credible field to be in for mainstream techies. Its products have already achieved significant traction among consumers — ChatGPT is believed to be the fastest app ever to hit 100 million users — and some of the figures at its forefront are familiar faces, now in their second acts, like Sam Altman, formerly the president of the start-up accelerator Y Combinator, and Greg Brockman, formerly the chief technology officer at Stripe, the payments-processing company. In short, you can’t help thinking that, as one friend recently proclaimed to me, “Everyone in S.F. is either starting or running an A.I. company or starting or running an A.I. fund.” A.I., in turn, seems to love San Francisco back. During the pandemic, as tech workers went remote and Twitter pundits evangelized the tax benefits of being in Austin or Miami, the San Francisco area seemed poised to cede its start-up primacy. But recently that trend has reversed. There’s a sense that if you want to be working in A.I., this is still the place to be. “We actually first considered doing the batch in New York, but when I went to New York and asked people what they thought of GitHub Copilot” — an A.I.-powered coding assistant — “people told me they maybe tried it once,” Fontenot said. “On the other hand, people in S.F. told me they were using it to write 50 percent of their code.” Fontenot’s anecdote gets at one of Silicon Valley’s enduring qualities: the willingness, even eagerness, to embrace new technology. Out in the rest of the world, A.I. is triggering nerves — fears and even predictions of wiped-out jobs, of existential doom — and endless commentary. In San Francisco, it has kindled all these things too, but also a question just as powerful: How do you get a piece of it? During the day, the Archbishop’s Mansion often feels surprisingly empty and quiet, perhaps because it’s so large. There are four floors and a grand staircase that winds up through the middle of the building, lit by a giant skylight. Many of the teams work in their rooms upstairs; some teams work in the “hackspace” in the basement, with its whiteboards and rows of standing desks. When I was visiting this spring, one wall displayed some ChatGPT-generated poetry: “In HF0, the hackers work and play/With laughter and fun, throughout the day./They’re a community of techies, with a heart of gold,/And their humor and hacking skills, never grow old.” But on a spring Friday night — the one night of the week when the broader tech community is welcomed in — the A.I. party was in full swing. Fontenot and Liu bounded around the common spaces at the front of the mansion, giving out effusive hellos and introductions. In one back room, a bar served elixirs. (The mansion is a no-alcohol zone.) In another, an A.I. rap battle raged. (It wasn’t much of a battle, actually — while the A.I. is no Eminem, it was still destroying everyone.) In a third, Jonathan Shobrook, a fellow, was demoing his product, Adrenaline, a tool that lets you ask natural-language questions of your code base. He had the interface up on a monitor and a small cluster of spectators around him, seemingly riveted. “Can you ask it to implement ReLU?” Sasha Sheng asked. Sheng, a former software engineer at Facebook, is now working on her own app; in dyed pigtails and a baseball cap, she is something of a personality in the community. “Oh, yeah, that’s a hard question,” Shobrook responded. On his keyboard, he typed, “Which neural networks use ReLU?” The right answer flashed on the screen, the cursor blinking as characters appeared. Someone asked how it worked. “I just basically chunked up all the files into functions and classes and groups of code, generated summaries of those code chunks and then recursively summarized the file,” Shobrook said. “Do you use an abstract syntax tree?” Only in San Francisco would people be talking about abstract syntax trees at 9 p.m. on a Friday night. Out in the main entryway, someone introduced himself as Bruno. I asked him whether he was in A.I. “My first two companies were in A.I., but now I’m in crypto,” he said jovially. Fontenot came up from behind and slung an arm over his shoulders. “I’m not popular anymore, no one wants to talk to me,” Bruno fake-moaned. But he didn’t seem deeply bothered by crypto’s abrupt comedown and A.I.’s ascendance. Bruno, it turned out, was Bruno Faviero, a well-known investor and entrepreneur. He and Fontenot have been buddies since they met as college students, when each of them was organizing hackathons. After Fontenot left school in 2013 — he dropped out of the University of Michigan, where he was studying computer science — he continued to run hackathons and networked in the tech world as Faviero built his first company. “Four years ago, he called me to say that he was raising a fund,” Faviero told me. “I was like, ‘Yeah, whatever, everyone is raising a fund.’ A week later, he calls me and is like, ‘Hey, the fund is oversubscribed, do you still want to put in a check?’ If Dave says he’s going to do something, he does it.” Or, as Emily Liu put it to me, “You show up to one of Dave’s things as a friend, and 10 minutes later you’re wearing a staff shirt.” Fontenot is charismatic, a forceful speaker with wild hair. Like all good venture capitalists — he is a general partner in the investment firm Backend Capital — he has an unerring nose for the thing of the moment, be it blockchain or A.I. That he seems agnostic about whether it’s blockchain or A.I., or some other underlying technology, is almost beside the point. In many ways, he personifies the modern Silicon Valley dichotomy between spirituality and hustle, between monasticism and flamboyance. His expertise, he believes, is people. “We look for three things — grit, storytelling ability and product sense,” he said, describing the selection process for the fellows. Notably absent from this list, I pointed out, was a background in machine learning. Fontenot shrugged. This generation of start-ups doesn’t have to come up with its own cutting-edge research. Big companies like OpenAI and Google will provide that. Instead, he said, the fellows need the ability to build prototypes quickly on top of the new models. And, in fact, the unifying thread among the first batch of 2023 fellows was their experience at that sort of enterprise. The average age was 28 (Fontenot is 30), and several of them were second-time founders. Adam Reis is a founder of Candid Health, which provides medical-billing software. Emma Salinas founded an online community called Gen Z Mafia. While various fellows often talk about how they’ve long been interested in A.I., it’s clear that some of the “why now” is opportunistic. But if the people at hackathons and programs like HF0 tend to be newcomers to A.I., this doesn’t preclude their success: The consensus is that building things in the A.I. field isn’t as complex as working in biology, say; you don’t need to get a separate Ph.D. in it. If you’re already good at math and good at engineering and good at business, there are few limits to what you can do. A few themes characterize the sorts of projects the HF0 fellows have been working on. On the one hand, there are applications to automate tedious business tasks like copywriting or spreadsheet wrangling. A company called Fileread falls into this category. Its law-firm customers upload all the documents relevant to a particular case into an online portal; Fileread indexes those documents into a special database that enables users to search the documents not only for exact terms like “truck” or “James,” but also for broader questions like “who made the transaction?” or “what are the relevant cases?” Under the hood, Fileread first fetches the most relevant documents from its database, then adds those documents to a user’s question and sends the whole, long query to the OpenAI application programming interface, or A.P.I. Fileread then spits out an answer, powered by the same large language models behind ChatGPT. Without A.I., identifying and crafting a legal narrative by piecing together textual evidence from thousands of sources is a painstakingly manual process. Most of Fileread’s customers specialize in business litigation, including antitrust and liability cases. Sometimes they are paid on contingency, which means when they succeed, they typically get a percentage of the award or settlement, but when they lose, they get nothing. Firms need the A.I. to efficiently search for evidence in the documents that might, for example, either establish or refute liability. “They don’t have the manpower or the budget to do unlimited document review,” says Chan Koh, a Fileread founder and an HF0 fellow. “They want to spend the minimal amount of effort in order to win the case.” Other HF0 fellows have been creating applications that lean into A.I.’s seemingly human affect in order to tackle some psychological need. For instance, Brian Basham, who has worked in Google’s Brain division and since 2018 has been a life coach in California, is working on Thyself, a subscription service for “guided emotional inquiry” that currently uses A.I. and human coaches but will eventually transition fully to A.I. I met him and his employee Maverick Kuhn over dinner one night at the Archbishop’s Mansion. After Kuhn waxed rhapsodic about a four-week-long retreat he attended last summer, called Sleepawake, I asked him whether the experience would have been as great if the facilitators had said and done all the same things but been A.I.’s. “Probably not,” he conceded. “That would very much be a disembodied head.” One-on-one life coaching from the current human-A.I. hybrid version of Thyself costs $50 an hour. Once the service is fully automated, Basham expects to be able to offer unlimited sessions for $30 a month. At that price, he believes, it would be broadly accessible. A few days later, I did my first Thyself session. Mostly it consisted of the bot asking me to visualize scenarios — remembered or imagined scenes — and then to describe the physical sensations and emotions that resulted from “surfing the emotional wave.” I didn’t feel much of an emotional wave, but I was impressed by how natural it felt to speak to an A.I.-filtered guide. Compared with calling, say, the automated hotline for a cell-service provider, it was a vast improvement, although it did tend to talk over me. Evan Stites-Clayton, an HF0 founder (and a fellow in the accelerator’s inaugural batch), has come up with a similarly intimate product, an A.I. assistant called Consort. To try it out, I had to go through a 15-minute quasi-therapy session, where I was asked about my childhood, my relationship to my parents and my favorite books. A few hours after my responses were fed into the A.I., Stites-Clayton — who was a founder of Teespring (now Spring), a platform that sells custom-made T-shirts and other merchandise — sent me a link to my “consort,” which I could then text. Over the course of the next couple of days, it texted me a daily message at midnight, reminding me to wind down for the night. On the weekends, it asked me whether I was planning to go out. The texts included appropriately casual spelling and (lack of) punctuation. I found myself warming to it, despite an earlier prejudice against becoming friends with A.I. A.I. and emotional regulation might seem like an odd juxtaposition, but it makes sense that emotional labor — at the end of the day, just another form of labor — could be one of the first job categories to be transformed by automation. And yet, setting aside its effectiveness, there’s something odd about using A.I. to manage our human brains when it’s not clear that the A.I. brain is at all similar to ours. “We’re obviously trying to anthropomorphize A.I., make it in our image,” said Matthew Rastovac, the founder of Respell, a tool that lets you create A.I. apps without doing any coding. “Because we don’t really know how else to build and understand a new kind of intelligence. But I think it’s much more likely that it’s going to be like a reptile, in that it has its instincts, but we can’t understand what’s going on inside its brain and listen to its actual thoughts.” We were sitting on the roof of Atmosphere, a hacker house in Nob Hill that he helped found; all around us, San Francisco was enchanting in the afternoon light. Earlier, he paraphrased for me some lines that he liked from Season 2 of “Westworld” that spoke to how early we still are, and how blinkered, when it comes to understanding this technology: “Sanity is a very narrow sliver of the possibilities of mind. Because we have culturally accepted norms, we have a certain way of acting and thinking and speaking, and if you deviate from that a little too much, then you’re, at best, weird, and at worst, clinically insane.” During the week I spent staying at HF0, everyone told me I had to make it down to the South Bay for the A.G.I. House GPT-4 hackathon. The organizers asked me to come after 6 p.m., though, so as to not distract the hackers before it started. A.G.I. stands for artificial general intelligence, a phrase that has come to represent a potential dream goal for A.I.: a machine intelligence with the flexibility to handle any intellectual task that humans can. A.G.I. House, it turns out, is a $68 million mansion in the small town of Hillsborough, 25 minutes from downtown Palo Alto. The mansion has a long thoroughfare of ferns out front, a pool and a barbecue pit in the back. Rocky Yu, previously the chief executive of an augmented-reality start-up, runs A.G.I. House, overseeing both its 10 residents and a raft of community events. He is warm and smiley and exceedingly well connected in the local A.I. community. The crowd at that night’s GPT-4 hackathon was so large as to render the Wi-Fi basically nonfunctional. Every room overflowed with hackers crowding around whiteboards. In the kitchen, Chinese takeout was laid out on a table. A smattering of investors were present to check out the demos, which started at 8 p.m. with short speeches from the organizers. The speeches were all variations on a theme: We are living in a momentous time. Maybe in a few decades from now, we’ll look back at all these seminal A.I. achievements and see that they all came from this house in Hillsborough. As at HF0, the demos here alternated between business uses and personal applications — a chatbot that impersonates business gurus like Mark Cuban, the owner of the Dallas Mavericks basketball team and a judge on “Shark Tank,” the business-reality TV show, and that allows you to ask for business advice; or an A.I. sommelier that will take your dinner menu and suggest an appropriate wine pairing. Six months ago, any one of these projects might have seemed remarkable, but the arrival of ChatGPT has remade expectations. “The one pattern I’m starting to see is that ChatGPT is the killer app,” the technologist Diego Basch has written on Twitter. “None of the tools built on top of the A.P.I. have been as useful to me.” Indeed, if you are building something on top of OpenAI’s A.P.I., it does seem as though your app’s marginal value has to be extremely high if it is to avoid being bulldozed by either OpenAI itself or one of the big tech companies like Google and Microsoft (or even later-stage start-ups that are rapidly rolling out A.I.-enabled features in their products). As two analysts at N.E.A., an investment firm, put it in a recent report, generative A.I. may not be as disruptive to established businesses, and beneficial to start-ups, as previous big shifts in tech platforms. “Unlike with the prior shifts, incumbents do not need to re-architect their entire products to adopt this new platform shift,” the analysts wrote. “In addition, this shift favors companies with bigger, proprietary data sets which can give an edge to more established companies.” During previous tech eras, start-ups could introduce a superior technology or interface and then race to build market share before entrenched competitors could match them. But with large language models, incumbents like Google and Microsoft have had a huge head start in both developing the technology and acquiring market share among consumers. The situation risks becoming like that of the pharmaceutical industry, in which research and development is outsourced to start-ups and many of the benefits ultimately accrue to the parent company. Moreover, the capital-intensive nature of training large language models means that smaller companies like OpenAI and Anthropic creating their own large language models have few alternatives beyond making Faustian “partnerships” with tech giants. This doesn’t mean that generative A.I. isn’t going to transform industries or eliminate jobs. Beyond the incumbents, one beneficiary might well be the indie hacker, the kind of coder for hire who does niche A.I. projects to fill specific needs in specific industries. Problems that have been too esoteric to solve or work flows that have been too complicated to improve might become easily automated with the help of ChatGPT. As the Gumroad founder Sahil Lavingia recently put it on a podcast, “If I had a friend who’s like, ‘I want to make 200K a year, building something in S.A.S.’” — software as a service — “ ‘building some A.I. tool,’ I would basically tell them to walk around their neighborhood, go to as many businesses as possible and see what manual thing, what piece of paper you sign, and figure out how to automate that.” That the era of V.C.-backed, outsize returns might be coming to an end is, of course, a source of anxiety. Everyone in Silicon Valley knows someone who worked at the last generation of successful start-ups, start-ups whose growth followed the proverbial hockey-stick line on a graph, and reaped the benefits. Who doesn’t want to get their share — whether money, status, fame — before it all runs out? This anxiety is compounded by the fact that it feels as if the deterioration of the physical world is happening roughly at a pace with the flourishing of the virtual one, and the only way to insulate yourself is by achieving vast financial success. In some ways, San Francisco perfectly embodies this tension: A.I. is moving toward A.G.I., but outside the high-end tech offices, there is rampant homelessness, house prices are so high that even couples with two tech incomes can’t afford to buy property and children are sufficiently rare as to make them a spectacle. But the anxiety runs deeper than concerns about success, prestige or even material safety. Change has always been accompanied by hand-wringing, and the pace of change in A.I. right now is mind-bending. The resulting mood is perhaps best encapsulated by tweets from Tiago Forte, the productivity guru known for a self-help system called Second Brain. “I’m feeling a broad loss of motivation for many projects and goals that used to excite me due to what I’m seeing with A.I.,” Forte posted in April. “It’s not fear of A.I. apocalypse or fear that I’ll lose my job or anything like that. ...More like a feeling of grief that many of the personal skills & qualities I’ve spent a lot of time developing have suddenly been devalued.” This, of course, is not a new kind of ennui. It has always been a bewildering experience to lose your livelihood because of technological change; Silicon Valley has just generally been on the right side of it. For the first time, such change heralds an era in which software engineers themselves may be less well compensated and less in demand. After years of disrupting other industries, Silicon Valley has disrupted itself. Back at the Archbishop’s Mansion, 52 general partners from the Valley’s top venture-capital funds were in attendance as the year’s first batch of HF0 fellows made their presentations at demo day on April 4. A few days later, Fontenot posted a behind-the-scenes video of the event. It started with drone footage zooming in on the mansion, then cut to a close-up of Adam Reis, a fellow, jittery with nerves before his presentation. “Y’all, this room is [expletive] stacked,” Fontenot said in the video. “Sequoia’s here, Benchmark’s here, [expletive] a16z is here. Everyone’s here. So anyone you would want to meet, they’re here. And they’re excited.” He was wearing a pink, woolly winter hat with a pom-pom, inexplicably, and he sounded like Caesar rallying the troops. Two weeks after demo day, all 10 teams had received initial offers from outside investors and some had chosen lead investors. Fontenot was optimistic. HF0’s next batch, to be hosted again at the mansion, was set to begin in May, and as he was reviewing applications, he texted me, “The talent coming in now is insane.” A few days later, I saw on Twitter that a friend of mine, Travis Fischer, would be joining the next HF0 batch. He and I last hung out in real life two years ago. At the time, the hot thing was the “creator economy,” and he was looking to develop tools that would enable people, particularly open-source software developers, to monetize their work. While that effort, in the end, wasn’t fruitful, last year he started a series of side projects in A.I. These included coming up with a way for other developers to use the ChatGPT A.P.I. so they can more easily incorporate large language models into their products. Travis no longer talks as much about the creator economy; at HF0, he is now working on an open-source framework for building reliable A.I. agents that do things such as booking airplane tickets or submitting tax documents. But despite the shifts in theme, my sense is that what he’s passionate about — making tools for the open-source community — hasn’t changed. He has just found a way to come at it from a different angle. And in that adaptability, that ability to reinvent himself while coming out on top, he resembles Silicon Valley itself. Yiren Lu is the chief executive of Frindle, a technical writing agency. She last wrote for the magazine about researchers’ designing and mass-producing genetic material. Laura Morton is a photographer based in San Francisco. A Pierre & Alexandra Boulat Grant recipient, she has been documenting tech start-up culture since 2014. Few people are as involved in so many A.I. efforts as Reid Hoffman, a Silicon Valley investor. His mission? Showing how new technologies can improve humanity. The pop singer Grimes has “open-sourced” her voice using new A.I. tools and invited everyone to use her sound to generate new songs. She has been impressed by the results. Tens of millions of American jobs could be automated by generative A.I. The companies behind these new technologies are looking to the government to regulate the industry. Amid those making pledges and sweeping plans aimed at reining in A.I., New York City has emerged as a modest pioneer in its regulation with a focused approach. The financial services industry is looking at how to incorporate tools like ChatGPT into its products. But humans will still be necessary to provide personal advice. Meta has publicly released its latest A.I. technology so people can build their own chatbots. Rivals like Google say that approach can be dangerous. Advertisement",16,4470,0.338909506797791,0.314948976039886,0.346141546964645
"The World Health Organization (WHO) is calling for caution to be exercised in using artificial intelligence (AI) generated large language model tools (LLMs) to protect and promote human well-being, human safety, and autonomy, and preserve public health.  LLMs include some of the most rapidly expanding platforms such as ChatGPT, Bard, Bert and many others that imitate understanding, processing, and producing human communication. Their meteoric public diffusion and growing experimental use for health-related purposes is generating significant excitement around the potential to support people’s health needs.  It is imperative that the risks be examined carefully when using LLMs to improve access to health information, as a decision-support tool, or even to enhance diagnostic capacity in under-resourced settings to protect people’s health and reduce inequity. While WHO is enthusiastic about the appropriate use of technologies, including LLMs, to support health-care professionals, patients, researchers and scientists, there is concern that caution that would normally be exercised for any new technology is not being exercised consistently with LLMs. This includes widespread adherence to key values of transparency, inclusion, public engagement, expert supervision, and rigorous evaluation.  Precipitous adoption of untested systems could lead to errors by health-care workers, cause harm to patients, erode trust in AI and thereby undermine (or delay) the potential long-term benefits and uses of such technologies around the world.  Concerns that call for rigorous oversight needed for the technologies to be used in safe, effective, and ethical ways include:  WHO proposes that these concerns be addressed, and clear evidence of benefit be measured before their widespread use in routine health care and medicine – whether by individuals, care providers or health system administrators and policy-makers.  WHO reiterates the importance of applying ethical principles and appropriate governance, as enumerated in the WHO guidance on the ethics and governance of AI for health, when designing, developing, and deploying AI for health. The 6 core principles identified by WHO are: (1) protect autonomy; (2) promote human well-being, human safety, and the public interest; (3) ensure transparency, explainability, and intelligibility; (4) foster responsibility and accountability; (5) ensure inclusiveness and equity; (6) promote AI that is responsive and sustainable.  Subscribe to our newsletters → Related",78,366,0.386264532804489,0.232041582465172,0.381693869829178
"To continue, please click the box below to let us know you're not a robot. Please make sure your browser supports JavaScript and cookies and that you are not
            blocking them from loading.
            For more information you can review our Terms of
                Service and Cookie Policy. For inquiries related to this message please contact
            our support team and provide the reference ID below.",64,111,0.220375716686249,0.337804913520813,0.441819399595261
"To ensure a human-centric and ethical development of Artificial Intelligence (AI) in Europe, MEPs endorsed new transparency and risk-management rules for AI systems.  On Thursday, the Internal Market Committee and the Civil Liberties Committee adopted a draft negotiating mandate on the first ever rules for Artificial Intelligence with 84 votes in favour, 7 against and 12 abstentions. In their amendments to the Commission’s proposal, MEPs aim to ensure that AI systems are overseen by people, are safe, transparent, traceable, non-discriminatory, and environmentally friendly. They also want to have a uniform definition for AI designed to be technology-neutral, so that it can apply to the AI systems of today and tomorrow. Risk based approach to AI - Prohibited AI practices The rules follow a risk-based approach and establish obligations for providers and users depending on the level of risk the AI can generate. AI systems with an unacceptable level of risk to people’s safety would be strictly prohibited, including systems that deploy subliminal or purposefully manipulative techniques, exploit people’s vulnerabilities or are used for social scoring (classifying people based on their social behaviour, socio-economic status, personal characteristics). MEPs substantially amended the list to include bans on intrusive and discriminatory uses of AI systems such as: High-risk AI  MEPs expanded the classification of high-risk areas to include harm to people’s health, safety, fundamental rights or the environment. They also added AI systems to influence voters in political campaigns and in recommender systems used by social media platforms (with more than 45 million users under the Digital Services Act) to the high-risk list.  General-purpose AI - transparency measures MEPs included obligations for providers of foundation models - a new and fast evolving development in the field of AI - who would have to guarantee robust protection of fundamental rights, health and safety and the environment, democracy and rule of law. They would need to assess and mitigate risks, comply with design, information and environmental requirements and register in the EU database. Generative foundation models, like GPT, would have to comply with additional transparency requirements, like disclosing that the content was generated by AI, designing the model to prevent it from generating illegal content and publishing summaries of copyrighted data used for training. Supporting innovation and protecting citizens' rights To boost AI innovation, MEPs added exemptions to these rules for research activities and AI components provided under open-source licenses. The new law promotes regulatory sandboxes, or controlled environments, established by public authorities to test AI before its deployment.  MEPs want to boost citizens’ right to file complaints about AI systems and receive explanations of decisions based on high-risk AI systems that significantly impact their rights. MEPs also reformed the role of the EU AI Office, which would be tasked with monitoring how the AI rulebook is implemented. Quotes After the vote, co-rapporteur Brando Benifei (S&D, Italy) said: “We are on the verge of putting in place landmark legislation that must resist the challenge of time. It is crucial to build citizens’ trust in the development of AI, to set the European way for dealing with the extraordinary changes that are already happening, as well as to steer the political debate on AI at the global level. We are confident our text balances the protection of fundamental rights with the need to provide legal certainty to businesses and stimulate innovation in Europe”. Co-rapporteur Dragos Tudorache (Renew, Romania) said: “Given the profound transformative impact AI will have on our societies and economies, the AI Act is very likely the most important piece of legislation in this mandate. It’s the first piece of legislation of this kind worldwide, which means that the EU can lead the way in making AI human-centric, trustworthy and safe. We have worked to support AI innovation in Europe and to give start-ups, SMEs and industry space to grow and innovate, while protecting fundamental rights, strengthening democratic oversight and ensuring a mature system of AI governance and enforcement."" Next steps Before negotiations with the Council on the final form of the law can begin, this draft negotiating mandate needs to be endorsed by the whole Parliament, with the vote expected during the 12-15 June session.",70,696,0.327206403017044,0.309111595153809,0.363681942224503
"To revisit this article, select My Account, then View saved stories To revisit this article, visit My Profile, then View saved stories By Ted Chiang When we talk about artificial intelligence, we rely on metaphor, as we always do when dealing with something new and unfamiliar. Metaphors are, by their nature, imperfect, but we still need to choose them carefully, because bad ones can lead us astray. For example, it’s become very common to compare powerful A.I.s to genies in fairy tales. The metaphor is meant to highlight the difficulty of making powerful entities obey your commands; the computer scientist Stuart Russell has cited the parable of King Midas, who demanded that everything he touched turn into gold, to illustrate the dangers of an A.I. doing what you tell it to do instead of what you want it to do. There are multiple problems with this metaphor, but one of them is that it derives the wrong lessons from the tale to which it refers. The point of the Midas parable is that greed will destroy you, and that the pursuit of wealth will cost you everything that is truly important. If your reading of the parable is that, when you are granted a wish by the gods, you should phrase your wish very, very carefully, then you have missed the point. So, I would like to propose another metaphor for the risks of artificial intelligence. I suggest that we think about A.I. as a management-consulting firm, along the lines of McKinsey & Company. Firms like McKinsey are hired for a wide variety of reasons, and A.I. systems are used for many reasons, too. But the similarities between McKinsey—a consulting firm that works with ninety per cent of the Fortune 100—and A.I. are also clear. Social-media companies use machine learning to keep users glued to their feeds. In a similar way, Purdue Pharma used McKinsey to figure out how to “turbocharge” sales of OxyContin during the opioid epidemic. Just as A.I. promises to offer managers a cheap replacement for human workers, so McKinsey and similar firms helped normalize the practice of mass layoffs as a way of increasing stock prices and executive compensation, contributing to the destruction of the middle class in America. A former McKinsey employee has described the company as “capital’s willing executioners”: if you want something done but don’t want to get your hands dirty, McKinsey will do it for you. That escape from accountability is one of the most valuable services that management consultancies provide. Bosses have certain goals, but don’t want to be blamed for doing what’s necessary to achieve those goals; by hiring consultants, management can say that they were just following independent, expert advice. Even in its current rudimentary form, A.I. has become a way for a company to evade responsibility by saying that it’s just doing what “the algorithm” says, even though it was the company that commissioned the algorithm in the first place. The question we should be asking is: as A.I. becomes more powerful and flexible, is there any way to keep it from being another version of McKinsey? The question is worth considering across different meanings of the term “A.I.” If you think of A.I. as a broad set of technologies being marketed to companies to help them cut their costs, the question becomes: how do we keep those technologies from working as “capital’s willing executioners”? Alternatively, if you imagine A.I. as a semi-autonomous software program that solves problems that humans ask it to solve, the question is then: how do we prevent that software from assisting corporations in ways that make people’s lives worse? Suppose you’ve built a semi-autonomous A.I. that’s entirely obedient to humans—one that repeatedly checks to make sure it hasn’t misinterpreted the instructions it has received. This is the dream of many A.I. researchers. Yet such software could easily still cause as much harm as McKinsey has. Note that you cannot simply say that you will build A.I. that only offers pro-social solutions to the problems you ask it to solve. That’s the equivalent of saying that you can defuse the threat of McKinsey by starting a consulting firm that only offers such solutions. The reality is that Fortune 100 companies will hire McKinsey instead of your pro-social firm, because McKinsey’s solutions will increase shareholder value more than your firm’s solutions will. It will always be possible to build A.I. that pursues shareholder value above all else, and most companies will prefer to use that A.I. instead of one constrained by your principles. Is there a way for A.I. to do something other than sharpen the knife blade of capitalism? Just to be clear, when I refer to capitalism, I’m not talking about the exchange of goods or services for prices determined by a market, which is a property of many economic systems. When I refer to capitalism, I’m talking about a specific relationship between capital and labor, in which private individuals who have money are able to profit off the effort of others. So, in the context of this discussion, whenever I criticize capitalism, I’m not criticizing the idea of selling things; I’m criticizing the idea that people who have lots of money get to wield power over people who actually work. And, more specifically, I’m criticizing the ever-growing concentration of wealth among an ever-smaller number of people, which may or may not be an intrinsic property of capitalism but which absolutely characterizes capitalism as it is practiced today. As it is currently deployed, A.I. often amounts to an effort to analyze a task that human beings perform and figure out a way to replace the human being. Coincidentally, this is exactly the type of problem that management wants solved. As a result, A.I. assists capital at the expense of labor. There isn’t really anything like a labor-consulting firm that furthers the interests of workers. Is it possible for A.I. to take on that role? Can A.I. do anything to assist workers instead of management? Some might say that it’s not the job of A.I. to oppose capitalism. That may be true, but it’s not the job of A.I. to strengthen capitalism, either. Yet that is what it currently does. If we cannot come up with ways for A.I. to reduce the concentration of wealth, then I’d say it’s hard to argue that A.I. is a neutral technology, let alone a beneficial one. Many people think that A.I. will create more unemployment, and bring up universal basic income, or U.B.I., as a solution to that problem. In general, I like the idea of universal basic income; however, over time, I’ve become skeptical about the way that people who work in A.I. suggest U.B.I. as a response to A.I.-driven unemployment. It would be different if we already had universal basic income, but we don’t, so expressing support for it seems like a way for the people developing A.I. to pass the buck to the government. In effect, they are intensifying the problems that capitalism creates with the expectation that, when those problems become bad enough, the government will have no choice but to step in. As a strategy for making the world a better place, this seems dubious. You may remember that, in the run-up to the 2016 election, the actress Susan Sarandon—who was a fervent supporter of Bernie Sanders—said that voting for Donald Trump would be better than voting for Hillary Clinton because it would bring about the revolution more quickly. I don’t know how deeply Sarandon had thought this through, but the Slovenian philosopher Slavoj Žižek said the same thing, and I’m pretty sure he had given a lot of thought to the matter. He argued that Trump’s election would be such a shock to the system that it would bring about change. What Žižek advocated for is an example of an idea in political philosophy known as accelerationism. There are a lot of different versions of accelerationism, but the common thread uniting left-wing accelerationists is the notion that the only way to make things better is to make things worse. Accelerationism says that it’s futile to try to oppose or reform capitalism; instead, we have to exacerbate capitalism’s worst tendencies until the entire system breaks down. The only way to move beyond capitalism is to stomp on the gas pedal of neoliberalism until the engine explodes. I suppose this is one way to bring about a better world, but, if it’s the approach that the A.I. industry is adopting, I want to make sure everyone is clear about what they’re working toward. By building A.I. to do jobs previously performed by people, A.I. researchers are increasing the concentration of wealth to such extreme levels that the only way to avoid societal collapse is for the government to step in. Intentionally or not, this is very similar to voting for Trump with the goal of bringing about a better world. And the rise of Trump illustrates the risks of pursuing accelerationism as a strategy: things can get very bad, and stay very bad for a long time, before they get better. In fact, you have no idea of how long it will take for things to get better; all you can be sure of is that there will be significant pain and suffering in the short and medium term. I’m not very convinced by claims that A.I. poses a danger to humanity because it might develop goals of its own and prevent us from turning it off. However, I do think that A.I. is dangerous inasmuch as it increases the power of capitalism. The doomsday scenario is not a manufacturing A.I. transforming the entire planet into paper clips, as one famous thought experiment has imagined. It’s A.I.-supercharged corporations destroying the environment and the working class in their pursuit of shareholder value. Capitalism is the machine that will do whatever it takes to prevent us from turning it off, and the most successful weapon in its arsenal has been its campaign to prevent us from considering any alternatives. People who criticize new technologies are sometimes called Luddites, but it’s helpful to clarify what the Luddites actually wanted. The main thing they were protesting was the fact that their wages were falling at the same time that factory owners’ profits were increasing, along with food prices. They were also protesting unsafe working conditions, the use of child labor, and the sale of shoddy goods that discredited the entire textile industry. The Luddites did not indiscriminately destroy machines; if a machine’s owner paid his workers well, they left it alone. The Luddites were not anti-technology; what they wanted was economic justice. They destroyed machinery as a way to get factory owners’ attention. The fact that the word “Luddite” is now used as an insult, a way of calling someone irrational and ignorant, is a result of a smear campaign by the forces of capital. Whenever anyone accuses anyone else of being a Luddite, it’s worth asking, is the person being accused actually against technology? Or are they in favor of economic justice? And is the person making the accusation actually in favor of improving people’s lives? Or are they just trying to increase the private accumulation of capital? Today, we find ourselves in a situation in which technology has become conflated with capitalism, which has in turn become conflated with the very notion of progress. If you try to criticize capitalism, you are accused of opposing both technology and progress. But what does progress even mean, if it doesn’t include better lives for people who work? What is the point of greater efficiency, if the money being saved isn’t going anywhere except into shareholders’ bank accounts? We should all strive to be Luddites, because we should all be more concerned with economic justice than with increasing the private accumulation of capital. We need to be able to criticize harmful uses of technology—and those include uses that benefit shareholders over workers—without being described as opponents of technology. Imagine an idealized future, a hundred years from now, in which no one is forced to work at any job they dislike, and everyone can spend their time on whatever they find most personally fulfilling. Obviously it’s hard to see how we’d get there from here. But now consider two possible scenarios for the next few decades. In one, management and the forces of capital are even more powerful than they are now. In the other, labor is more powerful than it is now. Which one of these seems more likely to get us closer to that idealized future? And, as it’s currently deployed, which one is A.I. pushing us toward? Of course, there is the argument that new technology improves our standard of living in the long term, which makes up for the unemployment that it creates in the short term. This argument carried weight for much of the post-Industrial Revolution period, but it has lost its force in the past half century. In the United States, per-capita G.D.P. has almost doubled since 1980, while the median household income has lagged far behind. That period covers the information-technology revolution. This means that the economic value created by the personal computer and the Internet has mostly served to increase the wealth of the top one per cent of the top one per cent, instead of raising the standard of living for U.S. citizens as a whole. Of course, we all have the Internet now, and the Internet is amazing. But real-estate prices, college tuition, and health-care costs have all risen faster than inflation. In 1980, it was common to support a family on a single income; now it’s rare. So, how much progress have we really made in the past forty years? Sure, shopping online is fast and easy, and streaming movies at home is cool, but I think a lot of people would willingly trade those conveniences for the ability to own their own homes, send their kids to college without running up lifelong debt, and go to the hospital without falling into bankruptcy. It’s not technology’s fault that the median income hasn’t kept pace with per-capita G.D.P.; it’s mostly the fault of Ronald Reagan and Milton Friedman. But some responsibility also falls on the management policies of C.E.O.s like Jack Welch, who ran General Electric between 1981 and 2001, as well as on consulting firms like McKinsey. I’m not blaming the personal computer for the rise in wealth inequality—I’m just saying that the claim that better technology will necessarily improve people’s standard of living is no longer credible. The fact that personal computers didn’t raise the median income is particularly relevant when thinking about the possible benefits of A.I. It’s often suggested that researchers should focus on ways that A.I. can increase individual workers’ productivity rather than replace them; this is referred to as the augmentation path, as opposed to the automation path. That’s a worthy goal, but, by itself, it won’t improve people’s economic fortunes. The productivity software that ran on personal computers was a perfect example of augmentation rather than automation: word-processing programs replaced typewriters rather than typists, and spreadsheet programs replaced paper spreadsheets rather than accountants. But the increased personal productivity brought about by the personal computer wasn’t matched by an increased standard of living. The only way that technology can boost the standard of living is if there are economic policies in place to distribute the benefits of technology appropriately. We haven’t had those policies for the past forty years, and, unless we get them, there is no reason to think that forthcoming advances in A.I. will raise the median income, even if we’re able to devise ways for it to augment individual workers. A.I. will certainly reduce labor costs and increase profits for corporations, but that is entirely different from improving our standard of living. It would be convenient if we could assume that a utopian future is right around the corner and develop technology for use in that future. But the fact that a given technology would be helpful in a utopia does not imply that it’s helpful now. In a utopia where there’s a machine that converts toxic waste into food, generating toxic waste wouldn’t be a problem, but, in the here and now, no one could claim that generating toxic waste is harmless. Accelerationists might argue that generating more toxic waste will motivate the invention of a waste-to-food converter, but how convincing is that? We evaluate the environmental impact of technologies in the context of the mitigations that are currently available, not in the context of hypothetical future mitigations. By the same token, we can’t evaluate A.I. by imagining how helpful it will be in a world with U.B.I.; we have to evaluate it in light of the existing imbalance between capital and labor, and, in that context, A.I. is a threat because of the way it assists capital. A former partner at McKinsey defended the company’s actions by saying, “We don’t do policy. We do execution.” But this is a pretty thin excuse; harmful policy decisions are more likely to be made when consulting firms—or new technologies—offer ways to implement them. The version of A.I. that’s currently being developed makes it easier for companies to lay people off. So is there any way to develop a kind of A.I. that makes it harder? In his book “How to Be an Anticapitalist in the 21st Century,” the sociologist Erik Olin Wright offers a taxonomy of strategies for responding to the harms of capitalism. Two of the strategies he mentions are smashing capitalism and dismantling capitalism, which probably fall outside the scope of this discussion. The ones that are more relevant here are taming capitalism and resisting capitalism. Roughly speaking, taming capitalism means government regulation, and resisting capitalism means grassroots activism and labor unions. Are there ways for A.I. to strengthen those things? Is there a way for A.I. to empower labor unions or worker-owned coöperatives? In 1976, the workers at the Lucas Aerospace Corporation in Birmingham, England, were facing layoffs because of cuts in defense spending. In response, the shop stewards produced a document known as the Lucas Plan, which described a hundred and fifty “socially useful products,” ranging from dialysis machines to wind turbines and hybrid engines for cars, that the workforce could build with its existing skills and equipment rather than being laid off. The management at Lucas Aerospace rejected the proposal, but it remains a notable modern example of workers trying to steer capitalism in a more human direction. Surely something similar must be possible with modern computing technology. Does capitalism have to be as harmful as it currently is? Maybe not. The three decades following the Second World War are sometimes known as the golden age of capitalism. This period was partially the result of better government policies, but the government didn’t create the golden age on its own: corporate culture was different during this era. In General Electric’s annual report from 1953, the company bragged about how much it paid in taxes and how much it was spending on payroll. It explicitly said that “maximizing employment security is a prime company goal.” The founder of Johnson & Johnson said that the company’s responsibility to its employees was higher than its responsibility to its shareholders. Corporations then had a radically different conception of their role in society compared with corporations today. Is there a way to get back to those values? It seems unlikely, but remember that the golden age of capitalism came after the enormous wealth inequality of the Gilded Age. Right now we’re living in a second Gilded Age, in which wealth inequality is about the same as it was back in 1913, so it’s not impossible that we could go from where we are now to a second golden age. Of course, in between the first Gilded Age and the golden age we had the Great Depression and two World Wars. An accelerationist might say that those events were necessary to bring about the golden age, but I think most of us would prefer to skip over those steps. The task before us is to imagine ways for technology to move us toward a golden age without bringing about another Great Depression first. We all live in a capitalist system, so we are all participants in capitalism whether we like it or not. And it’s reasonable to wonder if there’s anything you as an individual can do. If you work as a food scientist at Frito-Lay and your job is to invent new flavors of potato chip, I’m not going to say that you have an ethical obligation to quit because you’re assisting the engine of consumerism. You’re using your training as a food scientist to provide customers with a pleasant experience; that’s a perfectly reasonable way to make a living. But many of the people who work in A.I. regard it as more important than inventing new flavors of potato chip. They say it’s a world-changing technology. If that’s the case, then they have a duty to find ways for A.I. to make the world better without first making it worse. Can A.I. ameliorate the inequities of our world other than by pushing us to the brink of societal collapse? If A.I. is as powerful a tool as its proponents claim, they should be able to find other uses for it besides intensifying the ruthlessness of capital. If there is any lesson that we should take from stories about genies granting wishes, it’s that the desire to get something without effort is the real problem. Think about the story of “The Sorcerer’s Apprentice,” in which the apprentice casts a spell to make broomsticks carry water but is unable to make them stop. The lesson of that story is not that magic is impossible to control: at the end of the story, the sorcerer comes back and immediately fixes the mess the apprentice made. The lesson is that you can’t get out of doing the hard work. The apprentice wanted to avoid his chores, and looking for a shortcut was what got him into trouble. The tendency to think of A.I. as a magical problem solver is indicative of a desire to avoid the hard work that building a better world requires. That hard work will involve things like addressing wealth inequality and taming capitalism. For technologists, the hardest work of all—the task that they most want to avoid—will be questioning the assumption that more technology is always better, and the belief that they can continue with business as usual and everything will simply work itself out. No one enjoys thinking about their complicity in the injustices of the world, but it is imperative that the people who are building world-shaking technologies engage in this kind of critical self-examination. It’s their willingness to look unflinchingly at their own role in the system that will determine whether A.I. leads to a better world or a worse one. ♦ A security camera for the entire planet. In India, a clean-power plant the size of Manhattan could be a model for the world—or a cautionary tale. Saving the climate will depend on blue-collar workers. Can we train enough of them before time runs out? Cargo ships are among the dirtiest vehicles in existence. Can an ancient technology clean them up? A case for climate optimism, and pragmatism, from John Podesta. A heat shield for the most important ice on Earth. The climate solutions we can’t live without. Sign up for our daily newsletter to receive the best stories from The New Yorker. By signing up, you agree to our User Agreement and Privacy Policy & Cookie Statement. By The New Yorker By James Lasdun By Nathan Heller By Ann Patchett Sections More © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. The New Yorker may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",87,4043,0.351430833339691,0.317802011966705,0.330767184495926
"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. Paresh Dave Computer scientist Pascale Fung can imagine a rosy future in which polyglot AI helpers like ChatGPT bridge language barriers. In that world, Indonesian store owners fluent only in local dialects might reach new shoppers by listing their products online in English. “It can open opportunities,” Fung says—then pauses. She’s spotted the bias in her vision of a more interconnected future: The AI-aided shopping would be one-sided, because few Americans would bother to use AI translation to help research products advertised in Indonesian. “Americans are not incentivized to learn another language,” she says. Not every American fits that description—about one in five speak another language at home—but the dominance of English in global commerce is real. Fung, director of the Center for AI Research at the Hong Kong University of Science and Technology, who herself speaks seven languages, sees this bias in her own field. “If you don’t publish papers in English, you’re not relevant,” she says. “Non-English speakers tend to be punished professionally.” Fung would like to see AI change that, not further reinforce the primacy of English. She’s part of a global community of AI researchers testing the language skills of ChatGPT and its rival chatbots and sounding the alarm about evidence that they are significantly less capable in languages other than English. Although researchers have identified some potential fixes, the mostly English-spewing chatbots spread. “One of my biggest concerns is we’re going to exacerbate the bias for English and English speakers,” says Thien Huu Nguyen, a University of Oregon computer scientist who’s also been on the case against skewed chatbots. “People are going to follow the norm and not think about their own identities or culture. It kills diversity. It kills innovation.” At least 15 research papers posted this year on the preprint server arXiv.org, including studies co-authored by Nguyen and Fung, have probed the multilingualism of large language models, the breed of AI software powering experiences such as ChatGPT. The methodologies vary, but their findings fall in line: The AI systems are good at translating other languages into English, but they struggle with rewriting English into other languages—especially those, like Korean, with non-Latin scripts. Despite much recent talk of AI becoming superhuman, ChatGPT-like systems also struggle tofluently mix languages in the same utterance—say English and Tamil—as billions of people in the world casually do each day. Nguyen’s study reports that tests on ChatGPT in March showed it performed substantially worse at answering factual questions or summarizing complex text in non-English languages and was more likely to fabricate information. “This is an English sentence, so there is no way to translate it to Vietnamese,” the bot responded inaccurately to one query. Despite the technology’s limitations, workers around the world are turning to chatbots for help crafting business ideas, drafting corporate emails, and perfecting software code. If the tools continue to work the best in English, they could increase the pressure to learn the language on people hoping to earn a spot in the global economy. That could further a spiral of imposition and influence of English that began with the British Empire. Lauren Goode WIRED Staff Will Knight Jennifer M. Wood Not only AI scholars are worried. At a US congressional hearing this month, Senator Alex Padilla of California asked Sam Altman, CEO of ChatGPT’s creator, OpenAI, which is based in the state, what his company is doing to close the language gap. About 44 percent of Californians speak a language other than English. Altman said he hoped to partner with governments and other organizations to acquire data sets that would bolster ChatGPT’s’s language skills and broaden its benefits to “as wide of a group as possible.” Padilla, who also speaks Spanish, is skeptical about the systems delivering equitable linguistic outcomes without big shifts in strategies by their developers. “These new technologies hold great promise for access to information, education, and enhanced communication, and we must ensure that language doesn’t become a barrier to these benefits,” he says. OpenAI hasn’t hid the fact its systems are biased. The company’s report card on GPT-4, its most advanced language model, which is available to paying users of ChatGPT, states that the majority of the underlying data came from English and that the company’s efforts to fine-tune and study the performance of the model primarily focused on English “with a US-centric point of view.” Or as a staff member wrote last December on the company’s support forum, after a user asked if OpenAI would add Spanish support to ChatGPT, “Any good Spanish results are a bonus.” OpenAI declined to comment for this story. Jessica Forde, a computer science doctoral student at Brown University has criticized OpenAI for not thoroughly evaluating GPT-4’s capabilities in other languages before releasing it. She’s among the researchers who would like companies to publicly explain their training data and track their progress on multilingual support. “English has been so cemented because people have been saying (and studying), can this perform like a lawyer in English or a doctor in English? Can this produce a comedy in English? But they aren’t asking the same about other languages,” she says.  Large language models work with words using statistical patterns learned from billions of words of text grabbed from the internet, books, and other resources. More of those available materials are in English and Chinese than in other languages, due to US economic dominance and China’s huge population.  Because text data sets also have some other languages mixed in, the models do pick up capability in other languages. Their knowledge just isn’t necessarily comprehensive. As researchers at the Center for Democracy and Technology in Washington, DC, explained in a paper this month, because of the dominance of English, “a multilingual model might associate the word dove in all languages with peace even though the Basque word for dove (‘uso’) can be an insult.” Aleyda Solis encountered that weakness when she tried Microsoft’s Bing chat, a search tool that relies on GPT-4. The Bing bot provided her the appropriate colloquial term for sneakers in several English-speaking countries (“trainers” in the UK, “joggers” in parts of Australia) but failed to provide regionally appropriate terms when asked in Spanish for the local footwear lingo across Latin America (“Zapatillas deportivas” for Spain, “championes” for Uruguay).  Lauren Goode WIRED Staff Will Knight Jennifer M. Wood In a separate dialog, when queried in English, Bing chat correctly identified Thailand as the rumored location for the next setting of the TV show White Lotus, but provided “somewhere in Asia” when the query was translated to Spanish, says Solis, who runs a consultancy called Orainti that helps websites increase visits from search engines. Executives at Microsoft, OpenAI, and Google working on chatbots have said users can counteract poor responses by adding more detailed instructions to their queries. Without explicit guidance, chatbots’ bias to fall back on English speech and English-speaking perspectives can be strong. Just ask Veruska Anconitano, another search engine optimization expert, who splits her time between Italy and Ireland. She found asking Bing chat questions in Italian drew answers in English unless she specified “Answer me in Italian.” In different chat, Anconitano says, Bing assumed she wanted the Japanese prompt 元気ですか (“How are you?”) rendered into English rather than continuing the conversation in Japanese. Recent research papers have validated the anecdotal findings of people running into the limits of Bing chat and its brethren. Zheng-Xin Yong, a doctoral student at Brown University also studying multilingual language models, says he and his collaborators found in one study that generating better answers for Chinese questions required asking them in English, rather than Chinese. When Fung at Hong Kong and her collaborators tried asking ChatGPT to translate 30 sentences, it correctly rendered 28 from Indonesian into English, but only 19 in the other direction, suggesting that monoglot Americans who turn to the bot to make deals with Indonesian merchants would struggle. The same limited, one-way fluency was found to repeat across at least five other languages. Large language models’ language problems make them difficult to trust for anyone venturing past English, and maybe Chinese. When I sought to translate ancient Sanskrit hymns through ChatGPT as part of an experiment in using AI to accelerate wedding planning, the results seemed plausible enough to add into a ceremony script. But I had no idea whether I could rely on them or would be laughed off the stage by elders. Researchers who spoke to WIRED do see some signs of improvement. When Google created its PaLM 2 language model, released this month, it made an effort to increase the non-English training data for over 100 languages. The model recognizes idioms in German and Swahili, jokes in Japanese, and cleans up grammar in Indonesian, Google says, and it recognizes regional variations better than prior models. But in consumer services, Google is keeping PaLM 2 caged. Its chatbot Bard is powered by PaLM 2 but only works in US English, Japanese, and Korean. A writing assistant for Gmail that uses PaLM 2 only supports English. It takes time to officially support a language by conducting testing and applying filters to ensure the system isn’t generating toxic content. Google did not make an all-out investment to launch many languages from the beginning, though it’s working to rapidly add more. Lauren Goode WIRED Staff Will Knight Jennifer M. Wood As well as calling out the failings of language models, researchers are creating new data sets of non-English text to try to accelerate the development of truly multilingual models. Fung’s group is curating Indonesian-language data for training models, while Yong’s multi-university team is doing the same for Southeast Asian languages. They’re following the path of groups targeting African languages and Latin American dialects. “We want to think about our relationship with Big Tech as collaborative rather than adversarial,” says Skyler Wang, a sociologist of technology and AI at UC Berkeley who is collaborating with Yong. “There are a lot of resources that can be shared.” But collecting more data is unlikely to be enough, because the reams of English text are so large—and still growing. Though it carries the risk of eliminating cultural nuances, some researchers believe companies will have to generate synthetic data—for example, by using intermediary languages such as Mandarin or English to bridge translations between languages with limited training materials. “If we start from scratch, we will never have enough data in other languages,” says Nguyen at the University of Oregon. “If you want to ask about a scientific issue, you do it in English. Same thing in finance.” Nguyen would also like to see AI developers be more attentive to what data sets they feed into their models and how it affects each step in the building process, not just the ultimate responses. So far, what languages have ended up in models has been a “random process,” Nguyen says. More rigorous controls to reach certain thresholds of content for each language—as Google tried to do with PaLM—could boost the quality of non-English output.  Fung has given up on using ChatGPT and other tools born out of large language models for any purpose beyond research. Their speech too often comes off as boring to her. Due to the underlying technology’s design, the chatbots’ utterances are “the average of what’s on the internet,” she says—a calculation that works best in English, and leaves responses in other tongues lacking spice. 📩 Don’t miss our biggest stories, delivered to your inbox every day 🎧 Our new podcast wants you to Have a Nice Future The trillion-dollar auction to save the world The history of Russia’s most ingenious hacker group We put Google’s new AI writing assistant to the test How NASA plans to melt the moon—and build on Mars How can companies recycle clothes back into clothes? 🔊 Our Gear team sounds off on audiophile-grade speakers, vinyl accessories and the best wireless headphones for anyone David Nield Will Knight David Nield Matt Burgess Charles Platt Will Knight Roy Furchgott Will Knight More From WIRED Contact © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",14,2066,0.362694025039673,0.312040120363235,0.325265884399414
"To revist this article, visit My Profile, then View saved stories. To revist this article, visit My Profile, then View saved stories. Will Knight This week, at its annual I/O developer conference in Mountain View, Google showcased a head-spinning number of projects and products powered by or enhanced by AI. They included a new-and-improved version of its chatbot Bard, tools to help you write emails and documents or manipulate images, devices with AI baked in, and a chatbot-like experimental version of Google search. For a full recap of the event, complete with insightful and witty commentary from my WIRED colleagues, check out our Google I/O liveblog.  Google’s big pivot is, of course, largely fueled not by algorithms but by generative AI FOMO.  The appearance last November of ChatGPT—the remarkably clever but still rather flawed chatbot from OpenAI—combined with Microsoft adding the technology to its search engine Bing a few months later, triggered something of a panic at Google. ChatGPT proved wildly popular with users, demonstrating new ways to serve up information that threatened Google’s vice grip on the search business and its reputation as the leader in AI. The capabilities of ChatGPT and AI language algorithms like those powering it are so striking that some experts, including Geoffrey Hinton, a pioneering researcher who recently left Google, have felt compelled to warn that we might be building systems that we will someday struggle to control. OpenAI’s chatbot is often astonishingly good at generating coherent text on a given subject, summarizing information from the web, and even answering extremely tricky questions that require expert knowledge. And yet, unfettered AI language models are also silver-tongued agents of chaos. They will gladly fabricate facts, express unpleasant biases, and say unpleasant or disturbing things with the right prompting. Microsoft was forced to limit the capabilities of Bing chat shortly after launch to avoid such embarrassing misbehavior, in part because its bot divulged its secret codename—Sydney—and accused a New York Times columnist of not loving his spouse. Google worked hard to tone down the chaotic streak of text-generation technology as it prepared the experimental search feature announced yesterday that responds to search queries with chat-style answers synthesizing information from across the web. Google’s smarter version of search is impressively narrow-minded, refusing to use the first person or talk about its thoughts or feelings. It completely avoids topics that might be considered risky, refusing to dispense medical advice or offer answers on potentially controversial topics such as US politics. Lauren Goode WIRED Staff Will Knight Jennifer M. Wood Google deserves recognition for reining in generative chatbots’ wild side like that. But in my tests, the new search interface felt incredibly tame compared to ChatGPT or Google’s own chatbot Bard. As the company moves the technology into more of its products, perhaps the generative AI revolution will turn out to be a lot less fun than you might expect from the early shock and awe of ChatGPT, a chatbot that has an edgy charm. Gone are the wild ravings and imaginings of powerful AI bots. In their place are new ways to populate spreadsheets, compose email pleasantries, and find products to buy. Even if “AI doomers” warning about errant AI prove overblown, it will be interesting to watch how companies like Google and OpenAI balance the development of more powerful generative language models with the need to have them behave. Google has invested huge sums and major resources in AI over recent years, with CEO Sundar Pichai often pitching the company as “AI first”, and the company is desperate to show it can advance the technology more quickly than OpenAI. One high-level message from Google’s stream of AI announcements was that the company is not going to hold back anymore, as it did the LaMDA chatbot that was announced long before ChatGPT appeared but not made public. In March, some big names in AI research signed an open letter calling for a six-month pause on creating machine learning systems more powerful than GPT-4, which powers ChatGPT. Pichai was not a signatory and said in his keynote speech yesterday that the company is currently training a new, more powerful language model called Gemini. A source at Google tells me this new system will incorporate a range of recent advances from different large language models and may eclipse GPT-4. But don’t expect to get to experience the full power or charisma Gemini can offer. If Google applies the same chaos-taming methods seen in its chat-like search experiment, it may just seem like another surprisingly clever autocomplete. 📨 Understand AI advances with our Fast Forward newsletter 🎧 Our new podcast wants you to Have a Nice Future Doug Rushkoff is ready to renounce the digital revolution This is catfishing on an industrial scale The post office is spying on mail. Senators want to stop it Waluigi, Carl Jung, and the case for moral AI Tears of the Kingdom’s creators answer your questions 📷 Snap into spring with the Gear team’s picks for the best camera bags, fun instant cameras, and mirrorless cameras Lauren Goode Matt Burgess Caitlin Harrington Will Knight Khari Johnson Will Knight Khari Johnson Morgan Meaker More From WIRED Contact © 2023 Condé Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Condé Nast. Ad Choices",75,922,0.268611341714859,0.300861805677414,0.430526793003082
"TV & Videos Watch On Twitter Facebook Sign up for our newsletters The Airdrop Your weekly wrap of Web3 news and trends. First Mover The latest moves in crypto markets, in context. The Node The biggest crypto news and ideas of the day. State of Crypto Probing the intersection of crypto and government. Crypto Investing Course Be a smarter, safer investor in eight weeks. The Protocol Exploring the tech behind crypto. Crypto Long & Short News and analysis for the professional investor. Crypto for Advisors What financial advisors need to know about crypto. Money Reimagined The transformation of value in the digital age. By signing up, you will receive emails about CoinDesk products and you agree to ourterms & conditionsandprivacy policy. Podcasts Consensus Indices Markets Finance Policy Technology Web3 Learn Crash Courses Consensus Magazine Most Recent Issue Sponsored Content GNT V3 (Find Satoshi Lab) Find Satoshi Lab (FSL), the parent company behind popular Web3 game STEPN, is rolling out an artificial intelligence (AI) powered non-fungible token (NFT) generator. Called GNT V3, the tool aims to place creativity within the technological intersections of AI and Web3. Working in collaboration with MOOAR, FSL’s recently-released NFT marketplace, GNT V3 takes a user’s selfie as input to create AI-generated images that are minted on the Solana blockchain. Yawn Rong, co-founder of FSL, said in a press release that GNT V3 has the capability to redefine digital identity in Web3. “This is the advent of a new era of Web3 technology, where individuality merges with blockchain technology to redefine personal expression,” said Rong. “With GNT, we're pushing the boundaries of digital self-expression and enabling users to become creators of their own unique digital masterpieces, ready to be showcased and monetized in the vibrant Web3 ecosystem.” Last month, FSL released GNT V1, which allowed artists to mint individual NFTs on the MOOAR marketplace. Later in April, it rolled out GNT V2, which improved its AI model. “The world is discovering the pros and cons of AI technology, but we have been working for months to do something novel with the tech,” said Jerry Huang, FSL co-founder, in a press release. In November, FSL released MOOAR, providing STEPN users with a native marketplace to trade their NFT sneakers that are necessary for gameplay. DISCLOSURE Please note that our privacy policy, terms of use, cookies, and do not sell my personal information has been updated.The leader in news and information on cryptocurrency, digital assets and the future of money, CoinDesk is a media outlet that strives for the highest journalistic standards and abides by a strict set of editorial policies. CoinDesk is an independent operating subsidiary of Digital Currency Group, which invests in cryptocurrencies and blockchain startups. As part of their compensation, certain CoinDesk employees, including editorial employees, may receive exposure to DCG equity in the form of stock appreciation rights, which vest over a multi-year period. CoinDesk journalists are not allowed to purchase stock outright in DCG. Cam Thompson is a news reporter at CoinDesk. Learn more about Consensus 2024, CoinDesk’s longest-running and most influential event that brings together all sides of crypto, blockchain and Web3. Head to consensus.coindesk.com to register and buy your pass now. About Stay Updated Get In Touch The Fine Print Please note that our privacy policy, terms of use, cookies, and do not sell my personal information has been updated.The leader in news and information on cryptocurrency, digital assets and the future of money, CoinDesk is a media outlet that strives for the highest journalistic standards and abides by a strict set of editorial policies. CoinDesk is an independent operating subsidiary of Digital Currency Group, which invests in cryptocurrencies and blockchain startups. As part of their compensation, certain CoinDesk employees, including editorial employees, may receive exposure to DCG equity in the form of stock appreciation rights, which vest over a multi-year period. CoinDesk journalists are not allowed to purchase stock outright in DCG. ©2023 CoinDesk",58,653,0.31630340218544,0.314241200685501,0.369455367326736
"Warning follows open letter earlier this year that called for a six-month pause on AI development. Taipei, Taiwan – Artificial intelligence poses a “risk of extinction” that calls for global action, leading computer scientists and technologists have warned. “Mitigating the risk of extinction from AI should be a global priority alongside other societal scale risks such as pandemics and nuclear war,” a group of AI experts and other high-profile figures said in a brief statement released by the Center for AI Safety, a San Francisco-based research and advocacy group, on Tuesday. The signatories include technology experts such as Sam Altman, chief executive of OpenAI, Geoffrey Hinton, known as the “godfather of AI”, and Audrey Tang, Taiwan’s digital minister, as well as other notable figures including the neuroscientist Sam Harris and the musician Grimes. The warning follows an open letter signed by Elon Musk and other high-profile figures in March that called for a six-month pause on the development of AI more advanced than OpenAI’s GPT-4. “Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable,” the letter said. The rapid advancement of AI has raised concerns about potential negative consequences for society ranging from mass job losses and copyright infringement to the spread of misinformation and political instability. Some experts have raised fears that humanity could one day lose control of the technology. While current AI has yet to achieve artificial general intelligence (AGI), potentially allowing it to make independent decisions, researchers at Microsoft in March said that GPT-4 showed “sparks of AGI” and was capable of solving “novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting”. Since then, warnings about the potential dangers of AI have grown. Last month, Hinton, a renowned computer scientist, quit his job at Google so he could spend more time advocating about the risks of AI. In an appearance before the United States Congress earlier this month, Altman called on legislators to quickly develop regulations for AI technology and recommended a licensing-based approach. The US and other countries are scrambling to come up with legislation that balances the need for oversight with promising technology. The European Union has said it hopes to pass legislation by the end of the year that would classify AI into four risk-based categories. China has also taken steps to regulate AI, passing legislation governing deep fakes and requiring companies to register their algorithms with regulators. Beijing has also proposed strict rules to restrict politically-sensitive content and require developers to receive approval before releasing generative AI-based tech.          Follow Al Jazeera English:",15,452,0.351312041282654,0.308996766805649,0.339691162109375
"Watsonx.ai brings together new generative AI capabilities, powered by foundation models, and traditional machine learning into a powerful platform spanning the AI lifecycle. With watsonx.ai, you can train, validate, tune, and deploy models with ease and build AI applications in a fraction of the time with a fraction of the data. General availability of watsonx.ai is expected in July. Use open-source frameworks and tools for code-based, automated, and visual data science capabilities – all in a secure, trusted studio environment. Leverage foundation models and generative AI with minimal data, advanced prompt-tuning capabilities, full SDK and API libraries. Accelerate the full AI model lifecycle with all the tools and runtimes in one place to train, validate, tune and deploy AI models. With just a few lines of instruction you can draft job descriptions, classify customer complaints, summarize complex regulatory documents, extract key business information, and much more. Quickly tune models for your specific business needs using the latest open source and IBM trained foundation models. Work with IBM’s suite of curated foundation models trained to ensure model trust and efficiency in business applications, or you can bring your own models to further train and tune. You can experiment with open source models through IBM’s partnership with Hugging Face, allowing you to define the best models for your needs.  Work with IBM’s suite of foundation models designed to ensure model trust and efficiency in business applications. Through IBM’s partnership with Hugging Face, you can experiment with open source models, allowing you to define the best models for your needs. You can use watsonx.ai to train and tune your own models using our advanced prompt-tuning capabilities, full SDK and API libraries. Discover our next generation platform to scale and accelerate AI. See how our data store combines flexibility of a data lake with the performance of a warehouse. Build trust with our new toolkit for responsible, transparent and explainable AI. Be among the first to try the watsonx.ai next-generation studio for AI builders. General availability is expected in July.",89,332,0.229395344853401,0.272704392671585,0.497900307178497
"When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. AI for the masses.  I visited AMD's office here in Taipei, Taiwan, during Computex 2023 to have a conversation with David McAfee, the company's Corporate VP and General Manager of the Client Channel Business. I also had a chance to see AMD's Ryzen XDNA AI engine at work in a laptop demo, and McAfee discussed the steps AMD is taking to prepare the operating system and software ecosystem for the burgeoning AI use cases that will run locally on the PC, which we'll dive into further below.
After following the AMD codename-inspired hallway map, I found my way to the demo room to see AMD's latest tech in action.   AMD's demo laptop was an Asus Strix Scar 17 that comes powered by AMD's 4nm 'Phoenix' Ryzen 9 7940HS processor paired with Radeon 780M graphics. These 35-45W chips come with the Zen 4 architecture and RDNA 3 graphics. AMD also had an Asus ROG Zephyrus G14 running the same demo.  The XDNA AI engine is a dedicated accelerator that resides on-die with the CPU cores. The goal for the XDNA AI engine is to execute lower-intensity AI inference workloads, like audio, photo, and video processing, at lower power than you could achieve on a CPU or GPU while delivering faster response times than online services, thus boosting performance and saving battery power. First, I popped open the task manager to see if the AI engine would enumerate itself as visible cores with utilization metrics, but the XDNA AI engine doesn't show up as a visible device. As you can see in the above album, I found the AI engine listed as the 'AMD IPU Device' in the device manager. However, we couldn't observe the load or other telemetry from the cores during the tests. Here we can see the XDNA AI engine crunching away at a facial recognition workload. To the right of the screen, we can see a measurement of the latency for each step of the workload. The bars are impressively low, and the workload ran quickly through a series of images as the AI engine crunched through the inference workload, but we don't have any context on just how those figures compare to other types of solutions.  AMD's demo did have a button to test its onboard AI engine against the online Azure ONNX EP service, but the demo team told us they had encountered issues with the software, so it wasn't working. Naturally, we would expect the in-built Ryzen AI engine to have lower latency than the Azure service, and that is logically what AMD was trying to demonstrate. Unfortunately, we were left without a substantiative comparison point for the benchmark results.  However, the benchmark does show that AI is alive and breathing on AMD's Ryzen 7040 processors, and the company is also well underway in bolstering the number of applications that can leverage its AI engine.  This engine can handle up to 4 concurrent AI streams, though it can be rapidly reconfigured to handle varying amounts of streams. AMD claims this engine, a progeny of its Xilinx IP, is faster than the neural engine present on Apple's M2 processors. The engine is plumbed directly into the chips' memory subsystem, so it shares a pool of coherent memory with the CPU and integrated GPU.AMD announced last week at Microsoft's Build conference that it had created a new set of developer tools that leverage the open-source Vitis AI Execution Provider (EP), which is then upstreamed in ONNX runtime, to ease the work required to add software support for the XDNA AI engine. McAfee explained that the Vitis AI EP serves as a sort of bare metal translation layer that allows developers to run models without having to alter the base model. That simplifies integration. AMD isn't providing performance metrics for its AI engine yet, but McAfee noted that it's hard to quantify the advantages of an onboard AI engine with just one performance metric, like TOPS, as higher power efficiency and lower latency are all parts of the multi-faceted advantages of having an AI engine. AMD will share figures in the future, though.
McAfee reiterated AMD's plans to continue to execute its XDNA AI roadmap, eventually adding the engine to other Ryzen processors in the future. However, the software ecosystem for AI on the PC is still in its early days, and AMD will continue to explore the tradeoffs versus the real-world advantages.
Much of the advantage of having an inbuilt AI engine resides in power efficiency, a must in power-constrained devices like laptops, but that might not be as meaningful in an unconstrained desktop PC that can use a more powerful dedicated GPU or CPU for inference workloads -- but without the battery life concerns. I asked McAfee if those factors could impact AMD's decision on whether or not it would bring XDNA to desktop PCs, and he responded that it will boil down to whether or not the feature delivers enough value that it would make sense to dedicate valuable die area to the engine. AMD is still evaluating the impact, particularly as Ryzen 7040 works its way into the market.
For now, AMD isn't confirming any of its future plans, but McAfee said that while AMD is committed to the AI engine being a part of its future roadmaps, it might not come to all products. On that note, he said there could conceivably be other options for different types of chips, like desktop PCs, that leverage AMD's chiplet strategy. Other options, like add-in cards, are also possible solutions.
One thing is for sure: We'll continue to see the scalable integrated XDNA AI engine make an appearance in many of AMD's products in the future. Hopefully, next time we'll see a better demo, too.  Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Paul Alcorn is the Deputy Managing Editor for Tom's Hardware US. He writes news and reviews on CPUs, storage and enterprise hardware. SK Hynix Preps HBM3E Memory: A 25% Speed Boost Over HBM3 Nvidia CEO Says Intel's Test Chip Results For Next-Gen Process Are Good Logitech MX Anywhere 3S Mouse Review: Plenty of Portable Productivity By Brandon HillMay 30, 2023 By Aaron KlotzMay 30, 2023 By Avram PiltchMay 30, 2023 By Zhiye LiuMay 30, 2023 By Andrew E. FreedmanMay 30, 2023 By Paul Alcorn, Avram PiltchMay 30, 2023 By Mark TysonMay 30, 2023 By Brandon HillMay 30, 2023 By Stewart BendleMay 30, 2023 By Ash HillMay 30, 2023 By Paul AlcornMay 30, 2023 Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. ©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036. ",53,1160,0.333966493606567,0.321905940771103,0.344127535820007
"When you purchase through links on our site, we may earn an affiliate commission. Here’s how it works. Say cheese! Receive AI approximation of cheese description. When you think of a Raspberry Pi camera, you probably picture a Pi connected to the Global Shutter Camera, Camera Module 3 or even the HQ camera. You might even think of Otto a GIF creation camera from 2015. But this Raspberry Pi camera project, created by Bjørn Karmann, has no lens at all. In fact, Paragraphica, as he calls it, uses AI to generate an image based on your location data as well as a few other descriptors like date and weather. The device looks somewhat like a classic camera. It has a huge cover over where a lens would be that resembles the nose of a star-nosed mole. The viewfinder is a screen in which you can adjust the settings for the generation of your picture. It also has a series of dials that can be adjusted to set things like film grain and sharpness for the AI program to integrate into the photo generation process. Pressing the capture button will trigger the system to generate a photograph for you based on the selected parameters. Data used in the image generation is gathered using various open APIs. This includes things like your location, date, time, as well as nearby landmarks. Paragraphica uses all of this information to process the final photo using text-to-image AI. To power the project, Karmann has opted to use a Raspberry Pi 4. It’s housed inside of a custom 3D-printed shell along with all of the other electronic components. The viewfinder display doubles as a touchscreen so it can be used for some control input. Location is determined by a GPS module. The software for Paragraphica was written by Karmann just for the project. According to Karmann, it uses both Python as well as the Stable Diffusion API to create the final product. Everything works together using Noodl, a visual tool to develop web apps. A diagram is available on his website showing a breakdown of how the software flows. If you want to recreate this Raspberry Pi project or maybe just get a closer look at how it works, check out the original blog post shared by Karmann breaking down the build process. You can also test it yourself in browser at his website. Join the experts who read Tom's Hardware for the inside track on enthusiast PC tech news — and have for over 25 years. We'll send breaking news and in-depth reviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox. Ash Hill is a Freelance News and Features Writer at Tom's Hardware US. She manages the Pi projects of the month and much of our daily Raspberry Pi reporting. This Raspberry Pi Pico Macropad is a Mouse Controlled by Buttons Raspberry Pi Malware Infects Using Default Username and Password Corsair iCue Link Simplifies PC Building With Tidier Cabling, Chained Components By Ash HillMay 30, 2023 By Paul AlcornMay 30, 2023 By Anton ShilovMay 30, 2023 By Mark TysonMay 30, 2023 By Ash HillMay 30, 2023 By Brandon HillMay 30, 2023 By Andrew E. FreedmanMay 30, 2023 By Les PounderMay 30, 2023 By Paul AlcornMay 30, 2023 By Paul AlcornMay 30, 2023 By Avram PiltchMay 30, 2023 Tom's Hardware is part of Future US Inc, an international media group and leading digital publisher. Visit our corporate site. ©
Future US, Inc. Full 7th Floor, 130 West 42nd Street,
New York,
NY 10036. ",13,587,0.334789633750916,0.324834167957306,0.340376198291779
"Work Trend Index Annual Report The pace of work is outpacing our ability to keep up. AI is poised to create a whole new way of working. May 9, 2023 Illustration by Jon Han with inspiration from Bing Image Creator Read more about the process here. TThe platform shift to AI is underwayhe platform shift to AI is underway and will completely transform the way people work. And for many, the fix can't come soon enough. The pace of work has increased exponentially—along with the crush of data, information, and always-on communications. People are struggling to shoulder the weight of it all, while business leaders feel pressure to increase productivity amid economic uncertainty. We spend more and more of our days separating the signal from the noise—at the expense of creativity. And the tax on individual productivity is compounding, undermining organizational productivity and global GDP.AI can help lift the burden. To date, AI has mostly been on autopilot. Now, next-generation copilots will work alongside people, freeing us from digital debt and fueling innovation. Organizations that embrace AI will unleash creativity and unlock productivity for everyone—ushering in a new wave of productivity growth and value creation.
 “This new generation of AI will remove the drudgery of work and unleash creativity,” said Satya Nadella, Chairman and CEO, Microsoft. “There's an enormous opportunity for AI-powered tools to help alleviate digital debt, build AI aptitude, and empower employees.”
To ready leaders and businesses for the age of AI, we surveyed 31,000 people in 31 countries and analyzed trillions of Microsoft 365 productivity signals, along with labor trends from the LinkedIn Economic Graph. The data points to three urgent insights business leaders must know as they look to quickly and responsibly adopt AI. Download the executive summary For a PDF for the full report, see below. 
  1. Digital debt is costing us innovation We’re all carrying digital debt: the inflow of data, emails, meetings, and notifications has outpaced humans’ ability to process it all. And the pace of work is only intensifying. Everything feels important, so we spend our workdays trying to get out of the red. Nearly 2 in 3 people (64%) say they struggle with having the time and energy to do their job—and those people are 3.5x more likely to also struggle with innovation and strategic thinking. And nearly 2 in 3 leaders (60%) are already feeling the effects, saying that a lack of innovation or breakthrough ideas on their teams is a concern. There are only so many minutes in the day—and every minute we spend managing this digital debt is a minute not spent on the creative work that leads to innovation. In a world where creativity is the new productivity, digital debt is more than an inconvenience—it’s impacting business. 68% of people say they don’t have enough uninterrupted focus time during the workday. Hours worked, workday span, and time spent in meetings have steadily increased over the past few years—it’s easier than ever to communicate and harder than ever to keep up. Taking a closer look at how people spend their time, it’s clear that a lack of focus time, the search for information, and the volume of constant communications have an opportunity cost. Sixty-eight percent of people say they don’t have enough uninterrupted focus time during the workday. And 62% of survey respondents say they struggle with too much time spent searching for information in their workday. Across the Microsoft 365 apps, the average employee spends 57% of their time communicating (in meetings, email, and chat) and 43% creating (in documents, spreadsheets, and presentations).1 The heaviest email users (top 25%) spend 8.8 hours a week on email, and the heaviest meeting users (top 25%) spend 7.5 hours a week in meetings.2 And this global average includes frontline workers— for knowledge workers, who rely even more on digital communication, the share of the week taken up by emails and meetings is even greater. AI can tip the balance in people’s favor to reclaim time and energy for the important work that fuels innovation. Take meetings, for example. People report that the number one productivity disruptor is inefficient meetings, followed closely by having too many meetings at number three. Most people say it’s difficult to brainstorm in a virtual meeting (58%) or catch up if they joined a meeting late (57%), that next steps at the end of a meeting are unclear (55%), and that it’s hard to summarize what happens (56%). And since February 2020, people are in 3x more Teams meetings and calls per week (192%). The data shows a clear need to make meetings better. Today, only 1 in 3 people (35%) think they would be missed in the majority of their meetings. But meeting FOMO is real. Asked what makes meetings worthwhile, people’s top motivation was, “I will receive information that will help me do my job better”—ahead of giving feedback, making decisions, or advancing their career. With AI, every meeting becomes a digital artifact. When meetings are more than a point in time, you can engage and interact with them when and how it works best for you, whether synchronously or asynchronously.  Take action: Identify and address your organization’s productivity disruptors with insights from employee listening. Radically rethink the workday. As AI frees up time and energy, protect focus time for the creative work that leads to innovation. Think of meetings as a digital artifact and not just a point in time. Encourage people to leverage AI-powered intelligent meeting recaps, transcripts, and recordings to engage with meetings how and when it works best for them.   2. There’s a new AI-employee alliance Amid concerns of AI replacing jobs, the data revealed an unexpected insight: employees are more eager for AI to lift the weight of work than they are afraid of job loss to AI. While 49% of people say they’re worried AI will replace their jobs, even more—70%—would delegate as much work as possible to AI to lessen their workloads. “It’s fascinating that people are more excited about AI rescuing them from burnout than they are worried about it eliminating their jobs,” said author and organizational psychology professor Adam Grant. And it turns out people are looking for AI to help in almost every aspect of their work. Not only did 3 in 4 people tell us they would be comfortable using AI for administrative tasks (76%), but most people also said they would be comfortable using it for analytical (79%) and even creative work (73%). People are also looking for AI to assist with finding the right information and answers they need (86%), summarizing their meetings and action items (80%), and planning their day (77%).  “It’s fascinating that people are more excited about AI rescuing them from burnout than they are worried about it eliminating their jobs.” —Adam Grant, author and organizational psychology professor And the AI optimism doesn’t stop there. People also believe it can enhance creativity, from formulating ideas for their work (76%) to editing their work (75%). The more people understand AI, the more they see its promise to help with the most meaningful parts of their jobs. For example, 87% of workers in creative roles3 who are extremely familiar with AI said they’d be comfortable using AI for creative aspects of their job. The data shows that business leaders are looking to empower people with AI rather than replace them—they’re 2x more interested in using AI to increase productivity than to cut headcount. In fact, reducing headcount was last on the list of what leaders would value from AI. After “increasing productivity,” leaders’ top hopes for AI are to: help employees with necessary but repetitive tasks, increase employee wellbeing, eliminate employee time spent on low-value activities, enhance employees’ capabilities, and accelerate employees’ pace of work. We also asked employees and managers to envision how work could change by 2030. Their answers paint a bright future—fueled by AI. When asked what changes they value most, people imagined producing high-quality work in half the time (33%), being able to understand the most valuable ways to spend their time (26%) and energy (25%), and never having to mentally absorb unnecessary or irrelevant information again (23%). And with AI poised to remake work, the future will arrive in months not years. Take action: Bring leaders together across the organization to create guardrails that help people experiment safely and responsibly with AI. 
Be intentional and programmatic. Like any platform shift, adopting AI at scale requires change management. Pick specific disciplines, processes, and workflows to test and learn, and identify evangelists to lead the charge.  
As you begin to adopt AI, deploy it where people need the most relief based on your organization’s pain points and challenges.  3. Every employee needs AI aptitude The paradigm shift to AI as copilot requires a whole new way of working—and a new AI aptitude. Working alongside AI—using natural language—will be as inherent to how we work as the internet and the PC. Skills like critical thinking and analytical judgment, complex problem solving, and creativity and originality are new core competencies—and not just for technical roles or AI experts. Leaders we surveyed said it’s essential that employees learn when to leverage AI, how to write great prompts, how to evaluate creative work, and how to check for bias. As AI reshapes work, human-AI collaboration will be the next transformational work pattern—and the ability to work iteratively with AI will be a key skill for every employee.   Learning isn’t keeping up with the pace of work. Already, 60% of people say they don’t currently have the right capabilities to get their work done. AI will open new paths for learning, and success depends on leaders equipping employees for an AI-powered future. 82% of leaders say their employees will need new skills to be prepared for the growth of AI. And they’ll need to start building those new skills today: there are 33x as many LinkedIn posts mentioning topics like generative AI and GPT than there were one year ago. “We’re in the next phase of change with the introduction of generative AI, and it’s already starting to reshape the labor market,” said Karin Kimbrough, chief economist at LinkedIn. “While it’s still early days, this shift will expand opportunities, create new roles, and augment productivity.” In fact, as of March 2023, the share of US job postings on LinkedIn mentioning GPT are already up 79% year-over-year. And 82% of leaders in our survey say their employees will need new skills to be prepared for the growth of AI. Take action: 
Help people embrace a new way of working, starting with building AI aptitude—from practicing prompt engineering to fact-checking and verifying AI-generated content. Leverage learning resources and crowdsource best practices from employees as they adapt to AI as copilot.  Consider how roles and functions can evolve alongside AI, creating opportunities for reinvention.  The Path Forward AI is poised to lift the weight of work—and has great potential to free people from digital debt and fuel innovation. And for both overwhelmed employees and leaders looking to bolster productivity, that promise is overdue. But AI won’t simply “fix” work—it will create a whole new way of working. Leaders will need to help employees learn to work responsibly alongside AI to reap the rewards of the AI-employee alliance: more value creation for businesses and a brighter, more fulfilling future of work for everyone. Learn more about how Microsoft is innovating to create a whole new way of working.   Sign up for the WorkLab newsletter First person Work Trend Index Special Report",83,1932,0.333771556615829,0.318503320217133,0.347725093364716
